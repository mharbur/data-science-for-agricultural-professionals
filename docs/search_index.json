[["index.html", "Data Science for Agricultural Professionals Preface Welcome R-language", " Data Science for Agricultural Professionals Marin L. Harbur 2021-06-04 Preface Welcome Welcome to Data Science for Agricultural Professionals. I have written these course materials for a few reasons. First, it is my job. The more powerful motivation, however, was to write a guide that satisfied the following criteria: - covers basic statistics used in reporting results from hybrid trials and other controlled experiments - also addresses data science tools used to group environments and make predictions - introduced students to R, an open-source statistical language that you can use after your studies at Iowa State University and can use without installing on your laptop, or using a VPN connection, which your work laptop may not allow. I also wanted to develop a text that presented statistics around the situations in which you are most likely to encounter data: - yield maps used by farmers and agronomists - side-by-side or split-field trials used often at the retail level - smaller-plot controlled experiments used in seed breeding and other product development - rate recommendation trials for fertilizers and crop protection products - fertilizer prediction maps - decision support tools I began my career as an a university researcher and professor, but in 2010 entered the private sector, working first in retail as a technical agronomist for a regional cooperative and then as a data scientist for a major distributor, developing product data and insights for a team of researchers and agronomists. In seeing how data were used at the retail and industry levels, I gained an appreciation for what areas of statistics were more often used than others. What is important to the agricultural professional, in my experience, is data literacy and a basic ability to run analyses. It is easy, after years in the field, to lose skills gained as an undergraduate. My hope is that all of you upon completing this course will look at statistics you receive (from any source, but especially manufacturers) a little more critically. If you are involved in field research, I hope you will understand how to better layout and more creatively analyze field trials I wanted to develop a reference that appreciated not all of us are mathematical progedies, nor do we have flawless memories when it comes to formula. At the risk of redudancy, I will re-explain formulas and concepts  or at least provide links  so you arent forever flipping back and forth trying to remember sums of squares, standard errors, etc. I am committed to making this as painless as possible. R-language In this course, we will use the R language to summarise data, calculate statistics, and view trends and relationships in data. R is open-source (free) and incredibly versatile. I have used multiple languages, including SAS and SPSS, in my career; in my opinion, R is not only the cheapest, but the best, and I now use it exclusively and almost daily in my work. R also has personal connections to Iowa State: Hadley Wickam, Carson Seivert, two authors of the R language, are Iowa State graduates. We will use an application called R-Studio to work with R language. R Studio allows you to write and save scripts (code) to generate analyses. It also allows you to intersperse Markdown (html code) in between your statistical output so that you can explain and discuss your results. The beauty of Markdown is your report and your statistics are together in a single file; no cutting and pasting is needed between documents. R is all about having options, and so with R-Studio you have the option of installing it on your laptop or, in case you are using a work laptop without administrative priviledges, you can also use a cloud site, R-Studio Cloud, to work with R and save your work. I know that for most of you R (and coding itself) will be a new experience. I am sure the idea of coding will intimidate many of you. To head off your anxiety as much as possible, I offer this: I understand that coding is challenging. I spend my days making mistakes, searching for bugs, and looking up how to do something for the umpteenth time. If something confuses you, that is normal. But I can also assure you that you will likely have an easy time remembering what functions to use. In other words, which code to use will not be the problem. Most of your bugs will be due to mispellings, forgetting to close a parentheses, or referring to a dataset by the wrong name, e.g. soy_data instead of soybean_data. And you will get better at avoiding these mistakes over time  there is not more to learn, just practice. Our exercises in R will be designed to give you that practice, and introduce new functions as slowly as possible. At the end of this course, you will have not only your completed work, but the course materials themselves as a resource from which you can borrow code for future projects. There is no shame in copying lines of codes (or whole chunks of code) into your own original analyses. All of us data scientists do that, and it is one of the best ways to continue learning. R is supported by many great books which may be accessed for free online, or purchased online for very reasonable prices. These may be used as references during the course, but also to continue learning for years to come. These include many references from bookdown.org: Introduction to Data Science: although there are many Introduction to R books, this one closely matches how we approach it in Agronomy 513. (https://rafalab.github.io/dsbook/) R Graphics Cookbook: a comprehensive explanation of how to create just about any plot you could ever imaging in R, mainly using the ggplot package. (https://r-graphics.org/) Geocomputation with R: The best book I have found for learning how to work with spatial data. (https://geocompr.robinlovelace.net/) Hands-On Machine Learning with R: I have not read this book, but it appears to be a robust and beautifully-illustrated guid to machine learning concepts in R. (https://bradleyboehmke.github.io/HOML/) "],["population-statistics.html", "Chapter 1 Population Statistics 1.1 Populations 1.2 Case Study: Yield Map 1.3 Distributions 1.4 Exercise: Introduction to R 1.5 Exercise: Introduction to Shapefiles 1.6 Exercise: Histograms", " Chapter 1 Population Statistics 1.1 Populations Almost every statistics text begins with the concept of a population. A population is the complete set of individuals to which you want to predict values. Lets dwell on this concept, as it is something that did not hit home for me right away in my career. Again, the population is all of the individuals for which you are interested in making a prediction. What do we mean by individuals? Not just people  individuals can plants, insects, disease, livestock or, indeed, farmers. Just as important as what individuals are in a population is its extent. What do you want the individuals to represent? If you are a farmer, do you want to apply the data from these individuals directly to themselves, or will you use them to make management decisions for the entire field, or all the fields in your farm? Will you use the results this season or to make predictions for future seasons? If you are an animal nutritionist, will you use rations data to support dairy Herefords, or beef Angus? If you are a sales agronomist, will you use the data to support sales on one farm, a group of farms in one area, or across your entire sales territory? If you are in Extension, will the individuals you measure be applicable to your entire county, group of counties, or state? If you are in industry like me, will your results be applicable to several states? This is a very, very critical question, as you design experiments  or as you read research conducted by others. To what do or will the results apply? Obviously, an Iowa farmer should not follow the optimum planting date determined for a grower in North Carolina, nor should an Ohio farmer assume our pale clays will be as forgiving as the dark, mellow loam of southern Minnesota. Drilling down, you might further consider whether research was conducted in areas that have similar temperature or rainfall, how different the soil texture might be to the areas to which you want to apply results. At the farm level, you might ask how similar the crop rotation, tillage, or planting methods were to that operation. At the field level, you might wonder about planting date or the hybrid that was sown. When you use the results from set of individuals to make predictions about other individuals, you are making inferences  you are using those data to make predictions, whether it be for that same field next month or next year, or for other locations (areas in a field, fields in a farm, counties, or states). When we speak of an inference space, then, that is the total group of individuals to which you will apply your results. Or, in another word, your population. In summary, one of the most critical skills you can apply with data science has no formula and, indeed, little to do with math (at least in our application). It is to ask yourself, will my experiment represent the entire population in which I am interested? Indeed, one field trial likely will not address the entire population in which you are interested  it is up to you to determine the population to which you are comfortable applying those results. In fact, statistics or data science done without this domain knowledge whether a dataset is useful or experimental results are reasonable can be disasterous. Just because a model fits one dataset very well or treatments are significantly different does not mean they should be used to make decisions. Your competency as an agronomic data scientist depends on everything you learn in your program of study. Soil science, crop physiology, and integrated pest management, to name just a few subjects, are as much a prerequisite as any math course you have taken. In some cases all of the individuals in a population can be measured  in such case, we will use the basic statistics described in this unit. The yield map we will analyze in this unit is a loose example of a case where we can measure In most cases, however, it is not physically or financially feasible to measure all individuals in a population. In that case, subsets of the population, called samples, are used to estimate the range of individuals in a population. 1.2 Case Study: Yield Map For our first case study, we will use a situation where every individual in our population can be measured: a single soybean field in central Iowa. In this case, yield data were gathered using a combine monitor. In case you dont live and breathe field crops like me, combines (the machines that harvest grain) are usually equipped with a scale that repeatedly weighs grain as the combine moves across the field. The moisture of the grain is also recorded. These data are combined with measures of the combine speed and knowledge of the number of rows harvested at once to calculate the yield per area of grain, adjusted to the market standard for grain moisture. library(sf) library(tidyverse) library(rcompanion) yield = st_read(&quot;data-unit-1/merriweather_yield_map/merriweather_yield_map.shp&quot;) ## Reading layer `merriweather_yield_map&#39; from data source `C:\\Users\\559069\\Documents\\data-science-for-agricultural-professionals_3\\data-unit-1\\merriweather_yield_map\\merriweather_yield_map.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 6061 features and 12 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -93.15474 ymin: 41.66619 xmax: -93.15026 ymax: 41.66945 ## Geodetic CRS: WGS 84 Lets start tearing this dataset apart, using our very first chunk of code. We can examine the first 6 rows of the dataset using R. Our data are in a shapefile named yield. To view the top six rows of any dataset in R, we use the command head. Just click on the button Run Code in the upper right corner of the window below. head(yield) ## Simple feature collection with 6 features and 12 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -93.15033 ymin: 41.66641 xmax: -93.15026 ymax: 41.66644 ## Geodetic CRS: WGS 84 ## DISTANCE SWATHWIDTH VRYIELDVOL Crop WetMass Moisture Time ## 1 0.9202733 5 57.38461 174 3443.652 0.00 9/19/2016 4:45:46 PM ## 2 2.6919269 5 55.88097 174 3353.411 0.00 9/19/2016 4:45:48 PM ## 3 2.6263101 5 80.83788 174 4851.075 0.00 9/19/2016 4:45:49 PM ## 4 2.7575437 5 71.76773 174 4306.777 6.22 9/19/2016 4:45:51 PM ## 5 2.3966513 5 91.03274 174 5462.851 12.22 9/19/2016 4:45:54 PM ## 6 3.1840529 5 65.59037 174 3951.056 13.33 9/19/2016 4:45:55 PM ## Heading VARIETY Elevation IsoTime yield_bu ## 1 300.1584 23A42 786.8470 2016-09-19T16:45:46.001Z 65.97034 ## 2 303.6084 23A42 786.6140 2016-09-19T16:45:48.004Z 64.24158 ## 3 304.3084 23A42 786.1416 2016-09-19T16:45:49.007Z 92.93246 ## 4 306.2084 23A42 785.7381 2016-09-19T16:45:51.002Z 77.37348 ## 5 309.2284 23A42 785.5937 2016-09-19T16:45:54.002Z 91.86380 ## 6 309.7584 23A42 785.7512 2016-09-19T16:45:55.005Z 65.60115 ## geometry ## 1 POINT (-93.15026 41.66641) ## 2 POINT (-93.15028 41.66641) ## 3 POINT (-93.15028 41.66642) ## 4 POINT (-93.1503 41.66642) ## 5 POINT (-93.15032 41.66644) ## 6 POINT (-93.15033 41.66644) Thats it! You just ran your first R command! Seriously, R includes a free software package that allows R to run inside a document like this. How cool is that! You could type any R command in the window above and it would run. This dataset has several columns, but the two most important to us in this lesson are yield_bu and geometry. Harvest files may not come with a yield column  in this case, I calculated the yield for you. I will not get into the formula for that  after all, this is just the first lesson of the course. That this dataset has a column named geometry indicates it is a shapefile  a dataset in which the measures are georeferenced. That is, we know where these measurements were taken. The geometry column in this case identifies a point with each observation. We can see this by plotting the data below. This command has three parts. Plot tells R to draw a map. yield tells R to use the yield dataset. yield_bu is the column we want to plot  we place it in quotes and brackets ([]) to tell R to only plot this column. Otherwise R will plot each of the columns, which will require a longer runtime. plot(yield[&quot;yield_bu&quot;]) What do you observe if you replace yield_bu with Moisture in the above code? Does one area of the field appear a little drier than the others? 1.3 Distributions At this point, we have two options any time we want to know about soybean yield in this field. We can pull out this map or the complete dataset (which has over 6,500 observations) and look at try to intuitively understand the data. Or we can use statistics which, in a sense, provide us a formula for regenerating our dataset with just a few numbers. 1.3.1 Histograms Before we get into the math required to generate these statistics, however, we should look at the shape of our data. One of the easiest and most informative things for us to do is to create a particular bar chart known as a histogram. histogram = hist(yield$yield_bu, breaks = 12) plot(histogram) A histogram gives us a quick visualization of our data. In the histogram above, each bar represents range of values. This range is often referred to as a bin. The lowest bin includes values from 50 to 59.0000. The next bin includes values from 60 to 69.9999. And so on. The height of each bar represents the number of individuals in that population that have values within that range. Bins can also be defined by their midpoints. The midpoint is the middle value in each range. For the bin that includes values from 50 to 50.9999, the midpoint is 55. For the bin that includes values from 60 to 60.9999, the midpoint is 65. In the plot above, the midpoint for each bar is indicated by the orange bar beneath it. We can also draw a histogram using the power ggplot package from Hadley Wickham, which allows for much greater customization of plots. We will use this package extensively throughout the course. Here is just a taste: ggplot(data=yield, aes(x=yield_bu)) + geom_histogram(breaks=seq(50, 110, 10), fill=&quot;tomato&quot;, color=&quot;black&quot;) Varying the bin width provides us with different perspectives on our distribution. Please click on the link below to open an application where you can vary the bin width and see how it changes your perspective. 1.3.2 Percentiles We can also use percentiles to help us describe the data even more numerically. To identify percentiles, the data are numerically ordered (ranked) from lowest to highest. Each percentile is associated with a number; the percentile is the percentage of all data equal to or less than that number. We can quicly generate the 0th, 25th, 50th, and 75th, and 100th percentile in R using the summary command. summary(yield$yield_bu) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 55.12 74.96 80.62 80.09 85.44 104.95 This returns six numbers. The 0th percentile (alternatively referred to as the minimum) is 0.00  this is the lowest yield measured in the field. The 25th percentile (also called the 1st Quartile) is 87.16. This means that 25% of all observations were equal to 86.55 bushels or less. The 50th percentile (also known as the median) was 97.91, meaning half of all observations were equal to 97.91 bushels or less. 75% of observations were less than 111.13, the 75th percentile (or 3rd quartile). Finally, the 100th percentile (or maximum yield) recorded for this field was 199.60. We are now gaining a better sense of the range of observations that were most common. But we can do even better and describe this distribution with even fewer numbers. 1.3.3 Normal Distribution Model Lets overlay a curve, representing the normal distribution model, on our histogram with the following function, plotNormalHistogram. plotNormalHistogram(yield$yield_bu, breaks = 10) In a perfect situation, our curve would pass through the midpoint of each bar. This rarely happens with real-world data, and especially in agriculture. The data may be slightly skewed, meaning there are more individuals that measure above the mean than below, or vice versa. In this example, our data do not appear skewed. Our curve seems a little too short and wide to exactly fit the data. This is a condition called kurtosis. No, kurtosis doesnt mean that our data stink. It just means it is a little more spread out or compressed than in a perfect situation. No problem. We can  and should  conclude it is appropriate to fit these data with a normal distribution. If we had even more data, the curve would likely fit them even better. Many populations can be handily summarized with the normal distribution curve, but we need to know a couple of statistics about the data. First, we need to know where the center of the curve should be. Second, we need to know the width or dispersion of the curve. 1.3.4 Measures of Center To better describe our distribution, we will use the arithematic mean and median. The mean is the sum of all observations divided by the number of observations. \\[\\displaystyle \\mu = \\frac{\\sum x_i}{n}\\] The \\(\\mu\\) symbol (a u with a tail) signifies the true mean of a population. The \\(\\sum\\) symbol (the character next to \\(x_i\\) which looks like an angry insect) means sum. Thus, anytime you see the \\(\\sum\\) symbol, we are summing the variable(s) to its right. \\(x_i\\) is the value \\(x\\) of the ith individual in the population. Finally, n is the number of individuals in the population. For example, if we have a set of number from 1:5, their mean can be calculated as: (1+2+3+4+5)/5 ## [1] 3 R, of course, has a function for this: x = c(1,2,3,4,5) mean(x) ## [1] 3 In the example above, we created a vector (a dataset with one column) of numbers by listing within c(). We also assigned this vector a name, x. We wont create too many vectors in this course, but this is how to do it. Try switching the numbers in between the parentheses above and calculating a mean for that dataset. Lets calculate the mean yield for our field. mean(yield$yield_bu) ## [1] 80.09084 The mean is about 95 bushels per acre. Where does this fall on our histogram? After drawing the histogram, we can use additional code to add a reference line: hist(yield$yield_bu, breaks=10) abline(v = mean(yield$yield_bu), col=&quot;red&quot;) We use abline to tell R to draw a line on our plot. v tells it to draw a vertical line, mean(yield$yield_bu) tells it the X position for this vertical line is equal to the mean value of yield_bu, andcol, you guessed it, specifies the line color. Try changing it todarkblue\". Earlier, you were introduced to the median. As discussed, the median is a number such that half of the individuals in the population are greater and half are less. If we have an odd number of individuals, the median is the middle number as the individuals are ranked from greatest to least. median(c(1,2,3,4,5)) ## [1] 3 If we have an even number of measures, the median is the average of the middle two individuals in the ranking: median(c(1,2,3,4,5,6)) ## [1] 3.5 Lets add a second line representing the median. hist(yield$yield_bu, breaks=10) abline(v = mean(yield$yield_bu), col=&quot;red&quot;) abline(v = median(yield$yield_bu), col=&quot;blue&quot;) As you can see, they are practically identical. When the mean and median are similar, the number of individuals measuring greater and less than the mean are roughly equivalent. In this case, our data can be represented using a model called the normal distribution. We also need a statistic that tells us how wide to draw the curve. That statistic is called a measure of dispersion, and we will learn about it next. 1.3.5 Measures of Dispersion To describe the spread of a population, we use three related statistics, which describe the summed difference of all from the mean (sum of squares), the average difference from the mean (variance) and, finally, the difference from the mean in our original units of measurement (standard deviation). Although there is a little math involved in these three statistics, please make yourself comfortable with their concepts because they are very important in this course. Almost every statistical test we will learn during the next 14 weeks is rooted in these measures of population width. 1.3.5.1 Sum of Squares The first measure of population width is the Sum of Squares. This is, simply, the sum of the squared differences between each observation and the mean. The sum of squares of a measurement x is: \\[\\displaystyle S_{xx} = (x_i - \\mu)^2\\] Where again \\(x_i\\) is the value \\(x\\) of the \\(ith\\) individual in the population and \\(\\mu\\) is the true mean of a population. Why do we square the differences between the observations and means? Simply, if we were to add the unsquared differences they would add to exactly zero. Lets prove this to ourselves. Lets again use the set of numbers (1, 2, 3, 4, 5). We can measure the distance of each individual from the mean by subtracting the mean from it. This difference is called the residual. individuals = c(1,2,3,4,5) residuals = individuals - mean(individuals) residuals ## [1] -2 -1 0 1 2 In the first line of code, we defined our individuals as having values of 1, 2, 3, 4, and 5. In the second line of code, we subtracted their mean from each of us. We can see the output  residuals  consist of -2, -1, 0, -1, and -2. And if we sum these residuals we get zero. sum(residuals) ## [1] 0 Lets now do this with our field data. The number of residuals (almost 6800) is too many to visualize at once, so we can use the sample function to pick 20 at random. yield_residuals = yield$yield_bu - mean(yield$yield_bu) sample(yield_residuals, 20) ## [1] 4.730451 8.266829 14.462321 -6.799431 -21.722742 -7.404110 ## [7] -11.938173 -2.684276 2.887243 19.739052 -13.255220 -9.318519 ## [13] 2.589134 -7.475283 -20.710176 10.372908 7.134112 -6.178013 ## [19] -8.967195 -11.957666 If we sum up all the yield residuals, what number do we get? Any guesses before you click Run Code? sum(yield_residuals) ## [1] 1.139e-11 Not exactly zero, but a very, very small number  a 4 with 10 zeros in front of it. The difference from zero is the result of rounding errors during the calculation. The sum of squares is calculated by squaring each residual and then summing the residuals. For our example using the set (1, 2, 3, 4, 5): individuals = c(1,2,3,4,5) residuals = individuals - mean(individuals) squared_residuals = residuals^2 sum(squared_residuals) ## [1] 10 And for our yield data: yield_residuals = yield$yield_bu - mean(yield$yield_bu) squared_yield_residuals = yield_residuals^2 yield_sum_of_squared_residuals = sum(squared_yield_residuals) yield_sum_of_squared_residuals ## [1] 461211.3 1.3.5.2 Variance The sum of squares helps quantify spread: the larger the sum of squares, the greater the spread of observations around the population mean. There is one issue with the sum of squares, though: since the sum of square is derived from the differences between each observation and the mean, it is also related to the number of individuals overall in our population. In our example above, the sum of squares was 10: individuals = c(1,2,3,4,5) residuals = individuals - mean(individuals) squared_residuals = residuals^2 sum(squared_residuals) ## [1] 10 Change the first line of code above so it reads \"individuals = c(1,2,3,4,5,1,2,3,4,5). You will notice the sum of squares increases to 20. The spread of the data did not change, only the number of observations. Given any distribution, the sum of squares will always increase with the number of observations. If we want to compare the spread of two different populations with different numbers of individuals, we need to adjust our interpretation to allow for the number of observations. We do this by dividing the sum of squares, \\(S_{xx}\\) by its associated degrees of freedom, \\(n-1\\). In essense, we are calculating an average of the sum of squares. This value is the variance, \\(\\sigma^2\\). \\[\\sigma^2 = \\frac{S_{xx}}{n}\\] We can calculate the variance as follows. individuals = c(1,2,3,4,5) residuals = individuals - mean(individuals) squared_residuals = residuals^2 sum_squared_residuals = sum(squared_residuals) variance = sum_squared_residuals / length(residuals) variance ## [1] 2 We introduced a new function above, length. Length gives us the number of individuals in any dataset, in this case 5. By dividing the sum of squares by the number of individuals, we average our sum of squares. The variance of this dataset is 2. Change the first line of code above so it reads \"individuals = c(1,2,3,4,5,1,2,3,4,5) and run the code again. The variance will now divide the sum of squares by 10. The variance will again be 2. Our yield variance is: yield_variance = yield_sum_of_squared_residuals/length(yield_residuals) 1.3.5.3 Standard Deviation Our challenge in using the variance to describe population spread is its units are not intuitive. When we square the measure we also square the units of measure. For example the variance of our yield is measured in units of bushels2. Wrap your head around that one. Our solution is to report our final estimate of population spread in the original units of measure. To do this, we calculate the square root of the variance. This statistic is called the standard deviation, \\(\\sigma\\). \\[\\sigma = \\sqrt{(\\sigma^2)}\\] sqrt(variance) ## [1] 1.414214 The standard deviation of our yield data is: sqrt(yield_variance) ## [1] 8.72324 That is enough theory for this first week of class. The remainder of this lesson will focus on introducing you to RStudioCloud. 1.4 Exercise: Introduction to R This document you have just opened is called an R Notebook. This document allows you to combine code and commentary into a seamless document. This sentence is an example of commentary. Commentary is anything you would type in a Word document. It can be the identifier for a question, or your response to that question. Immediately below is what in R Studio is called a chunk. You can create a chunk anywhere in a document by typing CTRL-ALT_I. Note that a chunk always begins with {r} and always ends with. This tells the notebook that anything between those two markings is R code to be run. sample_data = c(1,2,3,4,5) # this creates a vector (a single column of data) and names it &quot;sample_data&quot; calculation = mean(sample_data) # this calculates the mean for the vector and names it &quot;calculation&quot; calculation # this tells R to print the value of calculation ## [1] 3 The code chunk above can be run by clicking on the green arrow in its upper-right corner. R will run and, if there are no errors, display the output of the code immediately below the chunk. If you click on the chunk above, it will create the sample_data vector, calculate the mean of that vector and name it calculation, and print the value of calculation, which is 3. There is nothing magical about the names we have selected. They are just placeholders for the data. 1.4.1 Your Very First Code Now you try it: - create a chunk by pressing CTRL_ALT_I - create a vector named my sample data that containes the numbers 6, 7, 8, 9, and 10. - calculate the mean and assign it to a variable named my_calculation - display the results [delete this line and create your chunk right here] Were you successful? 1.4.2 Reading and Working with Data Frames Of course, you will soon want to work with your own data, and it is likely to consist of multiple vectors (columns). That is, you have a table of data. In R, this is commonly referred to as a data frame. To read in data frame, use the read.csv command. In the chunk below, we will use read.csv to read in a .csv file and assign it to a data frame named yield. yield = read.csv(&quot;data-unit-1/exercise_data/sample_dataset_1.csv&quot;) The text in quotes is the path to the table of interest. data/ tells R the file is in a directory named data that is in the same folder as this file. The rest of the text is simply the file name. We can view this data frame in two ways. We can simply type its name: yield ## observation yield ## 1 1 227.2262 ## 2 2 218.4626 ## 3 3 219.8543 ## 4 4 209.2137 ## 5 5 218.3676 ## 6 6 219.5215 ## 7 7 224.6843 ## 8 8 221.5054 ## 9 9 222.6487 ## 10 10 226.4952 Or we can view it in more of a spreadsheet enviornment by double clicking on it in the window to the right, or by typing view(yield) in the console window below. Once the data frame is created, we can use it in any subsequent chunk. We can look at the data frame as a whole by typing its name. We can use a particular column by typing the name of the data frame, followed by $ and the name of the column. single_column = yield$yield single_column ## [1] 227.2262 218.4626 219.8543 209.2137 218.3676 219.5215 224.6843 221.5054 ## [9] 222.6487 226.4952 If you pause after typing $, R will display a list of the available columns. Just click on the one in which you are interested. 1.4.3 Basic Operations on Columns We can easily perform statistical summaries now that we have defined single_column. As above, we can calculate its mean: single_column_mean = mean(single_column) single_column_mean ## [1] 220.798 Or its median: single_column_median = median(single_column) single_column_median ## [1] 220.6798 Its maximum value: single_column_max = max(single_column) single_column_max ## [1] 227.2262 Or its minimum value: single_column_min = min(single_column) single_column_min ## [1] 209.2137 1.4.4 Knitting Your Results into a Document When you are done creating your R notebook, you can knit it into a final document by clicking on the Preview button in the menu bar above: library(knitr) include_graphics(&quot;data-unit-1/exercise_data/images/knitting.png&quot;) If you look in the exercise folder, you will find an .html printout of this notebook. You can email this file to collaborators, publish it to the internet, or incorporate it into your creative component. 1.4.5 Practice Now you try. There are four practice sets, labelled intro_to_r_practice_set_1.csv through intro_to_r_practice_set_4.csv. Practice reading in the .csv file and calculating summary statistics as shown above. 1.5 Exercise: Introduction to Shapefiles One of my more recent areas of growth as a data scientist has been learning to work with spatial data. If you have worked with yield monitor data, used ArcGIS, or used an application map to variable-rate your fertilizer, you have worked with spatial data. Although we wont return to spatial data until later in the course, here is a brief introduction to inporting and inspecting spatial data. 1.5.1 R Packages R programming consists of base software plus an ever increasing number of plug-ins called packages. These packages provide additional functions (programs) in R that cover everything from agricultural plot design, interactive maps, and advanced plotting techniques, to app development, access to census data, and even modules to teach statistics and generate quizes. If R tells us that a package is not installed, we can install it using the install.packages() command, where the package name is included between the quotes. This code is shown below, but it is preceded by a hashtag, so that line of code will not be executed by R. The hashtag can be used to comment code or coding notes/explanations to that they can remain inline with the code we want to run, but not be run themselves. # install.packages(&#39;sf&#39;, repos=&#39;http://cran.us.r-project.org&#39;) library(sf) 1.5.2 Reading Shapefiles To read in a shapefile, we use the st_read command. Reading in a shapefile is identical to how we read in a .csv file in the previous exercise. We simply follow st_read with the path to the shapefile, which has a .shp suffix. We will read in a shapefile from Indiana that includes county summaries for a couple of dozen agricultural statistics. indiana = st_read(&quot;data-unit-1/exercise_data/Agriculture_Census-shp/Agriculture_Census.shp&quot;) ## Reading layer `Agriculture_Census&#39; from data source `C:\\Users\\559069\\Documents\\data-science-for-agricultural-professionals_3\\data-unit-1\\exercise_data\\Agriculture_Census-shp\\Agriculture_Census.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 92 features and 31 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -88.08975 ymin: 37.77306 xmax: -84.78768 ymax: 41.76233 ## Geodetic CRS: WGS 84 1.5.3 Examining Spatial Feature Data Frames Lets inspect the first six rows of the spatial feature data frame using the head() command, where the name of the data frame is given between the parentheses. head(indiana) ## Simple feature collection with 6 features and 31 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -87.52704 ymin: 41.16195 xmax: -84.80631 ymax: 41.76233 ## Geodetic CRS: WGS 84 ## OBJECTID AGCENSP020 COUNTY N_FARM TOTCROP_AC HAVCROP_AC DOL_FARM ## 1 1 2515 Elkhart County 1335 160160 143324 92912 ## 2 2 2516 Saint Joseph County 666 139661 131004 82849 ## 3 3 2517 La Porte County 749 226816 212940 127922 ## 4 4 2518 Steuben County 581 99218 71944 44133 ## 5 5 2519 Lagrange County 1392 156233 127875 74194 ## 6 6 2539 Lake County 442 138929 132551 108206 ## NURS_DOL LVSTOK_DOL SALE_2500 SALE_5000 EXPEN_1000 NURSY_1000 N_CHICKENS ## 1 34688 89350 251 100 96862 514 3005354 ## 2 40051 15127 144 71 40479 583 852 ## 3 68041 27773 136 60 71666 742 -9999 ## 4 16776 8865 236 44 20288 0 -9999 ## 5 37364 65915 223 105 83668 0 1870836 ## 6 42747 5080 101 30 34712 66 0 ## N_CATTLE N_PIGS CORN_AC WHEAT_AC COTTON_AC SOYBEAN_AC VEGTABL_AC ORCHARD_AC ## 1 42719 73951 64955 3861 0 42251 1338 0 ## 2 6440 27430 69251 4073 0 45696 480 0 ## 3 24980 27110 113242 4186 0 76809 1125 0 ## 4 9257 6859 32152 4483 0 25120 304 0 ## 5 39275 69338 61262 3993 0 32666 1182 0 ## 6 3204 9435 68344 3101 0 55698 1256 0 ## PECAN_AC IRRGTD_AC N_MACHINE FEMALE_OP MINORTY_OP FAM_FARM igs_DBO_Ag ## 1 0 23524 495 32 0 1164 0 ## 2 0 12941 444 44 3 584 0 ## 3 0 27090 631 44 0 626 0 ## 4 0 1225 -9999 42 0 505 0 ## 5 0 23478 315 26 0 1232 0 ## 6 0 6211 327 42 3 356 0 ## SHAPE_Leng SHAPE_Area geometry ## 1 139058.4 1210349270 POLYGON ((-86.06246 41.7619... ## 2 155098.8 1196846667 POLYGON ((-86.22555 41.7614... ## 3 175216.0 1563031845 POLYGON ((-86.82619 41.7608... ## 4 116305.4 835438144 POLYGON ((-85.19779 41.7605... ## 5 128976.5 1000327995 POLYGON ((-85.29134 41.7606... ## 6 167477.6 1300778609 POLYGON ((-87.52401 41.7135... Spatial feature data frames are very similar to the data frames that were introduced to in the last exercise. There are a few differences, however. First, every spatial feature data frame consists of rows of observations that are linked, in the geometry column, with a georeference. That georeference may be a simple point on the map, or it may be a polygon that defines an area, such as a field, a county, or a state. Spatial feature data frames also include some metadata about the geometry of the file, including geometry type (POLYGON in this case, since we are working with county outlines) and  this is critical  CRS. CRS stands for coordinate reference system. Coordinate reference system refers to the projection we use to draw a map, and the georeferences used to define a place or area. Maps are flat representations of a round object, and there are different ways to draw these maps. We need not go into a deep review of those projects at this point; suffice it to know that each projection represents tradeoffs in how accurately it portrays areas in the center and periphery of the map and how much it represents the curvature. For most of my work, I use the CRS of 4326. You know this system more commonly as the latitude/longitude system. The bounding box defines where the map is centered. Latitude/longitude maps are more accurate in portraying areas near the center of the map than the periphery, so it is important the map be centered on the area of interest. 1.5.4 Visualizing Data To visualize the data, we can use the plot function from the sf package. You might assume we could map a column of data the same way we visualized its contents earlier, by indicating the data frame and column name, separated by the dollar sign. We will try that with TOTCROP_AC, the total number of acres per county in crop production. plot(indiana$TOTCROP_AC) Instead of a map, however, this shows us the range of values for TOTCROP_AC in the dataset. To view a map, we need to first subset the data so it only has two columns: the TOTCROP_AC data and geometry columns. We can then map these data: tot_crop = indiana[&quot;TOTCROP_AC&quot;] plot(tot_crop) No one can resist a pretty map. The technical name for a map like this, where you color-code political shapes like counties, is chloropleth. We are seeing plenty of these in 2020, between the pandemic and the election year. Back to happier thoughts. Lets look at the mean farm income per county. This is indicated by the DOL_FARM column. Again, we first subset the data and then plot it. farm_income = indiana[&quot;DOL_FARM&quot;] plot(farm_income) Where are the chickens in Indiana? Lets find out: chickens = indiana[&quot;N_CHICKENS&quot;] plot(chickens) It would appear the most finger-lickin good county is Elkart County, in the far north. Appropriately, there are also a couple of counties near Kentucky! 1.5.5 Create Your Own Maps Where are the hogs in Indiana? Create a map for N_PIGS. Where are most of the wheat acres? Create a map for WHEAT_AC. Which counties have greater farm ownership by minorities? Create a map for MINORTY_OP (note the mispelling). 1.6 Exercise: Histograms One of the most critical  and I will admit, often overlooked at first by me in my rush to look at treatment differences  steps in data science is to inspect the data for the shape of its distribution and extreme values. There are a multiple ways to do this. This week, we will focus on the histogram. 1.6.1 Case Study The first example we will use is a cotton uniformity trial conducted in Greece in 1938. Yes, I realize it is a bit dated, but the concept is one we deal with every day in agricultural research  how consistent are our plots. In other words, what is the variance among plots? In a uniformity trial, a series of plots are managed identically so that the distribution of their yields can be measured. These data can be used to determine whether that field is a good site for a research trial. Our first step is to load the data, which is in a .csv file. cotton = read.csv(&quot;data-unit-1/exercise_data/cotton_uniformity.csv&quot;) 1.6.2 Basic Histogram Drawing a simple histogram in R requires two steps. First, we need to define the column in which we are interested. Then we create the histogram. Since we are using a data frame, we need to tell R which column to plot. We will, unimaginedly, call this column yield. With our second line of code we will create the histogram. In the third line, we tell R to plot the histogram yield = cotton$yield histogram = hist(yield) plot(histogram) And, voila, we have a plot of our histogram. Each bar represents a range in values, or a bin. We can see the upper and lower yield limits of each bins using the histogram$breaks command. histogram$breaks ## [1] 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 Alternatively, we could see the midpoints, or yield values that define the middle of each bin, using the histogram$mids command. histogram$mids ## [1] 0.35 0.45 0.55 0.65 0.75 0.85 0.95 1.05 1.15 1.25 1.35 1.45 1.55 1.65 As we saw in the lesson, varying the number of columns can affect how we see patterns in the data. In the plot above, we have 14 bars. Each bar represents a bin width of 0.1. What if we tell R to use 28 bins to draw the histogram? We can do that by telling R to use about 28 breaks (divisions between bins). histogram = hist(yield, breaks = 28) Note that we ended up with 27, not 28 bins, but that each now has a bin width that is 0.05 kg. We can verify this by again inspecting the data with histogram$breaks. histogram$breaks ## [1] 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 ## [16] 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.6.3 Histograms with ggplot ggplot2 is a very powerful and population package in R for creating plots. I spend A LOT of time using ggplot2, essentially any time I am creating plots. With ggplot, if you can imagine it, you almost certainly can draw it. There are entire books written on ggplot; rather than focus on it, I will try to weave tidbits throughout this course. Since ggplot is a package, we need to install and load it before we can use it. Note: if you are running R on your desktop, you only need to install a package the very first time you use it. Afterwards, you only need to run library() to load it into the local memory when you need it. The first line of code calls ggplot(). We use data=cotton to tell R we are drawing data from the cotton data frame. We then need to specify an aesthetic. An aesthetic is any property of the plot that relates to a variable, that is, a column in our data frame. In this case, we use aes(x=yield) to tell R that our aesthetic is positioned horizontally according to the value of yield. In the second line, we use geom_histogram to tell R we are drawing a histogram. Knowing this, R will automatically assign bins and count the number of observations in each bin. # install.packages(&quot;ggplot2&quot;) library(ggplot2) ggplot(data=cotton, aes(x=yield)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. By default, R chooses to create 30 bins. We can easily specify a different number of bins using the bins= argument. ggplot(data=cotton, aes(x=yield)) + geom_histogram(bins=15) Alternatively, we can set a particular binwidth using the binwidth = argument. ggplot(data=cotton, aes(x=yield)) + geom_histogram(binwidth=0.05) If we want to make our plot a little less bland, we can tell ggplot to use a different color to fill the bars using the fill = argument ggplot(data=cotton, aes(x=yield)) + geom_histogram(binwidth=0.05, fill=&quot;darkolivegreen&quot;) Finally, we can outline the bars using the color = argument. ggplot(data=cotton, aes(x=yield)) + geom_histogram(binwidth=0.05, fill=&quot;darkolivegreen&quot;, color=&quot;black&quot;) We will introduce more ways to fine tune our plots as the course goes on. 1.6.4 Practice In the data folder there are three practice files: barley_uniformity.csv, peanut_uniformity.csv, and tomato_uniformity.csv. Practice creating histograms using both the basic and ggplot methods. To start you off, here is the command to load the barley file. barley = read.csv(&quot;data-unit-1/exercise_data/barley_uniformity.csv&quot;) "],["distributions-and-probability.html", "Chapter 2 Distributions and Probability 2.1 Case Study 2.2 The Normal Distribution Model 2.3 The Z-Distribution 2.4 Exercise: Z-Distribution and Probability", " Chapter 2 Distributions and Probability In this unit we will continue with normal distribution model introduced in the previous unit. As you will recall, the normal distribution is a symmetrical curve that represents the frequency with which individuals with different particular measured values occur in a population. The peak of the curve is located at the population mean, \\(\\mu\\). The width of the curve reflects how spread out other individuals are from the mean. We learned three ways to measure this spread: the sum of squares, the variance, and the standard deviation. These statistics can roughly be thought of as representing the sums of the squared differences, the average of the squared distances, and the distances presented in the original units of measure. These four statistics  mean, sum of squares, variance, and standard deviation  are among the most important you will learn in this course. 2.1 Case Study This week, we will continue to work with the Iowa soybean yield dataset introduced to us in Unit 1. Lets review the structure of this dataset: head(yield) ## Simple feature collection with 6 features and 12 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -93.15033 ymin: 41.66641 xmax: -93.15026 ymax: 41.66644 ## Geodetic CRS: WGS 84 ## DISTANCE SWATHWIDTH VRYIELDVOL Crop WetMass Moisture Time ## 1 0.9202733 5 57.38461 174 3443.652 0.00 9/19/2016 4:45:46 PM ## 2 2.6919269 5 55.88097 174 3353.411 0.00 9/19/2016 4:45:48 PM ## 3 2.6263101 5 80.83788 174 4851.075 0.00 9/19/2016 4:45:49 PM ## 4 2.7575437 5 71.76773 174 4306.777 6.22 9/19/2016 4:45:51 PM ## 5 2.3966513 5 91.03274 174 5462.851 12.22 9/19/2016 4:45:54 PM ## 6 3.1840529 5 65.59037 174 3951.056 13.33 9/19/2016 4:45:55 PM ## Heading VARIETY Elevation IsoTime yield_bu ## 1 300.1584 23A42 786.8470 2016-09-19T16:45:46.001Z 65.97034 ## 2 303.6084 23A42 786.6140 2016-09-19T16:45:48.004Z 64.24158 ## 3 304.3084 23A42 786.1416 2016-09-19T16:45:49.007Z 92.93246 ## 4 306.2084 23A42 785.7381 2016-09-19T16:45:51.002Z 77.37348 ## 5 309.2284 23A42 785.5937 2016-09-19T16:45:54.002Z 91.86380 ## 6 309.7584 23A42 785.7512 2016-09-19T16:45:55.005Z 65.60115 ## geometry ## 1 POINT (-93.15026 41.66641) ## 2 POINT (-93.15028 41.66641) ## 3 POINT (-93.15028 41.66642) ## 4 POINT (-93.1503 41.66642) ## 5 POINT (-93.15032 41.66644) ## 6 POINT (-93.15033 41.66644) And map the field: library(sf) plot(yield[&quot;yield_bu&quot;]) 2.2 The Normal Distribution Model Mean, sum of squares, variance, and standard deviation are so important because they allow us to reconstruct the normal distribution model. Before we go further, what is a model? Is it frightening we are using that term in the second week of class? A model is a simplified representation of reality. No, that doesnt mean that models are fake (SMH). It means that they summarize aspects of data, both measured and predicted. The normal distribution model describes the relationship between the values of individuals and how frequently they appear in the population. Its value is it can be reconstructed by knowing just two things about the original dataset  its mean and its standard deviation. 2.2.1 The Bell Curve The normal distribution is also referred to as the bell curve, since it is taller in the middle and flared on either side. This shape reflects the tendency of measures within many populations to occur more frequently near the population mean than far from it. Why does this occur and how do we know this? As agronomists, we can reflect on what it takes to produce a very good  or very bad crop. For a very good crop, many factors need to coincide: temperature, precipitation, soil texture, nitrogen mineralization, proper seed singulation (spacing), pest control, and hybrid or variety selection, to name a few. We may, in a typical season or within a field, optimize one or two of these conditions, but the possibility of optimizing every one is exceedingly rare. Thus, if we are measuring yield, measures near the mean yield will occur more frequently. Extremely high yields will occur less frequently. Conversely, very low yields require we manage a crop very badly or that catastrophic weather conditions occur: a hailstorm, flood, or tornado. A frost at exactly the wrong time during seed germination or a drought. A planter box running out of seed or a fertilizer nozzle jamming. These things do occur, but less frequently. The distribution of individuals around the mean is also a the result of measurement inaccuracies. Carl Friedrich Gauss, who introduced the normal distribution model, showed that it explained the variation in his repeated measurements of the position of stars in the sky. All measurements of continuous data (those that can be measured with a ruler, a scale, a gradated cylinder, or machine) have variation  we use the term accuracy to explain their variation around the population mean. 2.2.2 Distribution and Probability Some areas of mathematics like geometry and algebra produce proven concepts (ie theorems), the product of statistics are almost always probabilities. To be more specific, most of the statistical tests we will learn can be reduced to the probability that a particular value is observed in a population. These probabilities include: - the probability an individual measurement or population mean is observed (normal distribution) - the difference between two treatments is not zero (t-test and least-significant difference) - the probability of one measure given another (regression) - the probability that the spread of individual measures in a population is better predicted by treatment differences than random variation (F-test and analysis of variance) These probabilities are all calculated from distributions  the frequency with which individuals appear in a population. Another way of stating this is that probability is the proportion of individuals in a population that are have measured values within a given range. Examples could include: - the proportion of individual soybean yield measurements, within one field, that are less than 65 bushels per acre - the proportion of individual corn fields that have an an average less than 160 bushels per acre - the proportion of trials in which the difference between two treatments was greater than zero - the proportion of observations in which the actual crop yield is greater than that predicted from a regression model 2.2.3 Probability and the Normal Distribution Curve Probability can be calculated as the proportion of the area underneath the normal distribution that corresponds to a particular range of values. We can visualize this, but first we need to construct the normal distribution curve for our soybean field. We need just two data to construct our curve: the mean and standard deviation of our yield. These are both easily calculated in R. library(muStat) yield_mean = mean(yield$yield_bu) yield_sd = stdev(yield$yield_bu, unbiased = FALSE) # to see the value of yield_mean and yield_sd, we just type their names and run the code. yield_mean ## [1] 80.09084 yield_sd ## [1] 8.72252 Now we can build our curve with a single line of code. The plotDist function in R will reconstruct the distribution curve for any population, given three arguments, in the following order: what kind of curve to draw, the population mean, and population standard deviation. - dnorm tells plotDist to draw a normal distribution curve. This needs to enclosed in quotes - yield_mean tells plotDist to use the mean we calculated in the previous chunk as the population mean - yield_sd tells plotDist to use the standard deviation we calculated in the previous chunk as the standard deviation library(fastGraph) plotDist(&quot;dnorm&quot;, yield_mean, yield_sd) Lets now shade the area underneath the normal curve corresponding to X values from 70 - 80. This area will represent the proportion of the population Where individuals were measured to have values between 70 and 80 bushels. The shade.norm function makes effort painless. There are five arguments in this function: - xshade is the range of individuals in our population for which we want to calculate their area under the distribution curve. We pass this range as a vector using c(minimum, maximum), in this case c(70,80) - ddist tells the function what kind of distribution curve to draw. dnorm tells it to construct a normal distribution curve - the third value we include (and it must be in this order), is the population mean. In this case, we pass the variable yield_mean whose value we calculated in the previously - the fourth value is the population standard deviation, which we have already calculated and assigned to the variable yield_sd - lower.tail=FALSE tells shade.norm to shade the area under the curve between 70 and 80 bushels and to calculate its area. shadeDist(xshade=c(70,80), ddist = &quot;dnorm&quot;, yield_mean, yield_sd, lower.tail = FALSE) Pretty cool, huh? The red area is the proportion of the soybean yield population that was between 70 and 80 bushels. shadeDist has also presented us with the proportion of the curve represented by that area, which it has labelled Probability. The probabiliity in this case is 0.3722. What does that number mean? The total area of the curve is 1. The proportion of the area under the curve that corresponds with yields from 70 to 80 bushels, then, is 3.967 percent of the area. This means that 37.22 percent of the individuals in our yield population had values from 70 and 80 bushels But wait a second  why is R using the term Probability? Think of it this way. Imagine you sampled 1000 individuals from our population. If 37.22 percent of our individuals have values from 70 to 80 bushels, then about 37% of the individuals in your sample should have values from 70 to 80 bushels. In other words, there is a 37% probability that any individual in the population will have a value from 70 to 80 bushels. Lets test this. Lets randomly sample 1000 individuals from our sample. Then lets count the number of individuals that have yields between 70 and 80 bushels. We will run three lines of code here: - we will use the sample function to randomly sample our population and return a vector of those numbers - we will use the subset function to subset our data into those that meet logical conditions and return a dataframe or vector. We provide two arguments: first, the name of the dataset or vector, and second the conditions. - we will use the length function to count the number of observations yield_sample = sample(yield$yield_bu, 1000) # &quot;yield_sample &gt;=70 &amp; yield_sample &lt;=80 tells it to only include measures from 70 to 80 in the subset yield_subset = subset(yield_sample, yield_sample &gt;=70 &amp; yield_sample &lt;=80) length(yield_subset) ## [1] 346 Is the proportion predicted by the normal distribution curve exactly that of the actual population? Probably not. The normal distribution curve is, after all, a model  it is an approximation of the actual population. Run the code above multiple times and observe how the percentages change. The proportions will vary slightly but generall stay in a range from about 0.32 to 0.37. We will talk more about sampling in the next unit. 2.3 The Z-Distribution The relationship between probability and the normal distribution curve is based on the concept of the Z-distribution. In essence, Z-distribution describes a normal distribution curve with a population mean of 0 and a standard deviation of 1. plotDist(&quot;dnorm&quot;, 0, 1) The Z-distribution helps us understand how probability relates to standard deviation, regardless of the nature of a study or its measurement units. For example, the proportion of a population within one standard deviation of the mean is about 68 percent: shadeDist(xshade=c(-1, 1), ddist = &quot;dnorm&quot;, 0, 1, lower.tail = FALSE) Similarly, the proportion of a population within 1.96 standard deviations of the mean is about 95 percent: shadeDist(xshade=c(-1.96, 1.96), ddist = &quot;dnorm&quot;, 0, 1, lower.tail = FALSE) Conversely, the proportion of a population beyond 1.96 standard deviations from the mean is about 5 percent. We can visualize this by changing the last argument of our code to lower.tail = TRUE. shadeDist(xshade=c(-1.96, 1.96), ddist = &quot;dnorm&quot;, 0, 1, lower.tail = TRUE) We refer to the upper and lower ends of the distribution as tails. In a normal distribution we would expect about 2.5% of observations to less than -1.96 standard deviations of the mean. We can measure the proportion in one tail by changing our argument in the shadeDist command to xshade = -1.96. shadeDist(xshade=-1.96, ddist = &quot;dnorm&quot;, 0, 1, lower.tail = TRUE) And 2.5% of the population to be more than +1.96 above the mean: shadeDist(xshade=1.96, ddist = &quot;dnorm&quot;, 0, 1, lower.tail = FALSE) Notice we changed the last argument back to lower.tail = TRUE. The causes R to shade the area above 1.96 standard deviations of the mean. 2.3.1 Important Numbers: 95% and 5% Above we learned that 95% of a normal distribution is between 1.96 standard deviations of the mean, and that 5% of a normal distribution is outside this range. Perhaps these numbers sound familiar to you. Have you ever seen results presented with a 95% confidence interval? Have you ever read that two treatments were significantly different at the P=0.05 level? For population statistics, the normal distribution is the origin of those numbers. As we get further into this course, we will learn about additional distributions  binomial, chi-square, and F  and the unique statistical tests they allow. But the concept will stay the same: identifying whether observed statistical values are more likely to occur (i.e., within the central 95% of values expected in a distribution), or whether the values are unusual (occurring in the remaining 5%). set.seed(051520) output = matrix(ncol=1, nrow=100) for(i in c(1:100)){ yield_sample = sample(yield$yield_bu, 1000) N = length(yield_sample[yield_sample &gt;=80 &amp; yield_sample &lt;=100]) output[i] = N } 2.4 Exercise: Z-Distribution and Probability This week, we have just have one exercise. Its objective is for you gain comfort with the concept of distribution and probability by working with the Z-distribution. We will learn how to use the plot and probability function we used throughout the lesson. 2.4.1 Case Study: Barley Data We were introduced to the barley dataset last week. These data are from a trial that was conducted to measure the uniformity of plots, that is, how consistent were yield measurements from plots that were treated identically. barley = read.csv(&quot;data-unit-2/exercise_data/barley_uniformity.csv&quot;) Lets take a look at the top six rows of the dataset. We can see the plots are identified by their row nad column locations. The yields are not per acre, but per plot. head(barley) ## row col yield ## 1 20 1 185 ## 2 19 1 169 ## 3 18 1 216 ## 4 17 1 165 ## 5 16 1 133 ## 6 15 1 195 2.4.2 Measuring the Population Distribution: Mean and Standard Deviation Before we can calculate probability, we need to construct the normal distribution curve. And before we can do that, we need to measure the mean and standard deviation of the barley population. Lets do a quick visual check to make sure it is approximately normally-distributed. We will define the column of interest from the dataset as yield and then plot it using the histogram (hist) function. Remember, we reference a column from the barley dataframe by typing barley, the dollar sign $, and then the name of the column of interest. yield = barley$yield hist(yield) The histogram shows the barley distribution is approximately normal. No distribution is perfectly normal. Remember, when we fit the data with the normal distribution, we are using a model distribution, not the actual distribution. But the model tends to perform very, very well. We can quickly calculate the mean and standard deviation using the mean and sd functions on yield. mean(yield) ## [1] 151.92 sd(yield) ## [1] 31.14029 We will assign these to variables we can reference later on, so we dont have to re-enter these repeatedly in our analyses. Remember, the population mean is symbolized by the Greek letter mu, so we will call the mean mu. We will call the standard deviation sigma, which is the Greek letter used to symbolize it. mu = mean(yield) sigma = sd(yield) 2.4.3 The shadeDist Function To calculate probabilities, we will use the shadDist function which is part of the fastGraph package in R. Remember, packages are groups of functions (calcluations and plots) beyond those in the base R software. The first time we use fastGraph, we must install it. (Note: if R asks you to restart dur the installation, go ahead and click yes.) # install.packages(&quot;fastGraph&quot;) #delete pound sign at far left to use this command We can then load the package using the library(fastGraph) command. library(fastGraph) Now we are ready to look at the barley distribution. We will define the column we are interested in as an independentyield\" and then plot its distribution model using shadeDist. To plot the basic distribution, we have to enter four arguments into shadeDist: xshade = NULL : this tells R not to shade any part of the curve ddist = dnorm: this tells R we are working with the normal (Z) distribution parm1 = mu : parm1 is used to pass the appropriate paramenter to R for the distribution we have chosen. For a normal distribution, the first parameter is the mean. parm2 = sigma: parm2 passes the next appropriate parameter to R for the normal distribution, the standard deviation. yield = barley$yield shadeDist(xshade = NULL, ddist = &quot;dnorm&quot;, parm1 = mu, parm2 = sigma) And voila, we have our basic normal distribution curve. The horizontal (X) axis represents the range of observed values. The vertical axis, titled (probability density function), indicates the proportion of the population that would include a given range of X values. 2.4.4 Probabilities in Lower Tails Lets run our first calculation. What percentage of this population has a yield value less than 100? to answer this, we will pass the following two arguments to shadeDist: xshade = 100: This tells R we want to bisect the curve at X = 100 lower.tail = TRUE: This tells R we want to measure the area of the curve where X&lt;100. shadeDist(xshade = 100, ddist = &quot;dnorm&quot;, parm1 = mu, parm2 = sigma, lower.tail = TRUE) When we run the curve, we see that approximately 4.8 percent of the population has a yield value less than 100. If we were randomly subsets of this field, we would expect to measure a yield less than 100 in about 4.8 our of 100 samples. In other words, there is about a 4.8% probability our subset would have a value less than 100. What is the probability our subset would have a value less than 140? shadeDist(xshade = 140, ddist = &quot;dnorm&quot;, parm1 = mu, parm2 = sigma, lower.tail = TRUE) Our probability is about 35%. 2.4.5 Probabilities in Upper Tails If we want to measure the probability of higher values, we just need to change the lower.tail argument to lower.tail = FALSE. The probability of values gerter than 185 would be about 14%. shadeDist(xshade = 185, ddist = &quot;dnorm&quot;, parm1 = mu, parm2 = sigma, lower.tail = FALSE) The probability greater than 170 would be about 28%. shadeDist(xshade = 170, ddist = &quot;dnorm&quot;, parm1 = mu, parm2 = sigma, lower.tail = FALSE) 2.4.6 Probabilities Within A Range Sometimes we would like to measure the probability of a range of values in the middle of our distribution. To do this, we change our xshade argument to xshade = c(120, 170). We use c() whenever we are giving R a set of values to work with. For xshade, this set only has two values: the lower and upper X values for our area. shadeDist(xshade = c(120, 170), ddist = &quot;dnorm&quot;, parm1 = mu, parm2 = sigma, lower.tail = FALSE) The probability of observing individuals with yields between 120 and 170 is about 57%. What about between 136 and 182? shadeDist(xshade = c(136, 182), ddist = &quot;dnorm&quot;, parm1 = mu, parm2 = sigma, lower.tail = FALSE) The probability is about 53%. 2.4.7 Probabilities Outside a Range Finally, what if we wanted to measure the probability of observing a value outside the ranges in the previous section. To do this, we use the lower.tail=TRUE argument to the shadeDist function. shadeDist(xshade = c(120, 170), ddist = &quot;dnorm&quot;, parm1 = mu, parm2 = sigma, lower.tail = TRUE) The probability of observing individuals with yields less than 120 or greater than 170 is about 43%. What about outside 136 and 182? shadeDist(xshade = c(136, 182), ddist = &quot;dnorm&quot;, parm1 = mu, parm2 = sigma, lower.tail = TRUE) The probability of observing individuals with values less than 136 or greater than 182 is about 47%. 2.4.8 Probability and SD We learned during the lesson that about 68% of individuals would be expected to have values with one standard deviation of the mean. We can use shadeDist to verify that. First we need to determine the values for the mean minus one standard deviation and the mean plus one standard deviation. Given that we assigned the mean and standard deviation to the variables mu and sigma above, this is easy. mu_minus_sigma = mu-sigma mu_plus_sigma = mu+sigma print(mu_minus_sigma) ## [1] 120.7797 print(mu_plus_sigma) ## [1] 183.0603 We can see the population mean minus one standard deviation is about 120.8, and the mean plus one standard deviation is about 183.1 since we have assigned these variables to mu_minus_sigma, and mu_plus_sigma, we can pass them directly to the shadeDist function. Finally, we need to use lower.tail=FALSE to tell shadeDist to calculate the percentage of values between these two numbers. shadeDist(xshade = c(mu_minus_sigma, mu_plus_sigma), ddist = &quot;dnorm&quot;, parm1 = mu, parm2 = sigma, lower.tail = FALSE) We confirm the percentage of observations between the population mean minus one standard deviation and the population mean plus one standard deviation is about 68%. We also learned that about 95% of the population occurs between 1.96 standard deviations below the mean and 1.96 standard deviations above the mean. We can calculate these values as above. mu_minus_1.96_sigma = mu-(1.96*sigma) mu_plus_1.96_sigma = mu+(1.96*sigma) print(mu_minus_1.96_sigma) ## [1] 90.88502 print(mu_plus_1.96_sigma) ## [1] 212.955 We can see the population mean minus one standard deviation is about 90.9, and the mean plus one standard deviation is about 213.0. shadeDist(xshade = c(mu_minus_1.96_sigma, mu_plus_1.96_sigma), ddist = &quot;dnorm&quot;, parm1 = mu, parm2 = sigma, lower.tail = FALSE) We have confirmed 95% of individuals are between 1.96 standard deviations below and above the mean. 2.4.9 Practice: Cotton Dataset Lets load the cotton data. cotton = read.csv(&quot;data-unit-2/exercise_data/cotton_uniformity.csv&quot;) And examine its top six rows. head(cotton) ## col row yield block ## 1 1 1 1.23 B1 ## 2 2 1 0.90 B1 ## 3 3 1 0.97 B1 ## 4 4 1 1.10 B1 ## 5 5 1 0.99 B1 ## 6 6 1 1.43 B1 The data we want are in the yield column. Lets isolate those and plot them in a histogram. cotton_yield = cotton$yield hist(cotton_yield) Lets calculate our cotton mean and standard deviation. cotton_mu = mean(cotton_yield) cotton_sigma = sd(cotton_yield) print(cotton_mu) ## [1] 1.001309 print(cotton_sigma) ## [1] 0.2287059 Now lets explore: What is the probability an individual in the cotton popualation has a value less than 0.7? (You should get a value of about 0.09, or 9%.) What is the probability an individual has a value greater than 1.4? (You should get a value around 0.4, or 4%.) What is the probability of an individual having a value between 0.7 and 1.4? (Answer: about 13%) What is the probability of an individual having a value less than 0.7 or greater than 1.4? (Answer: about 87%) 2.4.10 Practice: Tomato Dataset Analyze the tomato_uniformity.csv dataset on your own. First, load the dataset: tomato = read.csv(&quot;data-unit-2/exercise_data/tomato_uniformity.csv&quot;) What is the probability an individual will have a value less than 40? (Answer: about 16%) What is the probability an individual will have a value greater than 60? (Answer: about 18%) What is the probability an individual will have a value greater than 40 but less than 60? (Answer: about 66%) What is the probability an individual will have a value less than 40 but greater than 60? (Answer: about 34%) "],["sample-statistics.html", "Chapter 3 Sample Statistics 3.1 Samples 3.2 Case Study 3.3 Distribution of Sample Means 3.4 Central Limit Theorem 3.5 Standard Error 3.6 Degrees of Freedom 3.7 The t-Distribution 3.8 Confidence Interval 3.9 Confidence Interval and Probability 3.10 Exercise: Standard Error 3.11 Exercise: t-Distribution 3.12 Exercise: Confidence Interval for Sample Mean", " Chapter 3 Sample Statistics In the previous two units, we studied populations and how to summarise them with statistics when the entire population was measured. In other words, the measurce of center (the mean) and measure of spread (standard deviation) were the summary of all observations. In the case of yield monitor, these are appropriate statistics. In most every other agricultural reality, however, we cannot measure every individual in a population. Instead, we only have enough sample the population, that is, measure a subset of individuals from the population. This, of course, raises questions. Was the sample (our subset) representative of the population? If we took another random sample, would we calculate a similar mean or standard deviation? And, perhaps, how far off could the mean of our sample be from the true population mean? In other words, there is always uncertainty that statistics calculated from samples represent the true values of a population. You might even say we lack complete confidence that a mean value calculated from a sample will closely estimate the mean of a population. Enter statistics. We can measure the variance of sample means to estimate the distribution of sample means around the true population mean. Indeed, this is fundamental concept of research and statistics  using the measured variance of sample statistics to determine how accurate the are in predicting population statistics. 3.1 Samples To measure the variation of sample means, we need at least two samples to compare. Ideally we can gather even more. As we will see, the more samples included in our estimates of the population mean, the more accurate we are likely to be. A second comment, which may seem intuitive  but at the retail level may be overlooked  is randomization. Samples, for example individual plants, or areas where yield will be measured, are ideally selected at random. In reality, the plants or areas selected for measures may be less than random. When I used to count weed populations, we used square quadrats (frames) to consistently define the area that was measured. We would throw them into different areas of the plot and count weeds where ever they landed. The most important thing about selecting samples, however, is that the researcher work to minimize bias. Bias is when the samples selected consistently overestimate or underestimate the population mean. The most aggregious example of this would be a researcher who consistently and purposely sampled the highest- or lowest-measuring parts of a field. But bias can enter in other ways. For example, if our weed populations were very uneven, our thrown quadrat might be more likely to skid to a stop in weedy areas. A researcher might unconsciously choose taller plants to sample. In August, we might be tempted to sample a corn field from the edge than walk into that sweltering, allergenic hell. Remember, our goal is to represent a population as accurately and as unbiasedly as our resources allow. Accuracy means our sample means are close to the population mean. Unbiased means our sample means are equivalently scattered above and below the population mean. Accuracy versus Bias 3.2 Case Study Once more, we will work with the Iowa soybean yield dataset from Units 1 and 2. Lets review the structure of this dataset: head(yield) ## Simple feature collection with 6 features and 12 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -93.15033 ymin: 41.66641 xmax: -93.15026 ymax: 41.66644 ## Geodetic CRS: WGS 84 ## DISTANCE SWATHWIDTH VRYIELDVOL Crop WetMass Moisture Time ## 1 0.9202733 5 57.38461 174 3443.652 0.00 9/19/2016 4:45:46 PM ## 2 2.6919269 5 55.88097 174 3353.411 0.00 9/19/2016 4:45:48 PM ## 3 2.6263101 5 80.83788 174 4851.075 0.00 9/19/2016 4:45:49 PM ## 4 2.7575437 5 71.76773 174 4306.777 6.22 9/19/2016 4:45:51 PM ## 5 2.3966513 5 91.03274 174 5462.851 12.22 9/19/2016 4:45:54 PM ## 6 3.1840529 5 65.59037 174 3951.056 13.33 9/19/2016 4:45:55 PM ## Heading VARIETY Elevation IsoTime yield_bu ## 1 300.1584 23A42 786.8470 2016-09-19T16:45:46.001Z 65.97034 ## 2 303.6084 23A42 786.6140 2016-09-19T16:45:48.004Z 64.24158 ## 3 304.3084 23A42 786.1416 2016-09-19T16:45:49.007Z 92.93246 ## 4 306.2084 23A42 785.7381 2016-09-19T16:45:51.002Z 77.37348 ## 5 309.2284 23A42 785.5937 2016-09-19T16:45:54.002Z 91.86380 ## 6 309.7584 23A42 785.7512 2016-09-19T16:45:55.005Z 65.60115 ## geometry ## 1 POINT (-93.15026 41.66641) ## 2 POINT (-93.15028 41.66641) ## 3 POINT (-93.15028 41.66642) ## 4 POINT (-93.1503 41.66642) ## 5 POINT (-93.15032 41.66644) ## 6 POINT (-93.15033 41.66644) And map the field: plot(yield[&quot;yield_bu&quot;]) In Unit 2, we learned how to describe these data using the normal distribution model. We learned about how the area under the normal distribution curve corresponds to the proportion of individuals within a certain range of values. We also discussed how this proportion gave way to inferences about probability. For example, the area under the curve that corresponded with yields from 70.0 to 79.9 represented the proportion of individuals in the yield population. But it also represented the probability that, were you to measure selected areas at random, you would measure a yield between 70.0 and 79.9. 3.3 Distribution of Sample Means In the last unit, we sampled the yield from 1000 locations in the field and counted the number of observations that were equal to or greater than 70 and equal to or less than 80. What would happen if we only sampled from 1 location. What would be our sample mean and how close would it be to the population mean? set.seed(1776) yield_sample = sample(yield$yield_bu, 1) %&gt;% as.data.frame() names(yield_sample) = c(&quot;yield&quot;) ggplot(yield_sample, aes(x=yield)) + geom_histogram(fill=&quot;white&quot;, color=&quot;black&quot;) + geom_vline(xintercept = mean(yield$yield_bu), color = &quot;red&quot;) + geom_vline(xintercept = mean(yield_sample$yield), color = &quot;blue&quot;) + lims(x=c(55,105)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 2 rows containing missing values (geom_bar). What would happen if we only sampled twice? set.seed(1776) yield_sample = sample(yield$yield_bu, 2) %&gt;% as.data.frame() names(yield_sample) = c(&quot;yield&quot;) ggplot(yield_sample, aes(x=yield)) + geom_histogram(fill=&quot;white&quot;, color=&quot;black&quot;) + geom_vline(xintercept = mean(yield$yield_bu), color = &quot;red&quot;) + geom_vline(xintercept = mean(yield_sample$yield), color = &quot;blue&quot;) + lims(x=c(55,105)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 2 rows containing missing values (geom_bar). What would happen if we only sampled four times? set.seed(1776) yield_sample = sample(yield$yield_bu, 4) %&gt;% as.data.frame() names(yield_sample) = c(&quot;yield&quot;) ggplot(yield_sample, aes(x=yield)) + geom_histogram(fill=&quot;white&quot;, color=&quot;black&quot;) + geom_vline(xintercept = mean(yield$yield_bu), color = &quot;red&quot;) + geom_vline(xintercept = mean(yield_sample$yield), color = &quot;blue&quot;) + lims(x=c(55,105)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 2 rows containing missing values (geom_bar). What would happen if we only sampled 15 times? set.seed(1776) yield_sample = sample(yield$yield_bu, 15) %&gt;% as.data.frame() names(yield_sample) = c(&quot;yield&quot;) ggplot(yield_sample, aes(x=yield)) + geom_histogram(fill=&quot;white&quot;, color=&quot;black&quot;) + geom_vline(xintercept = mean(yield$yield_bu), color = &quot;red&quot;) + geom_vline(xintercept = mean(yield_sample$yield), color = &quot;blue&quot;) + lims(x=c(55,105)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 2 rows containing missing values (geom_bar). Nineteen times? set.seed(1776) yield_sample = sample(yield$yield_bu, 19) %&gt;% as.data.frame() names(yield_sample) = c(&quot;yield&quot;) ggplot(yield_sample, aes(x=yield)) + geom_histogram(fill=&quot;white&quot;, color=&quot;black&quot;) + geom_vline(xintercept = mean(yield$yield_bu), color = &quot;red&quot;) + geom_vline(xintercept = mean(yield_sample$yield), color = &quot;blue&quot;) + lims(x=c(55,105)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 2 rows containing missing values (geom_bar). Click on this link to access an app to help you further understand this concept: app_central_limit_theorem_normal 3.4 Central Limit Theorem The Central Limit Theorem states that sample means are normally distributed around the population mean. This concept is so powerful because it allows us to calculate the probability that that a sample mean is a given distance away from the population mean. In our yield data, for example, the Central Limit Theorem allows us to assign a probability that we would observe a sample mean of 75 bushels/acre, if the population mean is 80 bushels per acre. More on how we calculate this in a little bit. What is even more powerful about the Central Limit Theorem is that our sample means are likely to be normally distributed, even if the population does not follow a perfect normal distribution. Lets take this concept to the extreme. Suppose we had a population where every value occurred with the same frequency. This is known as a uniform distribution. Click on the following link to visit an app where we can explore how the sample distribution changes in response to sampling an uniform distribution: app_central_limit_theorem_uniform 3.5 Standard Error When we describe the spread of a normally-distributed population  that is, all of the individuals about which we want to make inferences  we use the population mean and standard deviation. When we sample (measure subsets) of a population, we again use two statistics. The sample mean describes the center of the samples.. The spread of the sample means is described by the standard error of the mean (often abbreviated to standard error). The standard error is related to the standard deviation as follows: \\[SE = \\frac{\\sigma}{\\sqrt n} \\] The standard error, SE, is equal to the standard deviation, divided by the square root of the number of samples. This denominator is very important  it means that our standard error grows as the number of samples increases. Why is this important? The sample mean is an estimate of the true population mean. The distribution around the sample mean describes not only the sample means, the range of possible values for the true mean. I realize this is a fuzzy concept. By studying the distribution of our sample values, we are able to describe the probability that the population mean is a given value. To better understand this, please visit this link: app_number_of_samples If you take away nothing else from this lesson, understand whether you collect 2 or 3 samples has tremendous implications for your estimate of the population mean. 4 samples is much better than 3. Do everything you can to fight for those first few samples. Collect as many as you can afford, especially if you are below 10 samples. 3.6 Degrees of Freedom In Unit 1 we first came across degrees of freedom, which was the number of observations in a population or sample, minus 1. Degrees of Freedom are again used below in calculating the t-distribution. So what are they and why do we use them. Turns out there are two explanations. In the first explanation, degrees of freedom refer to the number of individuals or samples that can vary independently given a fixed mean. So for an individual data point to be free, it must be able to assume any value within a given distribution. Since the population mean is a fixed number, only n-1 of the data are able to vary. The last data point is determined by the value of all the other data points, plus the population mean. Confusing, huh? Who starts measuring samples thinking that the data point is fixed, in any case? But if you think about it, the purpose of the sample is approximate a real population mean out there  which is indeed fixed. Its just waiting for us to figure it out. So if our sample mean is equal to the population mean (which we generally assume), then the sample mean is also fixed. But it is a very weird way of thinking. Yet this is the answer beloved by all the textbooks, so you should know about it. The second answer I like better: samples normally underestimate the true population variance. This is because the sample variance is calculated from the distribution of the data around the sample mean. Sample data will always be closer to the sample mean  which is by definition based on the data themselves  then the population mean. Think about this a minute. Your sample data could be crazy high or low compared to the overall population. But that dataset will define a mean, and the variance of the population will be estimated from that mean. In many cases, it turns out that using n-1 degrees of freedom will increase the value of the sample variance so it is closer to the population variance. 3.7 The t-Distribution In the last unit, we used the Z-distribution to calculate the probability of observing an individual of a given value in a population, given its population mean and standard deviation. Recall that about 68% of individuals were expected to have values within one standard deviation, or Z, of the population mean. Approximately 95% of individuals were expected to have values within 1.96 standard deviations of the population mean. Alternatively, we can ask what the probability is of observing individuals of a particular or greater value in the population, given its mean and standard deviation. We can ask a similar question of our sample data: what is the probability the population mean is a given value or greater, given the sample mean? As with the Z-distribution, the distance between the sample mean and hypothesized population mean will determine this probability. There is one problem, however, with using the Z-distribution: it is only applicable when the population standard deviation is known. When we sample from a population, we do not know its true standard deviation. Instead, we are estimating it from our samples. This requires we use a different distribution: the t-distribution. In comparison with the Z-distribution differs from the Z-distribution in that its shape changes as the number of samples increases. Notice in the animation above that when the number of samples is low, the distribution is wider and has a shorter peak. As the number of samples increase, the curve becomes narrower and taller. This has implications for the relationship between the distance of a hypothetical population mean from the sample mean, and the probability of it being that distant. We can prove this to ourselves using the shadeDist function in R that was introduced in the last unit. The first argument to this function is c(-1,1), which tells r how many standard errors to shade above and below the population mean (0 in this demonstration). The second argument, dt, simply tells R to use the t-distribution. library(fastGraph) shadeDist( c(-1, 1), &quot;dt&quot;, parm2 = 4, lower.tail = FALSE ) The last argument, lower.tail = FALSE, tells R to shade the area between the t-values and zero and calculate its probability. If we set that argument toTRUE\", R would shade the area beyond the t-values and calculate its probability. The third argument, parm1 = 2, requires greater explanation. 2 is the degrees of freedom. Whenever we use sample data, the degrees of freedom is equal to one less than the number of samples. In this example, 2 degrees of freedom means 3 samples were taken from the population. With 4 degrees of freedom, there is about a 63% probability the population mean is within 1 standard error of the mean. Lets decrease the sample mean to 3 degrees of freedom library(fastGraph) shadeDist( c(-1, 1), &quot;dt&quot;, parm2 = 3, lower.tail = FALSE ) With only 3 degrees of freedom (4 samples), there is only a 61% probability the population mean is within one standard error of the mean. Now change the parm2 from 3 to 1, which would be our degree of freedom if we only had two samples. You should see the probability that the population mean is within 1 standard error of the sample mean fall to 50%. Set parm2 to 10 degrees of freedom (11 samples), and the probability should increase to about 66%. Set parm2 to 30 degress of freedom, and the probability the population mean is within 1 standard error of the mean increases to 67%. When parm2 is 50 degrees of freedom (51 samples) the probability is about 68%. At this point, the t-distribution curve approximates the shape of the z-distribution curve. We can sum up the relationship between the t-value and probability with this plot. The probability of the popualation mean being within one standard error of the population mean is represented by by the red line. The probability of of the population mean being within 2 standard errors of the mean is represented by the blue line. As you can see, the probability of the population mean being within 1 or 2 standard errors of the sample mean increases with the degrees of freedom (df). Exact values can be examined by tracing the curves with your mouse. df = c(1:100) %&gt;% as.data.frame() names(df) = &quot;df&quot; p_from_tdf = df %&gt;% mutate(p1 = ((pt(1, df)) -0.5) * 2) %&gt;% mutate(p2 = ((pt(2, df)) -0.5) * 2) %&gt;% gather(t, p, p1, p2) %&gt;% mutate(t=gsub(&quot;p&quot;, &quot;&quot;, t)) p = p_from_tdf %&gt;% ggplot(aes(x=df, y=p, group=t)) + geom_point(aes(color=t)) ggplotly(p) Conversely, the t-value associated with a given proportion / probability will also decrease as the degrees of freedom increase. The read line represents the t-values that define the area with a 68% chance of including the population mean. The blue line represents the t-values that define the area with a 95% chance of including the population mean. Exact values can be examined by tracing the curves with your mouse. Notice the t-value associated with a 68% chance of including the population mean approaches 1, while the t-value associated with a 95% chance approaches about 1.98. df = c(2:100) %&gt;% as.data.frame() names(df) = &quot;df&quot; t_from_pdf = df %&gt;% mutate(t68 = qt(0.84, df)) %&gt;% mutate(t95 = qt(0.975, df)) %&gt;% gather(p, t, t68, t95) %&gt;% mutate(p=gsub(&quot;t&quot;, &quot;&quot;, p)) p = t_from_pdf %&gt;% ggplot(aes(x=df, y=t, group=p)) + geom_point(aes(color=p)) ggplotly(p) Takeaway: the number of samples affects not only the standard error, but the t-distribution curve we use to solve for the probability that a value will occur, given our sample mean. 3.8 Confidence Interval The importance of the number of samples the standard error, and the t-distribution becomes even more apparent with the use of confidence interval. A confidence interval is a range of values around the sample mean that are selected to have a given probability of including the true population mean. Suppose we want to define, based on a sample size of 4 from the soybean field above, a range of values around our sample mean that has a 95% probability of including the true sample mean. The 95% confidence interval is equal to the sample mean, plus and minus the product of the standard error and t-value associated with 0.975 in each tail: \\[CI = \\bar x + t \\times se\\] Where CI is the confidence interval, t is determined by the degrees of freedom, and se is the standard error of the mean Since the t-value associated with a given probability in each tail decreases with the degrees of freedom, the confidence interval narrows as the degrees of freedom increase  even when the standard error is unaffected. Lets sample our yield population 4 times, using the same code we did earlier # setting the seed the same as before means the same 4 samples will be pulled set.seed(1776) # collect 4 samples yield_sample = sample(yield$yield_bu, 4) #print results yield_sample ## [1] 82.40863 71.68231 73.43349 81.27435 We can then calculate the sample mean, sample standard deviation, and standard error of the mean. sample_mean = mean(yield_sample) sample_sd = sd(yield_sample) sample_se = sample_sd/sqrt(4) sample_mean ## [1] 77.1997 sample_se ## [1] 2.713572 We can then determine the t-value we need to construct our confidence interval and multiply it by our standard error to determine the confidence interval. To get the upper limit of the 95% confidence interval, we request the t-value above which only 2.5% of the samples are expected to exist. In other words, we ask R for the t-value below which 95% of the samples are expected to exist. # t-value associated with 3 df upper_t = qt(0.975, 3) upper_t ## [1] 3.182446 We can then add this to the sample mean to get our upper confidence limit. upper_limit = sample_mean + upper_t upper_limit ## [1] 80.38214 We can repeate the process to determine the lower limit. This time, however, we ask R for the t-value below which only 2.5% of the samples are expected to exist. lower_t = qt(0.025, 3) lower_t ## [1] -3.182446 lower_limit = sample_mean + lower_t lower_limit ## [1] 74.01725 You will notice that lower_t, the t-value that measures from the sample mean to the lower limit of the confidence interval, is just the negative of upper_t. Since the normal distribution is symmetrical around the mean, we can just determine the upper limit and use its negative as the lower limit of our confidence interval. Finally, we can put this all together and express it as follows. The confidence interval for the population mean, based on the sample mean is: \\[ CI = 80.2 \\pm 3.2 \\] We can also express the interval by its lower and upper confidence limits. \\[(77.0, 83.4)\\] We can confirm this interval includes the true population mean, which is 80.1. 3.9 Confidence Interval and Probability Lets return to the concept of 95% confidence. This means if we were to collect 100 sets of 4 samples each, 95% of them would estimate confidence intervals that include the true population mean. The remaining 5% would not. Click on this link to better explore this concept: app_confidence_interval Again, both the standard error and the t-value we use for calculating the confidence interval decrease as the number of samples decrease, so the confidence interval itself will decrease as well. Click on this link to better explore this concept: app_ci_width As the number of samples increases, the confidence interval shrinks. 95 out of 100 times, however, the confidence interval will still include the true population mean. In other words, as our sample size increases, our sample mean becomes less biased (far to either side of the population mean), and its accuracy (the proximity of the sample mean and population mean) increases. In conclusion, the greater the number of samples, the better our estimate of the population mean. In the next unit, we will use these concepts to analyze our first experimental data: a side by side trial where we will us the confidence interval for the difference between two treatments to test whether they are different. 3.10 Exercise: Standard Error Just as the standard deviation describes the distribution of individuals around a population mean, so the standard error of the mean describes the distribution of samples around their sample mean. In fact, the standard error of the mean is simply the standard deviation of the samples around the sample mean. 3.10.1 Case Study: Tomatoes Once again, we will work with the tomato, barley, cotton, and peanut datasets. Lets load the tomato dataset and look at its top six rows of data. tomato = read.csv(&quot;data-unit-3/exercise_data/tomato_uniformity.csv&quot;) head(tomato) ## row col yield ## 1 1 1 48 ## 2 2 1 61 ## 3 3 1 69 ## 4 4 1 59 ## 5 5 1 71 ## 6 6 1 46 3.10.2 Calculating Standard Error The standard error of the mean is equal to the population standard deviation, divided by the square root of the number of samples. We can calculate these as follows. To get the standard deviation, use the sd() function. yield = tomato$yield # define the column of interest tomato_sd = sd(yield) Then we can divide the standard deviation by the square root of n to get the standard error of the mean. What would be the standard error of our sample mean, if we took 4 samples? tomato_se = tomato_sd / sqrt(4) tomato_se ## [1] 5.249217 We will talk more about the t-distribution in the next exercise, but for now lets assume about 95% of the sample means should be within about two standard errors of the mean of their distribution. Two times the standard error is about 10.6, so we would expect 95% of sample means to be within 50.3 +/-10.6, or (39.7, 60.9). Dont worry about understanding the code below. Just run it and observe the plot. In the plot below, we have simulated 1000 sample means, each based on four samples, from the tomato yield population. We can see the range (39.7, 60.9) does include most of the population, with the exception of the tails. library(tidyverse) set.seed(2003) sample_list = list() for(i in c(1:1000)){ samples = mean(sample(yield, 4))%&gt;% as.data.frame() sample_list[[i]] = samples } sample_list_df = do.call(rbind.data.frame, sample_list) names(sample_list_df) = &quot;yield&quot; pop_mean = mean(yield) ggplot(sample_list_df, aes(x=yield)) + geom_histogram(fill=&quot;white&quot;, color=&quot;black&quot;) + geom_vline(xintercept = pop_mean, color = &quot;red&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. What would be the standard error of our sample mean if we took 7 samples? tomato_se = tomato_sd / sqrt(7) tomato_se ## [1] 3.968035 3.10.3 Practice: Barley Now lets practice calculating the standard error. What would be the standard error of our sample mean if we took 8 samples? Ill get you started. First, lets load the barley_uniformity.csv data and inspect the first six rows. barley = read.csv(&quot;data-unit-3/exercise_data/barley_uniformity.csv&quot;) head(barley) ## row col yield ## 1 20 1 185 ## 2 19 1 169 ## 3 18 1 216 ## 4 17 1 165 ## 5 16 1 133 ## 6 15 1 195 Next lets define the column of interest and create a histograma of its distribution. barley_yield = barley$yield hist(barley_yield) Remember, we need to know the standard deviation in order to calculate the standard error of the sample mean. barley_sd = sd(barley_yield) What would the standard error of the sample mean be if we took two samples? barley_se = barley_sd/sqrt(2) barley_se ## [1] 22.01951 The standard error woud be about 22.0. Try calculating additional standard errors. Here are some of the values you should get. 4 samples, se=15.6 5 samples, se=13.9 7 samples, se=11.8 3.10.4 Practice: Cotton Calculate the standard errors of sample means from the cotton_uniformity.csv dataset, for sets of 3, 4, and 6 samples. You should get: 3 samples = 0.13 4 samples = 0.11 6 samples = 0.09 3.11 Exercise: t-Distribution This week, we are introduced to the t-distribution. The t-distribution describes the distribution of sample means around their mean value. Contrast this with the Z-distribution, which describes the distribution of individuals around the population mean. The most critical way in which the t-distribution is different from the Z-distribution is the shape of the t-distribution changes shape with the number of samples used to calculate the sample mean. As the number of samples increases, the t-distribution narrows in width, and expands in height. 3.11.1 Plotting the Distribution In the lecture, we once again used the shadeDist function from the fastGraph package. Use install.packages(fastGraph) to install fastGraph in your account if you did not do so in the last unit. Then use library(fastgraph) to tell R to run that package in the current section. We can use the shadeDist function to observe how the difference of the t-Distribution changes with the number of samples taken. Say we want to see the t-distribution for a sample size of 2. We will shade the area from t=-1 to t=1 for reference. library(fastGraph) shadeDist(c(-1,1), &quot;dt&quot;, 1, lower.tail=FALSE ) # t with 15 d.f. and non-centrality parameter=3 Remember, we must provide four arguments to the shadeDist function. The first, c(-1,1), is the range of t-values. The second, dt, tells R we are modeling the t-distribution. The third, 3 is the degrees of freedom associated with t, which is always one less than the number of samples. the final argument, lower.tail=FALSE, tells R to shade the middle of the plot. Now lets change the number of samples to 4. shadeDist(c(-1,1), &quot;dt&quot;, 3, lower.tail=FALSE ) # t with 15 d.f. and non-centrality parameter=3 We can see the shaded area gets wider, meaning the distribution is getting narrower. In we increase to 6, the shape doesnt seem to change much, yet the proportion of the curve that is between t=-1 and t=1 increases. shadeDist(c(-1,1), &quot;dt&quot;, 5, lower.tail=FALSE ) # t with 15 d.f. and non-centrality parameter=3 If we increase to n=20, the curve continues to narrow. shadeDist(c(-1,1), &quot;dt&quot;, 19, lower.tail=FALSE ) # t with 15 d.f. and non-centrality parameter=3 3.11.2 Calculating T To create the plot above, we entered t-values, and the degree of freedom. R not only drew the plot, but calculate the probability of observing sample means within the given range. To calculate t, we need to supply the degrees of freedom and the target probability to which t should respond. We will use the qt function in R to do this. Lets say we want to know the t-values that, given four samples per mean, would be expected to include 50% of potential sample means? In that case, we are interested in the middle 50% of the distribution. There will be 25% of the distribution below this range, and 25% above. So, for the lower t-value, we will tell R to calculate the t-value below which 25% of sample means are expected to occur. qt(0.25,3) ## [1] -0.7648923 In the argument above, we told R to calculate the t-value associated with the lower 0.25 of the population, and 3 degrees of freedom (for 4 samples). For the upper t-value, we want the value where 75% of the distribution is lower. qt(0.75,3) ## [1] 0.7648923 As you can see, the upper and lower limit are symmetrical  they only differ in sign. We can use the shadeDist from above to check our work, by plugging in the t-values we just calculated. shadeDist(c(-0.7649,0.7649), &quot;dt&quot;, 3, lower.tail=FALSE ) # t with 15 d.f. and non-centrality parameter=3 As we can see, the defined proportion equals our target. What if we want the t-values that bind the middle 68% of potential sample means? Then we would have 32% of the distribution outside this range. 16% of sample means would be expected to be below the range, and 16% percent above. The t-value associated with the lower range, again assuming 3 df, would be: qt(0.16,3) ## [1] -1.188929 And the t-value associated with the upper range would be: qt(0.84,3) ## [1] 1.188929 How about the t-values that would be expected to define the range where 90% of sample means would be expected? qt(0.05,3) ## [1] -2.353363 qt(0.95,3) ## [1] 2.353363 And, finally, 95%? qt(0.025,3) ## [1] -3.182446 qt(0.975,3) ## [1] 3.182446 3.11.3 Practice Calculate the t-values that would define the middle 60% of potential sample means if there were 10 samples. The lower limit is: qt(0.2,9) ## [1] -0.8834039 What is the upper limit? Q: What is the range of t-values associated with 70% of potential sample means, if the number of samples is 12? A: (-1.09, 1.09) Q: What is the range of t-values associated with 80% of potential sample means, if the number of samples is 14? A: (-1.35, 1.35) Q: What is the range of t-values associated with 90% of potential sample means, if the number of samples is 17? A: (-1.75, 1.75) Q: What is the range of t-values associated with 95% of potential sample means, if the number of samples is 20? A: (-2.09, 2.09) Q: What is the range of t-values associated with 95% of potential sample means, if the number of samples is 20? A: (-2.86, 2.86) 3.12 Exercise: Confidence Interval for Sample Mean In the last part of this lesson, we learned how the standard error and t-distribution can be combined to define a confidence interval: a range of values around the sample mean that we are confident will, in a given percentage of trials, include the true population mean. The confidence interval is bound by upper and lower confidence limits that are the product of the t-value and standard error. 3.12.1 Calculating the Confidence Interval Once we understand how to calculate the standard error and t-value, the confidence interval is easily constructed: multiply the standard error times the t-value find the lower confidence limit by subtracting the product from step 1 from the sample mean find the upper confidence limit by adding the product from step 1 to the sample mean report the confidence interval in parentheses, like this (lower confidence limit, upper confidence limit) 3.12.2 Case Study: Peanut Sample 1 In the data folder there are nine sample sets, of different size from the peanut trial. Lets load the first peanut = read.csv(&quot;data-unit-3/exercise_data/peanut_sample_1.csv&quot;) head(peanut) ## sample_no yield ## 1 1 2.30 ## 2 2 1.38 Lets create a new variable, yield, from the yield column in the peanut data frame. yield = peanut$yield First, lets calculate the sample mean and standard deviation. yield_mean = mean(yield) yield_sd = sd(yield) Remember how to calculate the standard error? Right, divide the standard deviation by the square root of the number of samples. In this case, we know it is 2, but if we were dealing with a larger sample, it might be easier to let excel do the counting. So lets use the length() argument to do that. no_samples = length(yield) Finally, we can calculate the standard error. yield_se = yield_sd/sqrt(no_samples) Now, we need to calculate the t-value to use in calculating the confidence interval. For this first example, we want a 90% confidence interval. So we need to tell R to calculate the value of t that leaves 5% of the distribution in each tail. That means the upper tail will begin at 100% - 5, or 95%. We can now use the qt function in R to calculate t. t_value = qt(0.95, 1) Where did the 1 come from? Remember, that is the degrees of freedom, which is equal to the number of samples minus 1. We have two samples in this first example, thus we have 1 degree of freedom. We now know that our standard error is 0.46 and our t_value is about 6.31. The last step is to add and subtract the product of the standard error and t-value from the sample mean. lower_limit = yield_mean - (yield_se*t_value) upper_limit = yield_mean + (yield_se*t_value) lower_limit ## [1] -1.064326 upper_limit ## [1] 4.744326 So our confidence interval is (-1.06, 4.74). Now we know the yield cannot possibly be less than zero. But because the sample mean is close to zero, and because our sample size is so small, the confidence interval is so wide that its lower limit is negative. This illustrates an important part of statistics or data science  never underestimate the importance of domain knowledge, that is, your knowledge of the science it is trying to represent. One last thing: whenever you report a confidence interval, you should report its confidence level and degrees of freedom, too. So we would report the above as: CI(0.90, 1) = (-1.06, 4.74) 3.12.3 Case Study: Peanut Sample 2 Lets go through this one a little faster. peanut_2 = read.csv(&quot;data-unit-3/exercise_data/peanut_sample_2.csv&quot;) head(peanut_2) ## sample_no yield ## 1 1 2.23 ## 2 2 1.48 ## 3 3 2.20 ## 4 4 1.71 ## 5 5 1.88 First, lets calculate the sample mean, standard deviation, and standard error. yield_2 = peanut_2$yield yield_mean_2 = mean(yield_2) yield_sd_2 = sd(yield_2) no_samples_2 = length(yield_2) yield_se_2 = yield_sd_2/sqrt(no_samples_2) This time we want a 95% confidence interval, so we want our distribution to have 2.5% in the top tail. 100% - 2.5% = 97.5%. We have 5 samples - 1 = 4 degrees of freedom t_value_2 = qt(0.975, 4) Finally, add and subtract the product of the standard error and t-value from the sample mean. lower_limit_2 = yield_mean_2 - (yield_se_2*t_value_2) upper_limit_2 = yield_mean_2 + (yield_se_2*t_value_2) lower_limit_2 ## [1] 1.501602 upper_limit_2 ## [1] 2.298398 Our confidence interval is now CI(0.95, 4) = (1.50, 2.30) 3.12.4 Practice Datasets peanut_sample_3.csv through peanut_sample_9.csv are available for your practice. The answers for the 95% confidence intervals are given below. # change the sample number in the code below to access datasets 3 through 9 peanut = read.csv(&quot;data-unit-3/exercise_data/peanut_sample_3.csv&quot;) peanut_mu = mean(peanut$yield) peanut_sd = sd(peanut$yield) peanut_no_samples = length(peanut$yield) peanut_se = peanut_sd / sqrt(peanut_no_samples) df=peanut_no_samples-1 peanut_t = qt(0.975, df=df) upper_conf_limit = peanut_mu + (peanut_se*peanut_t) lower_conf_limit = peanut_mu - (peanut_se*peanut_t) paste0(&quot;(&quot;, round(lower_conf_limit,2), &quot;, &quot;, round(upper_conf_limit,2), &quot;)&quot;) ## [1] &quot;(1.95, 2.41)&quot; peanut_sample_3.csv: (1.95, 2.41) peanut_sample_4.csv: (1.98, 2.34) peanut_sample_5.csv: (1.86, 2.65) peanut_sample_6.csv: (1.98, 2.57) peanut_sample_7.csv: (2.09, 2.42) peanut_sample_8.csv: (2.12, 2.49) peanut_sample_9.csv: (1.42, 2.15) "],["two-treatment-comparisons.html", "Chapter 4 Two-Treatment Comparisons 4.1 Side-by-Side Trials 4.2 Blocked Design 4.3 Case Study 4.4 Confidence Interval 4.5 T-Test 4.6 Exercise: Randomizing Plots 4.7 Exercise: Restructuring Columns and Rows 4.8 Exercise: Confidence Interval of Difference\" 4.9 Exercise: T-Test", " Chapter 4 Two-Treatment Comparisons Until now, we have worked with a single population. To recap our progress: - In Unit 1, we defined a population. We learned it was the complete group of individuals which we wanted to describe or for which we wanted to make a prediction. We learned how to describe the center of this population with the population mean, and how to describe its spread using the sum of squares, variance, and standard deviation. - In Unit 2, used the normal distributionto describe the pattern with which individuals are spread in many populations: individuals with values closer to the population mean were expected to be observed much more frequently that individuals with values that were more extreme. We learned to use probability toe quantify the likelihood of encountering an invididual within a particular range of values. - In Unit 3, we used samples, subsets from a population, in place of measuring each individual in that population. We saw how sample means were distributed normally, from both normally- and non-normally distributed populations. The standard error was used to describe the spread of individual samples around the sample mean. The distribution of sample means was described by the t-distribution, which was shorter and wider when the number of samples was low, and taller and narrower when the number of samples was greater. 4.1 Side-by-Side Trials In this unit, we will finally put our statistical knowledge to work to test treatment differences. We will work with a simple but important experimental design  the two-treatment comparison. Here in Ohio, this is referred to as a side-by-side trial, but you may have a different term for it where you work. If you work for or in support of an agronomy retailer, you have probably conducted these trials. Typically, you would split one or more fields into treated and untreated fields. For example, you might stripe and individual field with treated and untreated areas: Or, you might divide multiple fields in half like this: In either case, however, a side-by-side deals with two treatments and can be easily analyzed using a t-test. In these next two units we will compare different designs for side-by-side (or paired) trials, use Excel and R to randomly assign treatments, understand how the t-distribution allows us to test for significance, and run these tests in R. 4.2 Blocked Design In Unit 1 we learned the hallmarks of a designed trial are the randomzation and replication of treatments. Each treatment should be observed several times in different experimental units. Often in our work experimental unit is a fancy word for a plot, half a field, or a pot. Observing the treatment several times has two benefits. First, the average of those observations  the sample average  wil likely be closer to the true population average for that treatment than the individual observations. Second, variation among the observations can be used to give us a sense how much environmental factors outside, in contrast with our treatment, cause our observations to vary. It is also important to randomize treatments in order to avoid intentional or unintentional biases that might skew our interpretation of results. For example, one treatment might be biased by always putting it to the north of another treatment or, more insiduously, always putting it in the better half of a field. Deliberate randomization reduces the probability of either scenario. That said, reality often intervenes intervenes. Soil types change across a field, as do microclimates around field edges, management histories, etc. Though randomization will reduce the likelihood that treatments are concentrated in one of these areas, it may not result in as even a distribution of treatments across the experimental area as we would like. Even with randomization, we could conceivably with all the treatments in one half of the field: Or, if we are using multiple fields, both halves of a field could receive the same treatment in a randomized design Blocked experimental designs, place a restriction on the random assignment of treatments to plots. Instead of assigning treatments randomly within a field or across a state. We instead force both treatments to occur within a field section or in the same field, so that our treatment maps look more those we showed in the first two figures of this unit. Here is another way to think about this. Our statistical test this week is based on comparing two populations. Each population will receive a different treatment. We want to design our experiment so that aside from the treatment, the two populations are as identical as possible. The blocked approach, in general, helps create two populations that are similar. 4.3 Case Study An in-furrow corn starter (6-24-6 plus proprietary micronutrients) was tested against a control (no -starter) in a trial in western Ohio. Treatments were blocked (paired) so that each treatment occurred once per block. Plot numbers are given on the x-axis of the map below. There were 10 blocks. The data are in a data.frame named corn_starter. corn_starter = read.csv(&quot;data-unit-4/corn_starter.csv&quot;) Examine the corn_starter data.frame using the head function. head(corn_starter) ## block plot treatment yield ## 1 1 11 Starter 193.4 ## 2 1 12 Control 194.2 ## 3 2 21 Starter 192.2 ## 4 2 22 Control 189.0 ## 5 3 31 Control 193.8 ## 6 3 32 Starter 194.2 Note the column headings: trt refers to the treatment level (Control or Starter) and value refers to the measured yield. Lets make a quick histogram plot of the data using ggplot. For the first time, in this unit, we are working with two populations: one which received no starter (the control), and another which received starter. So we will have separate histograms for both populations. ggplot(data = corn_starter, aes(x=yield)) + geom_histogram() + facet_grid(~treatment) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. In the above code, ggplot launches the function to plot our data. Also on the first line, aes specifies the aesthetics to be plotted  these are the variable(s) that define where objects are positioned in the plot. In this case, yield determines which bin an observation is placed into, so x=value. The next line tells R to draw a histogram. The last line tells R to construct a series of plots, one for each level of treatment. This way, we can compare the distribution of the Control population with that of the Starter population. The mean yield of the population that population that received starter is greater than the center of the population that received the control (no starter). corn_starter %&gt;% group_by(treatment) %&gt;% summarise(yield = mean(yield)) ## # A tibble: 2 x 2 ## treatment yield ## &lt;chr&gt; &lt;dbl&gt; ## 1 Control 190. ## 2 Starter 196. Another way to look at this would be to create a new population from the differences between starter and control within each pair. This can be confusing, so lets slow it down. We created a new population by subtracting the yield of the control plot from the yield of the starter plot in each pair. In this new population, individuals with a positive value indicate a block where the starter outyielded the control. Individuals with a negative value indicate a block where the control outyielded the starter. We will now work with this new population of differences. We see most of the individuals in this new population are greater than 0. The population mean is 5.6 bushels/acre. What we want to know is: given the mean we observed (5.59 bushels/acre) and the distribution of this population, what is the probability, were we to repeat the trial, that the population mean would be zero. A population mean of zero would mean the starter yield and control yield were the same, i.e. there is no difference between the starter and control. 4.4 Confidence Interval We can use a confidence interval to determine whether the range of values we are likely to observe includes zero. Recall, to calculate the confidence interval we need two values: the minimal t-value that is associated with our degrees of freedom and encompases 95% of our population, and the standard error of the difference. We calculate the minimal t-value using the qt function in R. min_t = qt(0.975, df=9) Remember, degrees of freedom is equal to n-1. Our new population has 10 degrees of freedom, thereofre, df equals 9 above. We also need to know the standard error of the population. Given our population is composed of treatment differences, we will call this statistic the standard error of the difference. sd = sd(corn_differences$difference) sed = sd/sqrt(10) pop_mean = mean(corn_differences$difference) In the code above, we first calculated the standard deviation, sd, of of our differences. We divided the standard deviation by the square root of the number of observations to get the standard error of the difference. Our population mean is 5.59 and the standard error of the difference is about 1.71 We additionally calculated pop_mean, the mean of our population. We can now calculate the confidence interval, using min_t and sed: upper_limit = pop_mean + (min_t * sed) lower_limit = pop_mean - (min_t * sed) CI = paste(lower_limit, upper_limit, sep=&quot;,&quot;) CI ## [1] &quot;1.72421086829983,9.45578913170016&quot; The 95% confidence interval ranges from 1.72 to 9.46 bushels per acre and, notably, does not include zero. Since zero is outside the 95% confidence interval, there is greater than a 95% probability the population mean is not equal to zero. Another way of saying this is there is less than a 5% probability that the population mean is equal to zero. Or, finally, the population mean is significantly different from zero at the 5% (or 0.05) level. Going our population was composed of the difference of starter and control, we can say the starter effect is significantly different from the control at the 5% level. 4.5 T-Test An alternative to the confidence interval is the t-test. The first step of the t-test is to calculate our observed t-value: \\[ t = \\frac{\\bar{x} - \\mu}{SED} \\] Where \\(\\bar{x}\\) is the observed population mean, \\(\\mu\\) is the hypothesized population mean (usually 0, meaning no treatment difference), and \\(SED\\) is the standard error of the difference. In our example above: \\[ t = \\frac{5.59 - 0}{1.71} = 3.27\\] Our observed t-value is about 3.27. If there were no difference between the starter and the control, of course, t would be equal to zero. So there is a difference between our observed mean and zero of \\(t=3.27\\). Using R, we can quickly calculate the probability of observing a value \\(t=3.27\\) above  or below our population mean of 5.59. library(fastGraph) shadeDist(xshade=c(-3.27, 3.27), &quot;dt&quot;, parm2 = 9, lower.tail = TRUE) Note that this time with this function we used the argument lower.tail=TRUE. This tells R to calculate the probability of values futher than \\(t=3.27\\) both above and below the mean. Again there were 10 observed differences in our population, so there were 9 degrees of freedom. The value that is returned is the probability the population mean is actually zero, given the population mean. The probability of this t-value (sometimes abbreviated as \\(Pr \\ge \\lvert t \\lvert\\)) is very low, less tan 0.01, or 1%. 4.6 Exercise: Randomizing Plots Plot randomzation is a very important  and very easy to do with R. Below we will learn how to randomize plots for side-by-side trials in which plots are paired. 4.6.1 Example 1 For our first example, we create an experimental design with 4 replications of treatments A and B. We will use the agricolae package to do this. The agricolae package is 4.6.2 The Agricolae Package To randomize two-treatment trials with paired treatments, we will use the agricolae package of R. library(agricolae) 4.6.3 Randomizing the Plot We will use the design.rcbd function from the agricolae package. We need to supply the following arguments to this function: - trt: a vector with treatment names - r: the number of replications We will create a vector called treatment_names, containing treatments A and B. We will use the argument r=4 to tell R to generate 4 blocks. treatment_name=c(&quot;A&quot;, &quot;B&quot;) plot_plan = design.rcbd(trt = treatment_name, r=4) 4.6.4 Working with Lists The code above created an object called plot_plan. So far in R we have dealt with two kinds of objects: a data frame, which is basically a table of values, and a vector, which can be thought of as a single column of numbers. treatment_names above is an example of a vector. plot_plan is a third type of R object, called a list. A list is a collection of other R objects. It can contain multiple data frames, vectors, single values, etc. A list is a very useful way to bundle up information, but they can be a little tricky to unpack. If we just run plot_plan below, we will get two windows. One will list all of the objects in the list. The second window will show the plot plan. plot_plan ## $parameters ## $parameters$design ## [1] &quot;rcbd&quot; ## ## $parameters$trt ## [1] &quot;A&quot; &quot;B&quot; ## ## $parameters$r ## [1] 4 ## ## $parameters$serie ## [1] 2 ## ## $parameters$seed ## [1] -1586367598 ## ## $parameters$kinds ## [1] &quot;Super-Duper&quot; ## ## $parameters[[7]] ## [1] TRUE ## ## ## $sketch ## [,1] [,2] ## [1,] &quot;B&quot; &quot;A&quot; ## [2,] &quot;B&quot; &quot;A&quot; ## [3,] &quot;B&quot; &quot;A&quot; ## [4,] &quot;B&quot; &quot;A&quot; ## ## $book ## plots block treatment_name ## 1 101 1 B ## 2 102 1 A ## 3 201 2 B ## 4 202 2 A ## 5 301 3 B ## 6 302 3 A ## 7 401 4 B ## 8 402 4 A We can access this information more directly. If we type plot_plan$sketch, we get a plot map of sorts. plot_plan$sketch ## [,1] [,2] ## [1,] &quot;B&quot; &quot;A&quot; ## [2,] &quot;B&quot; &quot;A&quot; ## [3,] &quot;B&quot; &quot;A&quot; ## [4,] &quot;B&quot; &quot;A&quot; If we type plot_plan$book, we get a data frame with the plots, block, and treatment_names. plot_plan$book ## plots block treatment_name ## 1 101 1 B ## 2 102 1 A ## 3 201 2 B ## 4 202 2 A ## 5 301 3 B ## 6 302 3 A ## 7 401 4 B ## 8 402 4 A 4.6.5 Example 2 In a field trial, we wish to compare the biomass of two fiber hemp varieties, Jerry and Bob. We wish to design a paired experimental design with 10 replications. hemp_variety = c(&quot;Jerry&quot;, &quot;Bob&quot;) hemp_plot_plan = design.rcbd(trt = hemp_variety, r=10) Here is the plot layout: hemp_plot_plan$sketch ## [,1] [,2] ## [1,] &quot;Jerry&quot; &quot;Bob&quot; ## [2,] &quot;Jerry&quot; &quot;Bob&quot; ## [3,] &quot;Bob&quot; &quot;Jerry&quot; ## [4,] &quot;Bob&quot; &quot;Jerry&quot; ## [5,] &quot;Jerry&quot; &quot;Bob&quot; ## [6,] &quot;Jerry&quot; &quot;Bob&quot; ## [7,] &quot;Bob&quot; &quot;Jerry&quot; ## [8,] &quot;Jerry&quot; &quot;Bob&quot; ## [9,] &quot;Bob&quot; &quot;Jerry&quot; ## [10,] &quot;Jerry&quot; &quot;Bob&quot; And here is the table we can cut and paste into our spreadsheet: hemp_plot_plan$book ## plots block hemp_variety ## 1 101 1 Jerry ## 2 102 1 Bob ## 3 201 2 Jerry ## 4 202 2 Bob ## 5 301 3 Bob ## 6 302 3 Jerry ## 7 401 4 Bob ## 8 402 4 Jerry ## 9 501 5 Jerry ## 10 502 5 Bob ## 11 601 6 Jerry ## 12 602 6 Bob ## 13 701 7 Bob ## 14 702 7 Jerry ## 15 801 8 Jerry ## 16 802 8 Bob ## 17 901 9 Bob ## 18 902 9 Jerry ## 19 1001 10 Jerry ## 20 1002 10 Bob 4.7 Exercise: Restructuring Columns and Rows As we move into the treatment comparisons part of our course, it is a good time to talk about how datasets are structured. Datasets can have a \"wide structure, in which the values for different treatments are listed side by side: library(tidyverse) Trt_A = c(1:4) Trt_B = c(5:8) Rep = c(1:4) wide_df = cbind(Rep, Trt_A, Trt_B) %&gt;% as.data.frame() wide_df ## Rep Trt_A Trt_B ## 1 1 1 5 ## 2 2 2 6 ## 3 3 3 7 ## 4 4 4 8 In general, however, we prefer data be in the long form, so that each observation has its own row in the dataset. In the long form, the data above would be in three columns, the first listing the rep, the second listing the treatment, and the third listing the observed value for that treatment. To get the data into the correct form, we need to pivot or transpose it: that is, certain columns will become rows, or vice versa. Fortunately, this is easy to do with the tidyverse package in R. The tidyverse and tidy data concept are yet another product from Hadley Wickham, who is an alumnus of Iowa State. Lets run the tidyverse package: library(tidyverse) The tidyverse package introduces us to a new feature in R: pipes. Pipes are connections between lines of code that allow us to work on the same dataset with several lines of code at once. Pipes are signified with the %&gt;% string of code. To make our dataset into the long format, we use the gather function to collect the four treatment rows into two rows, one identifying the treatment and the other with the value. gather requires three arguments: 1) the name of the column that will identify the data (often called a key), 2) the observed values, and 3) the columns to be reorganized in the key and value columns. long_df = wide_df %&gt;% gather(Treatment, Value, Trt_A:Trt_B) long_df ## Rep Treatment Value ## 1 1 Trt_A 1 ## 2 2 Trt_A 2 ## 3 3 Trt_A 3 ## 4 4 Trt_A 4 ## 5 1 Trt_B 5 ## 6 2 Trt_B 6 ## 7 3 Trt_B 7 ## 8 4 Trt_B 8 In the code above, we created a new data frame, long_df, from the wide_df data frame. We used the gather function to create two new columns, Treatment and Value, to contain the treatment name and value associated with each observation. The pipe (%&gt;%) told the gather function to work with the wide_df data. What if we had a long dataset that we wanted to be wide, so that each treatment had its own column? Then we would use the spread function. Spread takes two arguments: the column used to identify the values, and the column with the value. Again, the pipe connects the two lines of code. wide_again_df = long_df %&gt;% spread(Treatment, Value) wide_again_df ## Rep Trt_A Trt_B ## 1 1 1 5 ## 2 2 2 6 ## 3 3 3 7 ## 4 4 4 8 4.7.1 Operations on Data One of the reasons we may want to restructure data in the wide format is so we can conduct a new statistic from the original values. For example, as we learn this week, we may want to model the distribution of the differences between Trt A and Trt B. Once the data is in the wide format above, we calculcate these differences very quickly. In R, we use the mutate function to build new columns. The argument to mutate is usually a mathematical formula. Say we want to cacluate the differences, row by row, between Trt A and Trt B. We would use the following mutate command. trt_differences = wide_again_df %&gt;% mutate(Trt_Diff = Trt_A - Trt_B) trt_differences ## Rep Trt_A Trt_B Trt_Diff ## 1 1 1 5 -4 ## 2 2 2 6 -4 ## 3 3 3 7 -4 ## 4 4 4 8 -4 We now have a new column of the differences between Trt_A and Trt_B. 4.7.2 Case Study: Soybean Fungicide Soybean treated with fungicide applied at R3 was compared with soybean that was untreated. Data are in the soybean_funcicide.csv soybean = read.csv(&quot;data-unit-4/exercise_data/soybean_fungicide.csv&quot;) soybean ## Block Treatment Yield ## 1 1 A 71.5 ## 2 1 B 75.1 ## 3 2 A 73.5 ## 4 2 B 77.7 ## 5 3 B 77.7 ## 6 3 A 72.4 ## 7 4 B 78.2 ## 8 4 A 75.0 ## 9 5 A 76.4 ## 10 5 B 75.9 ## 11 6 A 76.5 ## 12 6 B 77.5 ## 13 7 B 80.7 ## 14 7 A 74.8 ## 15 8 A 75.6 ## 16 8 B 78.4 ## 17 9 B 80.7 ## 18 9 A 73.2 ## 19 10 B 76.9 ## 20 10 A 74.6 This dataset is currently in the long form. To calculate the differenced between Treatments A and B, we need need to convert it to the wide form. soybean_wide = soybean %&gt;% spread(Treatment, Yield) soybean_wide ## Block A B ## 1 1 71.5 75.1 ## 2 2 73.5 77.7 ## 3 3 72.4 77.7 ## 4 4 75.0 78.2 ## 5 5 76.4 75.9 ## 6 6 76.5 77.5 ## 7 7 74.8 80.7 ## 8 8 75.6 78.4 ## 9 9 73.2 80.7 ## 10 10 74.6 76.9 We now have the two treatments in columns A and B. We can then calculate the difference betwee Treatment A and Treatment B using the mutate function.. diffs = soybean_wide %&gt;% mutate(diff = A-B) diffs ## Block A B diff ## 1 1 71.5 75.1 -3.6 ## 2 2 73.5 77.7 -4.2 ## 3 3 72.4 77.7 -5.3 ## 4 4 75.0 78.2 -3.2 ## 5 5 76.4 75.9 0.5 ## 6 6 76.5 77.5 -1.0 ## 7 7 74.8 80.7 -5.9 ## 8 8 75.6 78.4 -2.8 ## 9 9 73.2 80.7 -7.5 ## 10 10 74.6 76.9 -2.3 4.7.3 Practice 1 The dataset darwin.csv is from a trial by Charles Darwin in which he compared the heights of progeny of corn plants that were self-fertilized and cross-fertilized. Three pairs of plants were grown per pot. Measurements are in inches. darwin = read.csv(&quot;data-unit-4/exercise_data/darwin.csv&quot;) head(darwin) ## pot pair type height ## 1 I a cross 23.500 ## 2 I a self 17.375 ## 3 I b cross 12.000 ## 4 I b self 20.375 ## 5 I c cross 21.000 ## 6 I c self 20.000 The data are currently in the long format. We want to convert it to the wide format, where each treatment has its heights in a separate column. Re-structure the type and height columns using the spread_function. The first few rows of the resulting dataset should look like: I a 23.500 17.375 I b 12.000 20.375 I c 21.000 20.000 II d 22.000 20.000 4.7.4 Practice 2 These data are from a trial that compared apple genotypes. Calculate the difference between the two apple genotypes (Golden and Redspur) by completing the mutate function below. The first two rows should look like: R1 121.7750 127.5900 -5.81500 R2 146.7222 126.7625 19.95972 4.7.5 Practice 3 Manganese was applied to soybean at the V5 stage near New Baltimore, Ohio. manganese = read.csv(&quot;data-unit-4/exercise_data/soybean_manganese.csv&quot;) head(manganese) ## Block Treatment Yield ## 1 1 A 78.9 ## 2 1 B 81.7 ## 3 2 A 80.9 ## 4 2 B 84.3 ## 5 3 B 84.3 ## 6 3 A 79.8 Convert the data to wide format so the two treatments are in separate columns The first two rows of the resulting dataset should look like: 1 78.9 81.7 2 80.9 84.3 Then calculate the difference between treatments A and B by subtracting treatment B from treatment A. The first couple of rows of the final dataset should look like: 1 78.9 81.7 -2.8 2 80.9 84.3 -3.4 4.7.6 Practice 4 Data are from a wheat trial where fungicide was applied during flag leaf stage. wheat = read.csv(&quot;data-unit-4/exercise_data/wheat_fungicide.csv&quot;) head(wheat) ## Block Treatment Yield ## 1 1 B 95.9 ## 2 1 A 93.8 ## 3 2 A 92.3 ## 4 2 B 97.5 ## 5 3 A 98.7 ## 6 3 B 98.8 Convert the data to wide format. The first few rows should look like: 1 93.8 95.9 2 92.3 97.5 3 98.7 98.8 Calculate the difference between treatments by subtracting Treatment A from Treatment B. The first few rows should look like: 1 93.8 95.9 -2.1 2 92.3 97.5 -5.2 3 98.7 98.8 -0.1 4.8 Exercise: Confidence Interval of Difference\" In this unit, we learned that, in the case of two-treatment experiments, we can test whether the populations that receive each treatment are different from each other by testing whether their difference is different from zero. While we will often use the t.test to calculate the probability the two populations are equal, given their measured values, there are times when we may want to report the confidence interval around their difference. These confidence intervals also be can be gained from the t.test, but calculating them manually in these exercises is a good opportunity to sharpen your R skills and hopefully will give you are greater conceptual sense of how the confidence interval works. To create the confidence interval for the difference between two populations, there are the following steps: - calculate the differences between the populations in each pair or block - calculate the overall mean difference between the two populations - calculate the standard deviation and standard error of the difference - look up the appropriate t-value based on the desired level of confidence and degrees of freedom - add and subtract the product of standard error and t-value from the mean difference. 4.8.1 Example 1 We will work with the soybean fungicide data from the previous exercise library(tidyverse) soybean = read.csv(&quot;data-unit-4/exercise_data/soybean_fungicide.csv&quot;) head(soybean) ## Block Treatment Yield ## 1 1 A 71.5 ## 2 1 B 75.1 ## 3 2 A 73.5 ## 4 2 B 77.7 ## 5 3 B 77.7 ## 6 3 A 72.4 4.8.1.1 Calculate Differences We will use the spread and mutate functions from the previous exercises to calculate a the population difference within each block. soybean_wide = soybean %&gt;% spread(Treatment, Yield) soybean_diff = soybean_wide %&gt;% mutate(diff = A-B) soybean_diff ## Block A B diff ## 1 1 71.5 75.1 -3.6 ## 2 2 73.5 77.7 -4.2 ## 3 3 72.4 77.7 -5.3 ## 4 4 75.0 78.2 -3.2 ## 5 5 76.4 75.9 0.5 ## 6 6 76.5 77.5 -1.0 ## 7 7 74.8 80.7 -5.9 ## 8 8 75.6 78.4 -2.8 ## 9 9 73.2 80.7 -7.5 ## 10 10 74.6 76.9 -2.3 4.8.1.2 Calculate the Mean Difference This is the mean for the new column, diff. As a reminder, soybean_diff$diff tells R to use the diff column of the soybean_diff data frame. diff_mean = mean(soybean_diff$diff) diff_mean ## [1] -3.53 4.8.1.3 Calculate the Standard Deviation and Standard Error We can calculte the standard deviation using the sd function. diff_sd = sd(soybean_diff$diff) diff_sd ## [1] 2.351383 The standard error is the standard deviation, divided by the square root of the number of observations, n. In this example, n=10. We can verify this using the length function of R. We can then calculate the standard error, which we will call diff_se. diff_N = length(soybean_diff$diff) diff_se = diff_sd / sqrt(diff_N) diff_se is the standard error. The formula we used is equal to 2.35 / sqrt(10). We just used the variables to save retyping. 4.8.1.4 Calculate the t-value We calcuate the t-value to use in our confidence interval by using the qt function of R. Remember, the qt function uses two arguments. The first is the desired level of confidence. We want a 95% confidence interval, so we will use 0.975. That will leave 0.025, or 2.5%, beyond the upper and lower limits of our confidence interval. The second argument is the degrees of freedom. Since we have observed ten differences, we have nine degrees of freedom. t_value = qt(0.975, 9) t_value ## [1] 2.262157 Our t_value is approximately 2.26. 4.8.1.5 Calculate the Confidence Interval The confidence interval is bound by upper and lower confidence limits. The lower limit is equal to the mean difference minus the product of the standard error of the difference and t-value. The upper limit is the mean difference plus the product of the standard error of the difference. lower_limit = diff_mean - (diff_se * t_value) upper_limit = diff_mean + (diff_se * t_value) lower_limit ## [1] -5.212078 upper_limit ## [1] -1.847922 The 95% confidence interval, with 9 df, for the difference is (-5.21, -1.85). Note the confidence interval does not include zero, therefore the two populations (and, thus, the treatments they received) are significantly different at the P&lt;0.05 level. Which treatment was greater? Since we subtracted B from A, the negative confidence interval indicates B was greater. 4.8.2 Example 2 We will work with the Darwin corn data. darwin = read.csv(&quot;data-unit-4/exercise_data/darwin.csv&quot;) head(darwin) ## pot pair type height ## 1 I a cross 23.500 ## 2 I a self 17.375 ## 3 I b cross 12.000 ## 4 I b self 20.375 ## 5 I c cross 21.000 ## 6 I c self 20.000 4.8.2.1 Calcuate Differences darwin_wide = darwin %&gt;% spread(type, height) head(darwin_wide) ## pot pair cross self ## 1 I a 23.500 17.375 ## 2 I b 12.000 20.375 ## 3 I c 21.000 20.000 ## 4 II d 22.000 20.000 ## 5 II e 19.125 18.375 ## 6 II f 21.500 18.625 darwin_diff = darwin_wide %&gt;% mutate(diff = cross - self) head(darwin_diff) ## pot pair cross self diff ## 1 I a 23.500 17.375 6.125 ## 2 I b 12.000 20.375 -8.375 ## 3 I c 21.000 20.000 1.000 ## 4 II d 22.000 20.000 2.000 ## 5 II e 19.125 18.375 0.750 ## 6 II f 21.500 18.625 2.875 4.8.2.2 Calculate Mean Difference darwin_diff_mean = mean(darwin_diff$diff) darwin_diff_mean ## [1] 2.616667 4.8.2.3 Calculate Standard Deviation and Standard Error darwin_diff_sd = sd(darwin_diff$diff) darwin_diff_N = length(darwin_diff$diff) darwin_diff_se = darwin_diff_sd / sqrt(darwin_diff_N) darwin_diff_se ## [1] 1.218195 4.8.2.4 Calculate the t-value There were 15 pairs, so there are 14 degrees of freedom. We will again calculate the t-value for the 95% confidence interval. darwin_t_value = qt(0.975, 14) darwin_t_value ## [1] 2.144787 4.8.2.5 Calculate the Confidence Interval darwin_lower_limit = darwin_diff_mean - (darwin_diff_se * darwin_t_value) darwin_upper_limit = darwin_diff_mean + (darwin_diff_se * darwin_t_value) darwin_lower_limit ## [1] 0.003899165 darwin_upper_limit ## [1] 5.229434 The confidence interval was (0.004, 5.229). It does not include zero, so the difference is significant. Since we subtracted the self-pollinated values from the cross-pollinated values, the positive difference means the cross-pollinated plants were taller than the self-pollinated plants. 4.9 Exercise: T-Test The t-test is used to test the probability that two populations are different. In a two-treatment trial, each population will receive a different treatment (an input or management practice). In R, we can use the t.test function to quickly determine the probability that two treatments are different. 4.9.1 Case Study 1: Wheat Fungicide Lets start by loading the data and using the head command to examine its structure. Treatment B was a fungicide applied at flag leaf. Treatment A is the untreated control. wheat = read.csv(&quot;data-unit-4/exercise_data/wheat_fungicide.csv&quot;) head(wheat) ## Block Treatment Yield ## 1 1 B 95.9 ## 2 1 A 93.8 ## 3 2 A 92.3 ## 4 2 B 97.5 ## 5 3 A 98.7 ## 6 3 B 98.8 4.9.2 T-Test The t.test function in R requires at least three arguments. The first is a model statement. In testing whether populations that receive different treatments have different yields, we are modelling yield as a function of treatment. We can express this in the following statement: model = Yield ~ Treatment We can then plug the model statement into our t.test. The second argument to the t.test, data=wheat, tells R which data frame to analyze. The third, paired=TRUE, tells R the treatments were paired (blocked) in our experiment. wheat_t_test = t.test(model, data=wheat, paired=TRUE) wheat_t_test ## ## Paired t-test ## ## data: Yield by Treatment ## t = -3.7166, df = 9, p-value = 0.004796 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -5.871641 -1.428359 ## sample estimates: ## mean of the differences ## -3.65 What would happen if we left out the paired=TRUE argument? wheat_t_test_independent = t.test(model, data=wheat) wheat_t_test_independent ## ## Welch Two Sample t-test ## ## data: Yield by Treatment ## t = -3.9101, df = 17.91, p-value = 0.001034 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -5.611876 -1.688124 ## sample estimates: ## mean in group A mean in group B ## 94.65 98.30 We notice, that the t-value, degrees of freedom, and p-value are all different. This is because R analyzes the two treatments as independent populations with their own variances  instead of analyzing the differences between each pair of treatments. The df of 17.91 is not a whole number  this also tells us that R is treating the variances differently and using algorithms it would not use with a paired design. Lets look at the correct analysis again. wheat_t_test ## ## Paired t-test ## ## data: Yield by Treatment ## t = -3.7166, df = 9, p-value = 0.004796 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -5.871641 -1.428359 ## sample estimates: ## mean of the differences ## -3.65 Highlights of the output include the p-value, and the 95% confidence interval. The p-value tells us there is about a 0.005, or 0.5% probability that the difference between the treatments is actually zero, give the actual observed difference. The 95% confidence interval is a range around the measured difference, -3.65, that is likely to include the true difference between the populations. That the confidence interval does not include zero is a reminder that the difference between the two populations is significant. There are a couple of other arguments we may include in the t.test function. Remember that the t-test tests whether the measured difference between populations is different from a hypothetical difference. Usually, this hypothetical difference is zero, as we can see in the output above where it says true difference in means is not equal to 0. But it is good to extra-careful and specify this by including the argument mu=0 in our t.test function. wheat_t_test_mu_zero = t.test(model, data=wheat, paired=TRUE, mu=0) wheat_t_test_mu_zero ## ## Paired t-test ## ## data: Yield by Treatment ## t = -3.7166, df = 9, p-value = 0.004796 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -5.871641 -1.428359 ## sample estimates: ## mean of the differences ## -3.65 Occassionally, we might want to test whether the observed difference between populations is something other than 0. What if in our wheat trial we wanted to test whether the observed difference between populations was different than -1? In that case, we would specify mu=-1 in our code. wheat_t_test_mu_1 = t.test(model, data=wheat, paired=TRUE, mu=-1) wheat_t_test_mu_1 ## ## Paired t-test ## ## data: Yield by Treatment ## t = -2.6983, df = 9, p-value = 0.02446 ## alternative hypothesis: true difference in means is not equal to -1 ## 95 percent confidence interval: ## -5.871641 -1.428359 ## sample estimates: ## mean of the differences ## -3.65 We can see that our p-value has changed, as has the alternative hypothesis statement. 4.9.3 Case Study 2: Apple Variety Lets look at our apple data: apple = read.csv(&quot;data-unit-4/exercise_data/apple_genotype.csv&quot;) head(apple) ## rep gen yield ## 1 R1 Golden 121.7750 ## 2 R1 Redspur 127.5900 ## 3 R2 Golden 146.7222 ## 4 R2 Redspur 126.7625 ## 5 R3 Golden 145.7500 ## 6 R3 Redspur 132.9364 Our model is now that yield is a function of gen (genotype). Our model statement is therefore: apple_model = yield ~ gen And our t-test is as follows: apple_t_test = t.test(apple_model, data=apple, paired = TRUE, mu=0) apple_t_test ## ## Paired t-test ## ## data: yield by gen ## t = -0.41501, df = 4, p-value = 0.6994 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -28.58021 21.14724 ## sample estimates: ## mean of the differences ## -3.716484 We can see from above that our p-value is about 0.70. Sometimes it snows in April. And sometimes our treatments arent significantly different. If we look at our confidence interval, we can confirm that it includes zero. 4.9.4 Practice What is the t-test value for the data-unit-4/exercise_soybean_fungicide.csv trial? Assume the treatments are paired. Your t-test should reveal a p-value of 0.001048. What is the t-test value for the data-unit-4/exercise_soybean_manganese.csv trial? Assume the treatments are paired. Your t-test should reveal a p-value of 0.005143. "],["understanding-statistical-tests.html", "Chapter 5 Understanding Statistical Tests 5.1 Research Question 5.2 The Model 5.3 Hypotheses 5.4 P-Value 5.5 The P-Value and Errors 5.6 One-Sided vs Two-Sided Hypotheses 5.7 Exercise: Linear Additive Model 5.8 Exercise: One-Sided Hypotheses 5.9 Exercise: Type I and Type II Errors", " Chapter 5 Understanding Statistical Tests In the last unit, we introduced the concept of statistical testing and, although I endeavor to make this course as practical and painless as possible, I believe it worthwhile to spend a unit on some of the theory of statistical testing. This will help reinforce what we have learned so far in this course, and prepare us for the other statistical tests that lie ahead. Otherwise, it is easy to become ambiguous about what we are really testing, and unclear in reporting our results. In the last unit, we discussed experimental design and quickly jumped into data analysis. This unit, we will walk through the thought processes that surround our trial, including: - identifying our research question - creating a model from our question - identifying the hypotheses our data will be used to test - recognizing that we can mistakingly accept or reject these hypotheses - understanding how the confidence interval and p-value describe our measured difference - incorporating pre-existing knowledge into our hypotheses and tests This list seems intimidating, but we will take our time and break these down into as much plain language as statistics will allow. 5.1 Research Question As I have likely said before in this course, the first think you must have to design an experiment is a clear, testable research question. The question should be answerable using quantitative data and specific about what those data will measure. Here are some examples of bad and good questions: Bad: Is this fertilizer better than another? Good: Does corn treated with 6-24-6 fertilizer at planting differ in yield from corn that is not treated with an in-furrow starter. Bad: Is the winter wheat variety MH666 (the Beast) different than variety MH007 (the Bond)? Good: Does variety MH666 differ in test weight from variety MH007 ? Bad: Does herbicide deposition agent Stick-It! perform differently than agent Get-Down! ? Good: Do Stick-It! and Get-Down! differ in the number of live weeds two weeks after their application with glyphosate? Remember to be clear about what we are measuring. Otherwise, it is unclear whether we are testing fertilizer affects on corn yield or moisture at harvest. We dont know whether we are comparing winter wheat yield or head scab. We dont know whether we are measuring the effect of our deposition agent on weed survival or crop injury. 5.2 The Model The word model probably makes you shudder and think of a crowded blackboard filled with mathematical equations. Yes, models can be quite complex. All of you have worked with models, however, and most of you should recall this one: \\[ y = b + mx \\] Where \\(y\\) is the vertical coordinate of a point on a graph, \\(x\\) is its horizontal coordinate, and \\(b\\) is the Y-intercept (where the line crosses the y-axis). The most interesting variable is often \\(m\\), the slope. The slope is the unit increase in y with each unit increase in x. Suppose we took a field of corn and conducted a side-by-side trial where half of the plots were sidedressed with 150 lbs treated with an N stabilizer. The other half were sidedressed with 150 lbs actual N plus 1 unit of nitrogen stabilizer. The mean yield of plots treated with N plus nitrogen stabilizer was 195 bu and the mean yield of plots treated with N alone was 175 bu. How could we express this with a slope equation? First, lets state this as a table. We will express the N stabilizer quantitatively. The No Stabilizer treatment included zero units of N stabilizer. The Stabilizer treatment received 1 unit of stabilizer. library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- ## v ggplot2 3.3.3 v purrr 0.3.4 ## v tibble 3.1.0 v dplyr 1.0.5 ## v tidyr 1.1.3 v stringr 1.4.0 ## v readr 1.4.0 v forcats 0.5.1 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() set.seed(1776) `No Stabilizer` = rnorm(4, mean=175, sd = 5) `Stabilizer` = rnorm(4, mean = 195, sd=5) original_data = cbind(`No Stabilizer`, `Stabilizer`) %&gt;% as.data.frame() means_table = original_data %&gt;% gather(Treatment, Yield) %&gt;% group_by(Treatment) %&gt;% summarise(Yield = mean(Yield)) %&gt;% ungroup() %&gt;% mutate(Yield = round(Yield, 1)) %&gt;% mutate(Nitrogen = c(0,1)) %&gt;% select(Treatment, Nitrogen, Yield) %&gt;% column_to_rownames(var = &quot;Treatment&quot;) mean_yield = mean(means_table$Yield) means_table ## Nitrogen Yield ## No Stabilizer 0 177.7 ## Stabilizer 1 198.0 # yield = c(175, 195, 185) # nitrogen = c(0, 1, 0.5) # original_data = cbind(nitrogen, yield) %&gt;% # as.data.frame() # rownames(original_data) = c(&quot;No Stabilizer&quot;, &quot;Stabilizer&quot;, &quot;Mean&quot;) # # original_data In creating this table, we also calculated the mean stabilizer rate and corn yield across all plots. These are are population means for the field. Now, lets express the stabilizer rate and yield little differently, this time by their differences from their population mean. In half of the plots, the N stabilizer rate was 0.5 less than the population mean of 187.9; in the other half, the rate was 0.5 greater. Similarly, the yield in half of the plots was about 10 bushels less than the population mean of 188.9; in the other half of the plots, it was 10 bushels greater. Our table now looks like this: centered_data = means_table %&gt;% scale(scale = FALSE) %&gt;% as.data.frame() %&gt;% mutate(Yield = round(Yield, 1)) centered_data ## Nitrogen Yield ## No Stabilizer -0.5 -10.2 ## Stabilizer 0.5 10.2 # # yield = c(-10, 10, 0) # nitrogen = c(-0.5, 0.5, 0) # centered_data = cbind(nitrogen, yield) %&gt;% # as.data.frame() # rownames(centered_data) = c(&quot;No Stabilizer&quot;, &quot;Stabilizer&quot;, &quot;Mean&quot;) # # centered_data What we have just done is a statistical process called center scaling. Centering expresses measures by their distance from the population mean, instead of as absolute values. Now lets plot this out using our line equation. \\(y\\) equals yield. \\(x\\) equals nitrogen rate. \\(b\\) equals the mean yield, the y-intercept, which is zero in our centered data. library(ggpubr) means_plot = centered_data %&gt;% ggplot(aes(x = Nitrogen, y=Yield)) + geom_point(color=&quot;blue&quot;, size =5) + geom_line(size = 1.5) means_plot + geom_text(aes(x = Nitrogen + 0.06, y = Yield, label = Yield), size = 5) Ta da: our line plot. If we were to write this as a linear equation, it would be: \\[ Yield = 0 + 20*Stabilizer\\] Thus, as the N stabilizer rate increase from 0 (no stabilizer) to 1 (stabilizer), yield increases 20 bushels. 5.2.1 Treatment Effect Another way of expressing the effect of the treatment levels is in terms of their distance from the mean yield across all plots. Where sidedressed with nitrogen alone, our mean yield is equal to the population mean minus 10. Conversely, where we sidedressed with nitrogen plus stabilizer, our yield is the population mean plus 10.2. We can express these treatment effects as: \\[Unstabilized : T_0 = -10.2\\] \\[Stabilized: T_1 = +10.2\\] Our mean yield when corn is sidedressed with N without stabilizer is then equal to the mean yield across all plots plus the treatment effect: \\[Unstabilized: Y_0 = \\mu + T_0 \\] \\[Stabilized: Y_1 = \\mu + T_1 \\] We can prove this to ourselves by plugging in the actual yields for \\(Y\\) and \\(\\mu\\) and the actual treatment effects for \\(T_0\\) and \\(T_1\\): \\[ Unstabilized: 175 = 185 + (-10.2) \\] \\[Stabilized: Y_1 = 185 + (+10.2) \\] 5.2.2 Error Effect The treatment effect is known as a fixed effect: we assume it will be consistent across all plots within our trial. That said, will every plot that receives nitrogen plus stabilizer will yield 195 bushels? Will every field sidedressed with nitrogen without stabilizer yield 175? Of course not. Any yield map will show variation in yield within a field, even when the entire field has been managed uniformly. Differences in soil texture and microclimates, inconsistencies in field operations, and inaccuracies in measuring equipment contribute to variations in the values recorded recorded. These variations will also add to or subtract from the mean yield across all plots. We can visualize this in the plot below. centered_original_data_by_trt = original_data %&gt;% gather(Nitrogen, Yield) %&gt;% mutate(Nitrogen = gsub(&quot;No Stabilizer&quot;, -0.5, Nitrogen)) %&gt;% mutate(Nitrogen = gsub(&quot;Stabilizer&quot;, 0.5, Nitrogen)) %&gt;% mutate(mu = mean(Yield)) %&gt;% group_by(Nitrogen) %&gt;% mutate(T = mean(Yield) - mu) %&gt;% ungroup() %&gt;% mutate(E = Yield - T - mu) %&gt;% mutate(Effect = T + E) %&gt;% mutate(Nitrogen = as.numeric(Nitrogen)) %&gt;% mutate(E = round(E,1)) means_plot + geom_point(data = centered_original_data_by_trt, aes(x = Nitrogen, y=Effect, label=E), size=3, color=&quot;tomato&quot;) + geom_text(data = centered_original_data_by_trt, aes(x = Nitrogen+0.02, y = Effect, label = E), hjust = 0, size = 4) ## Warning: Ignoring unknown aesthetics: label The blue points still represent the treatment mean, and the black line represents the difference between treatments. The red points are the original data  note how they are distributed around each treatment mean. Any individual observation is going to add to or subtract from its treatment mean. The value which each point adds to the treatment mean is show to the right of the point. This is the error effect for that observation. Sometimes it is easier to view the experimental unit effect another way, by excluding the treatment effect so that just the effects are plotted around their mean of zero: centered_original_data_by_trt %&gt;% ggplot(aes(x = Nitrogen, y=E)) + geom_point(size = 3, color=&quot;tomato&quot;) + geom_hline(aes(yintercept = 0), size=1) This kind of plot is often called a residual plot, because the error can be thought of as the unexplained, leftover (ie residue) effect after the population mean and and treatment effects are accounted for. When a correct model is fit to the data, about half the observations for each treatment should be greater than zero, and half below zero. The residual plot is a valuable tool to inspect and verify this assumption. The yield observed in each plot, then, will be the sum of three values: - the mean yield across all plots - the effect of the treatment applied to that plot - the combined effect of environment, field operations, and measurements This model can be expressed as: \\[ Y_{ij} = \\mu + T_i + \\epsilon_{ij} \\] Where: - \\(Y_{ij}\\) is the yield of the \\(i^{th}\\) treatment level in the \\(j^{th}\\) block - \\(\\mu\\) is the yield mean across all plots - \\(T_i\\) is the effect of the \\(i^{th}\\) level of stabilizer - \\(\\epsilon_{ij}\\) is the random effect associated with the plot in the \\(j^{th}\\) block that received the \\(i^{th}\\) level of stabilizer For example, a plot in the 3rd block that received nitrogen treated with stabilizer (\\(T_1\\)) would be indicated by the equation: \\[ Y_{13} = \\mu + T_1 + \\epsilon_{13} \\] If the error for this plot, \\(\\epsilon_{13}\\), was -2, the observed yield would be: \\[ Y_{13} = 185 + 10 -2 = 193 \\] Why bother with the linear model when we simply want to know if one treatment yields more than the other? There are two reasons. First, although in agriculture we often think of field trials as testing differences, what we are really doing is using the data from those trials to predict future differences. In my opinion, this is one of the key differences betweeen classical statistics and data science. Classical statistics describes what haas happened in the past. Data science predicts what will happen in the future. The linear model above is exactly how we would use data from this trial to predict yields if the product is used under similar conditions. Adding the stabilizer to nitrogen during sidedress will increase the mean yield for a field by 10 bushels. But any given point in that field will have a yield that is also determined by the random effects that our model cannot predict: soil and microclimate, equipment, and measurement errors. Second, the linear model illustrates what statistics will test for us. Ultimately, every statistical test is a comparison between fixed and random effects: what explains the differences in our observations more: the fixed effects (our treatment) or random effects (error)? In our current example, we can visualize this as follows: centered_original_data = original_data %&gt;% gather(Nitrogen, Yield) %&gt;% mutate(Nitrogen = gsub(&quot;No Stabilizer&quot;, -0.5, Nitrogen)) %&gt;% mutate(Nitrogen = gsub(&quot;Stabilizer&quot;, 0.5, Nitrogen)) %&gt;% mutate(Yield = Yield - mean(Yield)) %&gt;% mutate(Nitrogen = as.numeric(Nitrogen)) library(ggpubr) library(pBrackets) library(grid) means_plot + geom_point(data = centered_original_data, aes(x = Nitrogen, y=Yield), size=3, color=&quot;tomato&quot;) + geom_text(aes(x=-0.40, y=-10), label=&quot;Does the spread of\\nindividuals within a treatment\\nexplain more of the variance?&quot;, hjust=0, vjust=1, size=5) + geom_text(aes(x=0.05, y=0), label=&quot;Or does the difference\\nbetween treatments explain\\nmore of the variance?&quot;, hjust=0, vjust=1, size=5) grid.locator(unit=&quot;native&quot;) grid.brackets(95, 200, 95, 370, h=0.05, lwd=2, col=&quot;red&quot;) grid.brackets(370, 100, 370, 285, h=0.05, lwd=2, col=&quot;red&quot;) The purpose of a trial is to measure both types of effects and render a verdict. Which is hypotheses are important, as we will now see. 5.3 Hypotheses Before we design any experiment, however, we have to define our research question. In the case of a side-by-side trial, the question is generally: \"Is one treatments better than the other? This question then needs to be translated into hypotheses. Outside of statistics, a hypothesis is often described as an educated guess. Experiments are designed, however, to test two or more hypotheses. We may casually describe a side-by-side trial as comparing two treatments, but the data are formally used as evidence to test two, opposing hypotheses: Ho: The difference between the two treatments is zero. Ha: The difference between the two treatments is not zero. The first hypothesis, Ho, is called the null hypothesis. The second hypothesis, Ha, is the alternative hypothesis. Typically, we tend to focus our effort on gathering enough evidence to support the alternative hypothesis: after all this work, we typically want to see a treatment difference! But we need to remember the null hypothesis may also be supported. This process, like the linear model ahead, probably seems overly-formal at first. But like the linear model, it helps us to understand what statistics really tell us. We cannot prove either of these hypotheses. The world is full of one-off exceptions that will prevent either hypothesis from being universal truths. Our science is about comparing the evidence for each hypothesis, and selecting the hypothesis that is probable enough to meet our standards. To illustrate this, look no further than the t-test we used in the last unit: \\[ t = \\frac{\\bar{x}-\\mu}{SED} \\] Recall that \\(\\bar{x}\\) was our treatment difference, \\(\\mu\\) was the hypothesized treatment difference (zero), and \\(SED\\) was the standard error of the difference. The numerator is our treatment effect on plot yield. The denominator quantifes the random effects on plot yield. As this ratio increases, so does t. As t increases, the probability that the true population difference is zero decreases. Another nuance of hypotheses is this, especially when it comes to the alternative hypothesis. If the evidence fails to support the alternative hypothesis, that does not mean it is wrong. The fixed (treatment) effect we observed was real. But the random effect was so great we could not rule out the differences we observed were the result of chance. Simply put, our confidence interval was too big to rule out the true difference between treatments was actually zero. There was too much variation among plots. In a trial with a lower standard error of the difference, our t-value would have been greater, and the probability that the true difference between treatments was zero would be lesser. Statistics is not about finding the truth. It is about quantifying the probability an observed difference is the result of chance. Lower probabilities suggest less chance in our observations, and the greater likelihood this difference will be observed if the trial is repeated by someone else, in a laboratory, or in a production field. 5.4 P-Value The P-Value is always a source of confusion in statistics. What does it mean? What is so magical about the 0.05, or 5%, cutoff for declaring populations different? Even if you think youve mastered the P_value concept already, lets review it one more time. The P-value, as applied to a distribution, is the probability of obseverving a value with a given (or greater) difference from the population mean. For example, if we have a t-distribution with a mean of 0 and a standard error of 1, the probability we will, the probability we will observe a value 2.3 standard errors away than the mean, given a population size of 4, is 0.047, or 4.7%. library(fastGraph) shadeDist(xshade=c(-2.3, 2.3), &quot;dt&quot;, parm2 = 9, lower.tail = TRUE) What does a P-value of 0.047 mean? It means the probability of us measuring this value, by dumb luck, when the true population mean is 0, is about 4.7%. Put another way, given the standard error we observed, if the true population mean was zero, we would observe a value equal to or more than 2.3 units away from the mean in less than 5 out of 100 trials. If this concept sounds the same as that described for a confidence interval, thats because it is the same principle. If we constructed a 95% confidence interval around the sample mean of 2.3, we would see it excludes zero. Knowing this, we have three options. We can conclude, for now, that the population mean really was zero and we were just very lucky (or unlucky) in our sampling. We could also repeat the trial multiple times, to see what other values occur. If this is a field trial, that will incur additional research expenses. Even worse, it means delaying a recommendation for several seasons. Our final option would be to conclude that our population mean is probably not zero. If the probability of observing a sample mean 2.3 or more units away from the mean, when the true population mean is zero, is 4.7% or less, then we can also say that the probability that the population mean has a value of zero or less, given our sample mean of 2.3, is 4.7% or less. Given this knowledge, we may conclude the true population mean is different from zero. This is the power  and beauty!  of statistics. By properly designing an experiment (with sufficient replication and randomization), we can estimate the variability of individuals within a population. We can then use these estimates to test the probability of a hypothetical population mean, given the variability in our sample. And from that, we decide whether one population (which, for example, may have received a new product) is different from the other (which was untreated). 5.5 The P-Value and Errors There are two kinds of P-value: the P-Value we measure, and the maximum P-Value we will accept before determinng an observed difference between populations is insignificant. This maximum P-Value is referred to as alpha (\\(\\alpha\\)). You have probably seen statistical summaries that included whether treatments were \"different at the \\(P \\le 0.05\\) level. In this case, the \\(\\alpha\\) is 0.05, or 5%. Before we discuss why an alpha of 0.05 or 5% is so often used for statistical tests, we need to understand how it relates to the likelihood we will reach the correct inference when comparing populations. You see, once we have gathered our data, calculated the variance in those populations (or, in the case of the paired t-test, the variance in their differences), and run our test(s), we will conclude either that the two populations are the same, or that they are different. Either conclusion may be right. Or it may be wrong. And since we rarely measure entire populations, we never know their exact population means. We work with probabilities. In the above example, there was a 4.7% chance we could observe a sample mean 2.3 units from a true population of zero. That means there is a 95.3 % (100 - 4.7) chance we would not see that value by chance. But there is still a chance. In other words, there is still a chance we could conclude the population mean is not zero, when in fact it is. When we infer the mean of one population is significantly different from another (whether the second mean be measured or hypothesized), when in fact the two population means are equal, we commit a Type I Error. One example would be concluding the yield of one population, having received additional fertilizer, yielded more than an unfertilized population, when in fact their yields were equal. Another example would be concluding there is a difference in the percent of corn rejected from two populations, each treated with a different insecticide. The P-value is the probability of making that Type I Error: of observing a sample mean so improbable enough that it leads us to conclude two populations are different, when for all purposes they are the same. If we are worried that recommending a product to a grower that does not increase yield will cost us their business, then we are worried about making a Type I Error. Outside of agriculture, if we are worried about releasing a treatment for COVID-19 that does not work and will leave users unprotected, we are worried about a Type I Error. If we are really, really worried about wrongly inferring a difference between populations, we might use an even lower P-value. We might use P=0.01, which means we will not infer two treatments are different unless the mean difference we observe has less than a 1% probability of being observed by chance. This might be the case if the product we recommend to a grower is not $10 per acre, but $30. If our COVID treatment is very expensive or has serious side effects, we might insist on an even lower alpha, say P=0.001, or 0.1%. So why not always use an alpha of 0.01 or 0.001 to infer whether two populations are different? There is a second error we can make in the inferences from our research: we can conclude two populations are not different, when in fact they are. In this case, we observed, by chance, a sample mean from one population that was very close to the mean (hypothesized or measured) of another population, when in fact the two population means are different. For example, a plant breeder might conclude a there performance of a new hybrid is similar to an existing hybrid, and fail to advance it for further testing. An agronomist might erroneously conclude there is no difference in plants treated with one of two micronnutrient fertilizers, and fail to recommend the superior one to a grower. Even more dangerously, a health researcher might conclude there is no difference in the incidence of strokes between a population that receives a new medication and an untreated population, and erroneously conclude a that mdeciation is safe. Thus, there is a tradeoff betwteen Type I and Type II errors. By reducing the alpha used as critierial to judge whether an one value is significantly different from another, we reduce the likelihood of a Type I error, but increase the likelihood of a Type II error. In agronomic research, we conventionally use an alpha of 0.05. I cannot explain why we use that particular alpha, other than to suggest it provides an acceptable balance between the risks of Type I and Type II errors for our purposes. It is a value that assures most of the time we will only adopt new practices that very likely to increase biomass or quality, while preventing us wrongly rejecting other practices. In my research, I might use an alpha of 0.05 in comparing hybrids for commercial use. But I might use an alpha of 0.10 or 0.15 if I was doing more basic work in weed ecology where I was testing a very general hypothesis to explain their behavior. To make things simple, wew will use an alpha of 0.05 for tests in this course, unless states otherwise. 5.6 One-Sided vs Two-Sided Hypotheses So far, we have treated our hypotheses as: Ho: there is no difference between two populations, each treated with a different input or practice Ha: there is a difference between two populations, each treated with a different input or practice We could casually refer to these as no difference and any difference. But often in side-by-side trials, we have an intuitive sense (or hope) that one population will be better than another. If we are the yield of the two populations, one planted with an older hybrid and the other with a newer hybrid, we may be trying to determine whether the new hybrid is likely to yield more. If we comparing the number of infected plants in populations treated with different fungicides, we may hope the population that receives the new technology will have fewer diseased plants that the population that receives the older technology.. Is this bias? Yes. Is it wrong? I would argue no. The whole purpose of product development, in which I work, is to identify better products. Better hybrids, better micronutrients, better plant growth regulators. If we were equally satisfied whether a new product performed significantly better or significantly worse than an older product  well, in that case, Id better look into teaching another section of this course. It is okay to have this kind of bias, so long as we keep our research honest. Proper experimental design, including replication and randomization of plots in the field, or pots in the greenhouse, will go a long way towards that honest. So will selection of a P-value that acceptably minimizes Type I errors, so that we dont advance a new product which isnt highly likely to perform better in future trials, or in the growers field. When we have this kind of bias, or interest, however, it also changes our hypotheses. Our null hypothesis is that the population treated with the new product will not perform better than the population treated with the old product. Our alternative hypothesis is the population treated with the new product will perform better. If we are comparing yield in corn populations treated with a new fungicide, our hypotheses will be: Ho: The yield of the population that receives the new fungicide will be equal too  or lesser  than the yield of the population that receives the old fungicide. Ha: The yield of the population that receives the new fungicide will be greater than the yield of the population that receives the old fungicide. The reason these are called one-sided hypotheses is because we are only interersted in one-side of the normal distribution. In the two-sided hypotheses we have worked with, we would only care if the yield of the two populations (one receiving the old fungicide, the other receiving the new fungicide) were different. To visualize this library(fastGraph) alpha_05_2side = qt(0.975, 4) shadeDist(xshade=c(-alpha_05_2side, alpha_05_2side), &quot;dt&quot;, parm2 = 4, lower.tail = TRUE) If either \\(t\\le-2.78\\) or \\(t\\ge-2.78\\) (either of the red areas above), we declare the two populations different. In other words, the observed t-value can occur in either of the two tails and be significant. Accordingly, we refer to this as a two-tailed test. In testing a one-sided hypothesis, we only care if the difference between the population that received the new fungicide and the population that received the old fungicide (ie new fungicide - old fungicide) has a t-value of 1.98 or greater. We would visualize this as: library(fastGraph) alpha_05_1side = qt(0.975, 4) shadeDist(xshade=alpha_05_1side, &quot;dt&quot;, parm2 = 4, lower.tail = FALSE) Only if the measured t-value falls in the upper tail will the population that receives the new fungicide be considered significantly better than the population that received the old fungicide. We therefor  you guessed it!  refer to this test as a one-tailed test. In using a one-tailed test, however, we need to use a different t-value to achieve our alpha (maximum p-value for significance). If you look at the plot, only 2.5% of the distribution is in the upper tail. If we leave this as it is, we will only conclude the populations are different is their P-value is equal to or less than 2.5%. Reducing our P-value from 5% to 2.5% will, indeed, reduce our probability of Type I errors. But it will increase our likelihood of Type II errors. If we are going to conduct a one-tailed test with an alpha of 0.05, we need to adjust the percentage of the distribution in the upper tail to 5% of the distribution: alpha_05_1side = qt(0.95, 4) shadeDist(xshade=alpha_05_1side, &quot;dt&quot;, parm2 = 4, lower.tail = FALSE) The implication is that the minimum difference between populations to be significant at an alpha=0.05 is lesser than for a two-tailed test. A common first response to the one-tailed test is: Isnt that cheating? Arent we just switching to a one-tailed test so we can nudge our difference passed the goal line for significance? And, indeed, if you switch to a one-tailed test for that reason alone, it is cheating. That is why it is important we declare our hypotheses before we begin our trial. If we are going to run a one-tailed test, it needs to be based on a one-sided hypothesis that declares, from the beginning, we are only testing the difference in one direction, either because we have intuition that the difference will be one-sided, or we have a practical reason for only being interested in a positive or negative difference between populations. 5.7 Exercise: Linear Additive Model The linear additive model may seem a very esoteric concept in data science, but it is critical to understand how we make predictions from statistical models, and how we test whether those models significantly explain the variance in our observations. It also underscores an important concept in statistical modelling: the assumption that the different effects that combine to explain an observed value are additive. 5.7.1 Case Study: Barley Effects The barley dataset is the same we used in the other exercises, except I have calculated the population mean, mu, treatment effet, and error effect. Ignore the last four columns  those were calculated only to create the chart you will run below. barley_effects = read.csv(&quot;data-unit-5/exercise_data/barley_effects_table.csv&quot;) barley_effects ## block trt y plot mu trt_mean trt_effect error_effect trt_bar_min ## 1 1 new 291.1 1 312.26 339.06 26.8 -47.96 312.26 ## 2 1 old 223.9 2 312.26 285.46 -26.8 -61.56 -26.80 ## 3 2 new 321.4 3 312.26 339.06 26.8 -17.66 312.26 ## 4 2 old 249.9 4 312.26 285.46 -26.8 -35.56 -26.80 ## 5 3 new 399.3 5 312.26 339.06 26.8 60.24 312.26 ## 6 3 old 330.8 6 312.26 285.46 -26.8 45.34 -26.80 ## 7 4 new 362.6 7 312.26 339.06 26.8 23.54 312.26 ## 8 4 old 349.8 8 312.26 285.46 -26.8 64.34 -26.80 ## 9 5 new 320.9 9 312.26 339.06 26.8 -18.16 312.26 ## 10 5 old 272.9 10 312.26 285.46 -26.8 -12.56 -26.80 ## trt_bar_max err_bar_min err_bar_max ## 1 339.06 -47.96 0.0 ## 2 0.00 -88.36 -26.8 ## 3 339.06 -17.66 0.0 ## 4 0.00 -62.36 -26.8 ## 5 339.06 339.06 399.3 ## 6 0.00 312.26 357.6 ## 7 339.06 339.06 362.6 ## 8 0.00 312.26 376.6 ## 9 339.06 -18.16 0.0 ## 10 0.00 -39.36 -26.8 5.7.2 Plotting the Effects Below is the code to create a bar plot. This kind of bar plot is stacked  it shows multiple measures for each observation (plot), stacked on top each other. Normally, a stacked bar plot can be created in a couple of lines of code  the intricate code below was so I could customize the order of stacking for each plot. Run the code and observe the plot. ggplot(barley_effects) + geom_segment(aes(x=plot, y=0, xend=plot, yend=mu), color=&quot;lightblue&quot;, size=14) + geom_segment(aes(x=plot, xend=plot, y=trt_bar_min, yend=trt_bar_max), color=&quot;lightgreen&quot;, size=14) + geom_segment(aes(x=plot, xend=plot, y=err_bar_min, yend=err_bar_max), color=&quot;tomato&quot;, size=14) + # geom_point(aes(x=plot, y=y), size=4) + geom_text(aes(x=plot, y=mu/2, label= round(mu,1))) + geom_text(aes(x=plot, y=trt_bar_min+(trt_bar_max-trt_bar_min)/2, label=round(trt_effect,1))) + geom_text(aes(x=plot, y=err_bar_min+(err_bar_max-err_bar_min)/2, label=round(error_effect,1))) + geom_segment(aes(x=plot+0.4, xend=plot+0.2, y=y, yend=y), arrow = arrow(length = unit(0.01, &quot;npc&quot;)), size=1) + geom_text(aes(x=plot+0.4, y=y, label=round(y,1)), hjust=0) + scale_x_continuous(breaks=c(1:10)) If this plot is too big to fit comfortably on this screen, remember you can knit this code using the menu immediately above this window, and either view it in HTML or print it out to use. What I have tried to do with this plot is to visualize the additive effects of the population mean (light blue bar), treatment effect (green bar), and error effect (red bar). The sum of their effects (the observed value) is shown by a black arrow to the right of each bar. The blue bars are the same, 312.3, reflecting that the population mean for the whole trial does not change: every plot starts out with this same value, with the effects then adding or subtracting from its observed value. Looking at this plot, you can see half of the green bars have a treatment effect of 26.8, while the other half have a treatment effect of -26.8. In a two treatment trial, the treatment effects will be exactly opposite for the two levels of treatment. If we look at the plot above, we can see the positive treatment effect is associated with the new genotype, and the negative effect with the old genotype. The red bars represent the error. This is the unexplained variance in observed value. Part of it reflects how, try as hard as we might, not every plot has exactly the same environment. Soil types and microclimates differ. Plot lengths may differ slightly among plots. One plot may be planted more evenly than others. A fertilizer applicator might skip. An ear might bounce out of the combine. Note that the errors are negative in some cases and positive in others. In addition, they are not consistent  their values are random. The treatment effect above is an example of a fixed effect  it has a specific, consistent effect, based on the level of treatment. The error effect above is an example of a random effect. Its value varies from plot to plot. Its variation usually follows a normal distribution with a mean close to zero. Lets run the histogram below and check. hist(barley_effects$error_effect) mean(barley_effects$error_effect) ## [1] 1.775489e-16 5.7.3 Examining Individual Plots We can see Recall our linear model is: Yij = mu + Ti + e(i)j Y(i)j is the observed value for plot j that has received treatment i. mu is the population mean. Ti is the treatment effect associated with level i. e(i)j is the So lets plug a few plots into this equation and see how it works. In plot 2: - Yij = 223.9 - mu = 312.3, - Ti, associated with the old hybrid = -26.8 - e(i)j, associated with plot 2, = -61.6 Our model for plot 2, then is: 223.9 = 312.3 - 26.8 - 61.6 What about for plot 5? Yij = 399.3 mu = 312.3 - Ti, associated with the new hybrid = +26.8 - e(i)j, associated with plot 5 = +60.2 Our model for plot 5, then is: 399.3 = 312.3 + 26.8 + 60.2 What about for plot 8? Yij = 349.8 mu = 312.3 - Ti, associated with the old hybrid = -26.8 - e(i)j, associated with plot 8 = +64.3 Our model for plot 8, then is: 349.8 = 312.3 - 26.8 + 64.3 What is interesting in this last example is that the positive error masks the negative effect of the treatment  the yield with the old hybrid, in this case, is greater than plots 3 and 9, which received the old hybrid! This underscores why it is important to break down the effects behind an observation to better understand the true contribution of each treatment level. 5.7.4 Practice: Groudnut Lets load and plot the groundnut data from the other exercises this unit. Like the barley data above, we have added several columns in order to calculate treatment and error effects and to draw the plot below. groundnut_effects = read.csv(&quot;data-unit-5/exercise_data/groundnut_effects_table.csv&quot;) groundnut_effects ## block row col trt y dry plot mu trt_mean trt_effect error_effect ## 1 B1 4 2 A 5.2 3.3 1 3.225 4.30 1.075 0.90 ## 2 B1 4 6 C 2.4 1.4 2 3.225 2.15 -1.075 0.25 ## 3 B2 3 1 C 1.7 0.9 3 3.225 2.15 -1.075 -0.45 ## 4 B2 3 6 A 4.8 3.0 4 3.225 4.30 1.075 0.50 ## 5 B3 2 3 A 2.4 1.4 5 3.225 4.30 1.075 -1.90 ## 6 B3 2 6 C 2.5 1.5 6 3.225 2.15 -1.075 0.35 ## 7 B4 1 3 C 2.0 1.0 7 3.225 2.15 -1.075 -0.15 ## 8 B4 1 5 A 4.8 3.1 8 3.225 4.30 1.075 0.50 ## trt_bar_min trt_bar_max err_bar_min err_bar_max ## 1 3.225 4.3 4.300 5.200 ## 2 -1.075 0.0 3.225 3.475 ## 3 -1.075 0.0 -1.525 -1.075 ## 4 3.225 4.3 4.300 4.800 ## 5 3.225 4.3 -1.900 0.000 ## 6 -1.075 0.0 3.225 3.575 ## 7 -1.075 0.0 -1.225 -1.075 ## 8 3.225 4.3 4.300 4.800 5.7.5 Plotting the Effects Below is the code to create a bar plot. This kind of bar plot is stacked  it shows multiple measures for each observation (plot), stacked on top each other. Normally, a stacked bar plot can be created in a couple of lines of code  the intricate code below was so I could customize the order of stacking for each plot. Run the code and observe the plot. ggplot(groundnut_effects) + geom_segment(aes(x=plot, y=0, xend=plot, yend=mu), color=&quot;lightblue&quot;, size=14) + geom_segment(aes(x=plot, xend=plot, y=trt_bar_min, yend=trt_bar_max), color=&quot;lightgreen&quot;, size=14) + geom_segment(aes(x=plot, xend=plot, y=err_bar_min, yend=err_bar_max), color=&quot;tomato&quot;, size=14) + # geom_point(aes(x=plot, y=y), size=4) + geom_text(aes(x=plot, y=mu/2, label= round(mu,1))) + geom_text(aes(x=plot, y=trt_bar_min+(trt_bar_max-trt_bar_min)/2, label=round(trt_effect,1))) + geom_text(aes(x=plot, y=err_bar_min+(err_bar_max-err_bar_min)/2, label=round(error_effect,1))) + geom_segment(aes(x=plot+0.4, xend=plot+0.2, y=y, yend=y), arrow = arrow(length = unit(0.01, &quot;npc&quot;)), size=1) + geom_text(aes(x=plot+0.4, y=y, label=round(y,1)), hjust=0) + scale_x_continuous(breaks=c(1:10)) Model plots 1, 5, and 6, using the linear model as we did above. 5.8 Exercise: One-Sided Hypotheses We learned in the lesson there are times when it is appropriate to use a one-sided hypothesis. A one-sided hypothesis specifies how two treatments will rank in a trial, for example that variety B will have greater yield than variety A: Ho: A &gt;= B Ha: A &lt; B A two-sided hypothesis, in contrast, only specifies that variety A and variety B will be different: Ho: A = B Ha: A &lt;&gt; B As we learned in the lecture, the one sided t-test requires a lesser difference for significance than the two-sided test. Given 9 degrees of freedom, and a standard error of the difference of 1, for example, a difference equal to or greater than 2.26  or equal to or less than -2.26  between treatments would need to be observed between treatments for the two-sided test to be significant. library(tidyverse) library(fastGraph) alpha_05_2side = qt(0.975, 9) shadeDist(c(-alpha_05_2side, alpha_05_2side), &quot;dt&quot;, parm2 = 9, lower.tail = TRUE) In a one-sided test, a lower difference, between treatments, 1.83, is required for significance at the p &lt; 0.05 level. alpha_05_1side = qt(0.95, 9) shadeDist(xshade=alpha_05_1side, &quot;dt&quot;, parm2 = 9, lower.tail = FALSE) 5.8.1 Case Study: Groundnut In this study, the wet weight of groundnut, in kg/plot, was measured for two genotypes, coded A and C. The plots were paired. groundnut = read.csv(&quot;data-unit-5/exercise_data/groundnut.csv&quot;) head(groundnut) ## block row col gen wet dry ## 1 B1 4 2 A 5.2 3.3 ## 2 B1 4 6 C 2.4 1.4 ## 3 B2 3 1 C 1.7 0.9 ## 4 B2 3 6 A 4.8 3.0 ## 5 B3 2 3 A 2.4 1.4 ## 6 B3 2 6 C 2.5 1.5 groundnut %&gt;% group_by(gen) %&gt;% summarise(wet = mean(wet)) ## # A tibble: 2 x 2 ## gen wet ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 4.3 ## 2 C 2.15 5.8.2 One-Sided T-Test In the last unit, we learned to use the t.test() function to conduct a paired two-sided t-test. Lets first analyze the groundnut data that way. Out hypotheses are t.test(wet ~ gen, groundnut, paired=TRUE) ## ## Paired t-test ## ## data: wet by gen ## t = 2.854, df = 3, p-value = 0.0649 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.2474194 4.5474194 ## sample estimates: ## mean of the differences ## 2.15 We see we have a p-value of 0.0649. The two genotypes do not produce different wet weights of groundnuts at the p &lt; 0.05 level of significance. Now lets run the one-sided test. To specify our hypothesis properly, we need to know which treatment will be the subtractant: the number that is subtracted. This is really important. In R, the treatment which comes second in alphabetical order is subtracted from the treatment that comes first. Lets say our hypotheses are these: Ho: A &gt;= C Ha: A &lt; C To tell R to run the t.test this way, we add the alternative = \"\" argument to our t-test. If A is greater than C, we will have a positive difference, so we specify alternative = greater. t.test(wet ~ gen, groundnut, paired=TRUE, alternative = &quot;greater&quot;) ## ## Paired t-test ## ## data: wet by gen ## t = 2.854, df = 3, p-value = 0.03245 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.3771502 Inf ## sample estimates: ## mean of the differences ## 2.15 We can now see the p-value for the test is 0.03  genotype C produces a greater wet weight of groundnut than geontype A at the p &lt; 0.05 level of significance. 5.8.3 Practice: Barley In this study, yield of a new and old genotype were compared. Treatments were paired. barley = read.csv(&quot;data-unit-5/exercise_data/barley.csv&quot;) Run a two-sided t.test to compare the yield of the two genotypes. You should get a p-value of 2.158e-06. Run a one-sided t.test to test the hypothesis the new hybrid yields greater than the old hybrid. Going by alphabetical order, R will subtract the mean of old from new. Given our hypothesis that the yield of the new genotype will be greater than that of the old, our difference will again be positive. Again, use the alternative=greater\"* argument with the t.test() function. Your answer should have a p-value = 1.079e-06. 5.8.4 Practice: Strawberry The yield of two strawberry genotypes was tested in a paired treatment design. strawberry = read.csv(&quot;data-unit-5/exercise_data/strawberry.csv&quot;) Test the difference between genotypes using a two-sided test. You should get p-value = 0.055. Test the hypothesis that genotype F is greater than genotype R1. Since R will subtract R1 from F, our difference will be positive. You should get p-value = 0.0275. 5.9 Exercise: Type I and Type II Errors Please use the following link to test whether you understand the difference between Type I and Type II errors. "],["multiple-treatment-trials.html", "Chapter 6 Multiple Treatment Trials 6.1 Case Study 6.2 The Linear Additive Model 6.3 Analysis of Variance 6.4 The F statistic 6.5 The ANOVA Table 6.6 Visualizing How the Anova Table Relates to Variance 6.7 Exercise: Treatment Means", " Chapter 6 Multiple Treatment Trials Here is our course, so far, in a nutshell: statistics is all about populations when we compare treatments, we are actually comparing populations that have been exposed to different products or management when we can measure every individual in that population, we can use the Z-distribution to describe their true population means and variance most of the time in agricultural research, however, we must use a samples (subsets from those populations) to estimate their population means and variance when we use the sample mean to estimate the population mean, we use the t-distribution to descibe the distribution of sample means around the mean of their values the distribution of sample means can be used to calculate the probability (the p-value) the true population mean is a hypothetical value in a paired t-test of two populations, we create a new population of differences, and calculate the probability its mean is zero proper hypotheses and alphas (maximum p-values for significance) can reduce the likelihood we conclude populations are different when they are likely the same, or the same when they are likely different I include this brutal distillation so you can see how the course has evolved from working with complete populations to samples, from true or certain estimates of population means to estimates, from working with single populations to comparing differences between two populations. In the last two units, we learned how to design and evaluate the results of side-by-side trials: trials in fields or parts of fields were divided into two populations that were managed with different treatments or practices. This was a practical, powerful, jumping-off point for thinking about experiments and their analyses. Lets face it, however: if we only compared two treatments per experiment in product testing or management trials, our knowledge would advance at a much slower pace. So in the next three units, we will learn how to design and evaluate trials to test multiple categorical treatments. By categorical, we mean treatments we identify by name, not quantity. Treatments that, in general, cannot be ranked. Hybrids are a perfect example of categorical treatments. Herbicides, fungicides, or fertilzers that differ in brand name or chemical composition are also categorical treatments. Comparisons of cultural practices, such as tillage systems or crop rotations are categorical treatments as well. 6.1 Case Study For our case study, we will look at a comparison of four hybrids from the Marin Harbur seed company. We will compare hybrids MH-052672, MH-050877, MH-091678, and MH-032990, which are not coded by relative maturity or parent lines, but represent some of the owners favorite Grateful Dead concerts: - MH-052672 has a great early disease package and is ideal for environments that have heavy morning dews - MH-050877 is very resilient in poor fertility soils and among the last to have its leaves fire under low nitrogen conditions. It can also grow well on everything from flatlands to mountains. - MH-091678 has a very strong root base that will not be shaken down by late-season wind storms - MH-032990 offers a very rich performance and responds well to additional inputs. (This one catches growers eyes at every field day  not to blow our own horn.) library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- ## v ggplot2 3.3.3 v purrr 0.3.4 ## v tibble 3.1.0 v dplyr 1.0.5 ## v tidyr 1.1.3 v stringr 1.4.0 ## v readr 1.4.0 v forcats 0.5.1 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() hybrid_data = read.csv(&quot;data-unit-6/grateful_data.csv&quot;) head(hybrid_data) ## plot_number Hybrid Yield col row ## 1 1 MH-050877 189.5 1 1 ## 2 2 MH-052672 186.4 1 2 ## 3 3 MH-091678 196.9 1 3 ## 4 4 MH-050877 191.2 1 4 ## 5 5 MH-091678 191.8 2 1 ## 6 6 MH-032990 198.9 2 2 The hybrids were grown in a field trial with four replications as shown below. hybrid_data %&gt;% ggplot(aes(x=col, y=row, label=Hybrid)) + geom_tile(fill=&quot;grey&quot;, color=&quot;black&quot;) + geom_text(aes(label = Hybrid)) The arrangement of treatments above is a completely randomized design. This means the hybrids were assigned at random among all plots  there was no grouping or pairing of treatments as in our side-by-side trials earlier. This design  to be honest  is used more often in greenhouse or growth chamber research where the soil or growing media are more uniform. Still, it is the most appropriate design to start with in discussing multiple treatment trials. 6.2 The Linear Additive Model In order to understand how we will analyze this trial, lets plot out our data. hybrid_rank = hybrid_data %&gt;% group_by(Hybrid) %&gt;% summarise(Yield = mean(Yield)) %&gt;% ungroup() %&gt;% arrange(Yield) %&gt;% mutate(rank = row_number()) %&gt;% select(-Yield) effects_data = hybrid_data %&gt;% select(-col, -row) %&gt;% mutate(mu = mean(Yield)) %&gt;% arrange(Hybrid) %&gt;% group_by(Hybrid) %&gt;% mutate(T = mean(Yield) - mu) %&gt;% ungroup() %&gt;% mutate(E = Yield - T - mu) %&gt;% left_join(hybrid_rank) ## Joining, by = &quot;Hybrid&quot; hybrids = unique(effects_data$Hybrid) effects_data %&gt;% ggplot(aes(x=rank, y=Yield)) + geom_point() + geom_text(aes(x=rank-0.1, y=mu+T, label=Hybrid, hjust=1)) + lims(x=c(0.5, 4.5)) Our treatment means are shown below: effects_data %&gt;% ggplot(aes(x=rank, y=T)) + geom_point() + geom_text(aes(x=rank-0.1, y=T, label=Hybrid, hjust=1)) + geom_text(aes(x=rank+0.1, y=T, label=round(T, 1)), hjust=0) + lims(x=c(0.5, 4.5)) We can see the plot effects as well. effects_data %&gt;% ggplot(aes(x=rank, y=T+E)) + geom_point()+ geom_text(aes(x=rank-0.1, y=T, label=Hybrid, hjust=1)) + geom_text(aes(x=rank+0.1, y=T+E, label=round(E, 1)), hjust=0, size=4) + lims(x=c(0.5, 4.5)) We can observe the linear model using this table: effects_data %&gt;% select(Hybrid, mu, T, E, Yield) %&gt;% mutate(mu = round(mu, 1), T = round(T, 1), E = round(E, 1)) %&gt;% rename(`Yield (mu + T + E)` = Yield) ## # A tibble: 16 x 5 ## Hybrid mu T E `Yield (mu + T + E)` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MH-032990 193. 6.8 -1.3 199. ## 2 MH-032990 193. 6.8 -1.1 199. ## 3 MH-032990 193. 6.8 2.7 203. ## 4 MH-032990 193. 6.8 -0.3 200. ## 5 MH-050877 193. -2.5 -1.4 190. ## 6 MH-050877 193. -2.5 0.2 191. ## 7 MH-050877 193. -2.5 0.9 192. ## 8 MH-050877 193. -2.5 0.4 191. ## 9 MH-052672 193. -7 0 186. ## 10 MH-052672 193. -7 -1.5 185. ## 11 MH-052672 193. -7 2 188. ## 12 MH-052672 193. -7 -0.5 186. ## 13 MH-091678 193. 2.7 0.8 197. ## 14 MH-091678 193. 2.7 -4.3 192. ## 15 MH-091678 193. 2.7 2 198. ## 16 MH-091678 193. 2.7 1.5 198. Just as in the side-by-side trials, our statistical test is based on the ratio of the variation among treatment means to the variation among observations within each treatment. library(grid) library(pBrackets) p = effects_data %&gt;% ggplot(aes(x=rank, y=T+E)) + geom_point(size = 2) + lims(x=c(0, 6)) p + geom_text(aes(x=1.5, y=-7), label=&quot;Does the spread of\\nindividuals within a treatment\\nexplain more of the variance?&quot;, hjust=0, vjust=0.5, size=5) + geom_text(aes(x=4.4, y=0), label=&quot;Or does the\\ndifference between\\ntreatment means\\nexplain more\\nof the variance?&quot;, hjust=0, vjust=0.5, size=5) grid.locator(unit=&quot;native&quot;) # grid.brackets(180, 300, 180, 375, h=0.05, lwd=2, col=&quot;red&quot;) grid.brackets(473, 60, 473, 335, h=0.05, lwd=2, col=&quot;red&quot;) 6.3 Analysis of Variance Chances are if you have spent time around agronomic research you have probably heard of ANOVA. A Nova No, not that fine example of Detroit muscle, but a statistical test, the Analysis of Variance (ANOVA). The ANOVA test performs the comparison described above when there are more than two more populations that differ categorically in their management. Ill admit the nature of this test was a mystery to me for years, if not decades. As the name suggests, however, it is simply an analysis (a comparison, in fact) of the different sources of variation as outlined in our linear additive model. An Analysis of Variance tests two hypotheses: Ho: There is no difference among population means. Ha: There is a difference among population means. In our hybrid trial, we can be more specific: Ho: There is no difference in yield among populations planted with four different hybrids. Ha: There is a difference in yield among populations planted with four different hybrids. But the nature of the hypotheses stays the same. 6.4 The F statistic The Analysis of Variance compares the variance from the treatment effect to the variance from the error effect. It does this by dividing the variance from treatments by the variance from Error: \\[F = \\frac{\\sigma{^2}_{treatment}}{\\sigma{^2}_{error}} \\] The \\(F-value\\) quanitfies the ratio of treatment variance to error variance. As the ratio of the variance from the treatment effect to the variance from the error effect increases, so does the F-statistic. In other words, a greater F-statistic suggests a greater treatment effect  or  a smaller error effect. 6.5 The ANOVA Table At this point, it is easier to explain the library(broom) hybrid_test = tidy(aov(Yield ~ Hybrid, hybrid_data)) hybrid_test ## # A tibble: 2 x 6 ## term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Hybrid 3 434. 145. 38.4 0.00000197 ## 2 Residuals 12 45.2 3.76 NA NA summary(hybrid_test) ## term df sumsq meansq ## Length:2 Min. : 3.00 Min. : 45.17 Min. : 3.764 ## Class :character 1st Qu.: 5.25 1st Qu.:142.41 1st Qu.: 39.000 ## Mode :character Median : 7.50 Median :239.65 Median : 74.237 ## Mean : 7.50 Mean :239.65 Mean : 74.237 ## 3rd Qu.: 9.75 3rd Qu.:336.89 3rd Qu.:109.473 ## Max. :12.00 Max. :434.13 Max. :144.709 ## ## statistic p.value ## Min. :38.44 Min. :2e-06 ## 1st Qu.:38.44 1st Qu.:2e-06 ## Median :38.44 Median :2e-06 ## Mean :38.44 Mean :2e-06 ## 3rd Qu.:38.44 3rd Qu.:2e-06 ## Max. :38.44 Max. :2e-06 ## NA&#39;s :1 NA&#39;s :1 As we did with the t-test a couple of units ago, lets go through the ANOVA output column by column, row by row. 6.5.1 Source of Variation The furthest column to the left, term specifies the two effects in our linear additive model: the Hybrid and Residual effects. As mentioned in the last chapter, the term Residual is often used to describe the leftover variation among observations that a model cannot explain. In this case, it refers to the variation remaining when the Hybrid effect is accounted for. This column is also often referred to as the Source of Variation column. 6.5.2 Sum of Squares Lets skip the df column for a moment to expain the column titled sumsq in this output. This column lists the sums of squares associated with the Hybrid and Residual Effect. Remember, the sum of squares is the sum of the squared differences between the individuals and the population mean. Also, we need to calculate the sum of squares before calculating variance The Hybrid sum of squares based on the the difference between the treatment mean and the population mean for each observation in the experiment. In the table below we have mu, the population mean, and T, the effect or difference between the treatment mean and mu. We square T for each of the 16 observations to create a new column, T-square trt_ss = effects_data %&gt;% select(Hybrid, mu, T) %&gt;% mutate(`T-square` = T^2) # %&gt;% # mutate(mu = round(mu, 1), # T = round(T, 1)) trt_ss ## # A tibble: 16 x 4 ## Hybrid mu T `T-square` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MH-032990 193. 6.79 46.1 ## 2 MH-032990 193. 6.79 46.1 ## 3 MH-032990 193. 6.79 46.1 ## 4 MH-032990 193. 6.79 46.1 ## 5 MH-050877 193. -2.46 6.06 ## 6 MH-050877 193. -2.46 6.06 ## 7 MH-050877 193. -2.46 6.06 ## 8 MH-050877 193. -2.46 6.06 ## 9 MH-052672 193. -7.01 49.2 ## 10 MH-052672 193. -7.01 49.2 ## 11 MH-052672 193. -7.01 49.2 ## 12 MH-052672 193. -7.01 49.2 ## 13 MH-091678 193. 2.69 7.22 ## 14 MH-091678 193. 2.69 7.22 ## 15 MH-091678 193. 2.69 7.22 ## 16 MH-091678 193. 2.69 7.22 We then sum the squares of T to get the Hybrid sum of squares. sum(trt_ss$`T-square`) ## [1] 434.1275 We use a similar approach to calculate the Error (or Residual) sum of squares. This time we square the error effect (the difference between the observed value and the treatment mean) for each observation in the trial. err_ss = effects_data %&gt;% select(Hybrid, mu, E) %&gt;% mutate(`E-square` = E^2) err_ss ## # A tibble: 16 x 4 ## Hybrid mu E `E-square` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MH-032990 193. -1.30 1.69 ## 2 MH-032990 193. -1.10 1.21 ## 3 MH-032990 193. 2.7 7.29 ## 4 MH-032990 193. -0.300 0.0900 ## 5 MH-050877 193. -1.45 2.10 ## 6 MH-050877 193. 0.25 0.0625 ## 7 MH-050877 193. 0.85 0.723 ## 8 MH-050877 193. 0.35 0.123 ## 9 MH-052672 193. 0 0 ## 10 MH-052672 193. -1.5 2.25 ## 11 MH-052672 193. 2 4 ## 12 MH-052672 193. -0.5 0.25 ## 13 MH-091678 193. 0.8 0.64 ## 14 MH-091678 193. -4.30 18.5 ## 15 MH-091678 193. 2 4 ## 16 MH-091678 193. 1.5 2.25 We again sum the squared error effects to get the Error or Residual sum of squares. sum(err_ss$`E-square`) ## [1] 45.17 6.5.3 Degrees of Freedom The df column above refers to the degrees of freedom. Remember, the variance is equal to the sum of squares, divided by its degrees of freedom. The hybrid sum of squares is simply the number of treatments minus 1. In this example, there were 4 hybrids, so there were three degress of freedom for the Hybrid effect. The concept behind the hybrid degrees of freedom is that if we know the means for three hybrids, as well as the population mean, then we can calculate the fourth hybrid mean, as it is determined by the first three hybrids and the population mean. Degrees of freedom are a weird concept, so try not to overanalyze them. The degrees of freedom for the error or residual effect are a little more confusing. The degrees of freedom are equal to the Hybrid degrees of freedom, times the number of replications. In this case, the error degrees of freedom are 12. The idea behind this is: if for a hybrid you know the values of three observations, plus the hybrid mean, you can calculate the value of the fourth observation. 6.5.4 Mean Square In the Analysis of Variance, the Sum of Squares, divided by the degrees of freedom, is referred to as the Mean Square. As we now know, the mean squares are also the variances attributable to the Hybrid and Error terms of our linear model. Our hybrid mean square is about 144.7; the error mean square is about 3.8. 6.5.5 F-Value THe F-Value, as introduced earler, is equal to the hybrid variance, divided by the error variance. In the ANOVA table, F is calculated as the hybrid mean square divided by the error mean square. When the F-value is 1, it means the treatment effect and error effect have equal variances, and equally describe the variance among observed values. In other words, knowing the treatment each plot received adds nothing to our understanding of observed differences. 6.5.6 P-value The F-value is the summary calculation for the relative sizes of our Hybrid and Error variances. Its value is 38.4, which means the Hybrid variance is over 38 times the size of the Error variance. In other words, the Hybrid variance accounts for much, much more of the variation in our observations than the Error variance. But, given this measured F-value, what is the probability the true F-value is 1, meaning the Hybrid and Error variances are the same? To calculate the probability our F-value could be the product of chance, we use the F-distribution. The shape of the F-distribution, like the t-distribution, changes with the number of replications (which changes the Error degrees of freedom). It also changes with the treatment degrees of freedom. Please click the link below to access an app where you will be able to adjust the number of treatments and number of replications: [link] Adjust those two inputs and observe the change in the response curve. In addition, adjust the desired level of significance and obsere how the shaded area changes. Please ignore the different color ranges under the curve when you see them: any shaded area under the curve indicates significance. The F-distribution is one-tailed  we are only interested in the proportion remaining in the upper tail. If we were to visualize the boundary for the areas representing \\(P\\ge0.05\\) for our example above, we would test whether F was in the following portion of the tail. As we can see, our observed F of 38.4 is much greater than what we would need for significance at \\(P\\ge0.05\\). What about \\(P\\ge0.01\\)? library(sjPlot) dist_f(p = 0.01, deg.f1 = 3, deg.f2 = 12, xmax = 10) Our value is also way beyond the F-value we would need for \\(P\\ge0.05\\). 6.6 Visualizing How the Anova Table Relates to Variance Please follow the following link to an app that will allow you to simulate a corn trial with three treatments: [link] Use your observations to address the following four questions in Discussion 6.1: What do you observe when distance between the trt means increases? what do you observe when the pooled standard deviation decreases? Set the treatment means to treatment 1 = 180, treatment 2 = 188, and treatment 3 = 192. What do you observe about the shapes of the distribution curve for the treatment means (black curve) and treatment 2 (green curve)? What does an F-value of 1 mean? ##Exercise: Completely Randomized Design Anova In this unit we were introduced to the Analysis of Variance (ANOVA) and the most basic experimental design, the Completely Randomized Design. Running an analysis of variance for this trial is very easy, as we will now see. 6.6.1 Case Study: Barley barley_data = read.csv(&quot;data-unit-6/exercise_data/barley_crd.csv&quot;) head(barley_data) ## rep gen yield ## 1 S1 G01 0.05 ## 2 S1 G02 0.00 ## 3 S1 G03 0.00 ## 4 S1 G04 0.10 ## 5 S1 G05 0.25 ## 6 S1 G06 0.05 6.6.2 ANOVA There are two steps to conducting a basic ANOVA analysis. First, define the model. We will use the lm() function to create a linear model. We need to provide two arguments to lm(). First, the model itself. We are going to model yield as a function of gen, that is, that yield differs among levels of gen. We express this as yield~gen The second argument is simply the dataset in which these variables are located: barley. barley_model = lm(yield~gen, data = barley_data) The second step is to use the command anova() to generate an ANOVA table from the model. anova(barley_model) ## Analysis of Variance Table ## ## Response: yield ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gen 9 19604 2178.25 3.599 0.0008369 *** ## Residuals 80 48420 605.24 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Its that simple. 6.6.3 Calculating the Coefficient of Variation The Coefficient of Variation (CV) is a quality-control measurement for research. It essentially asks: were our experimental units (plots or pots) consistent enough for us to properyl observe the difference among treatments? Or was our trial, in essence, sloppy? Mathematically, the CV is calculated as: \\[CV = \\frac{\\sqrt{EMS}}{\\mu} \\times 100 \\] Where MSE is the Error (or Residual) Mean Square and \\(\\mu\\) is the population mean. In the barley example, the EMS is 605.2. THe population mean for yield is 20.71 So the CV is equal to: \\[CV = \\frac{\\sqrt{605.2}}{20.71} = \\frac{24.60}{20.71} = 118.75 \\] We can quickly calculate this for any model we create using the cv.model() function in the agricolae package. library(agricolae) cv.model(barley_model) ## [1] 118.7551 This is a really, really large CV. In fact, I checked my math a few times before I accepted it was correct. In corn research, I like to see a CV below 10. For soybean I prefer a CV below 5. Any greater than that, and a trial will be closely examined and potentially excluded from analysis. Before using a CV to evaluate the quality of a trial, however, you should be familiar with CVs experienced by previous researchers in your discipline. 6.6.4 Practice: Beet Data beet_data = read.csv(&quot;data-unit-6/exercise_data/beets_crd.csv&quot;) head(beet_data) ## plot fert yield ## 1 1 None 2.45 ## 2 2 P 6.71 ## 3 3 K 3.22 ## 4 4 PK 6.34 ## 5 5 PN 6.48 ## 6 6 KN 3.70 Model the relationship of yield and fert by completing the analysis of variance below. Your Pr(&gt;F) for fert should be 1.299e-10. You will need to uncomment (delete the hashtags) to run this code. # beet_model = lm( , data = ) # summary.aov(beet_model) Calculate the Coefficient of Variance (CV) using the cv.model() function. Your CV should be 17.38. # cv.model() 6.6.5 Practice: Potato Data potato_data = read.csv(&quot;data-unit-6/exercise_data/potato_crd.csv&quot;) head(potato_data) ## inf trt row col ## 1 9 F3 4 1 ## 2 12 O 4 2 ## 3 18 S6 4 3 ## 4 10 F12 4 4 ## 5 24 S6 4 5 ## 6 17 S12 4 6 Model the relationship between infection (inf) and treatment (trt) using the analysis of variance. You should get Pr(&gt;F) = 0.0103. Calculate the Coefficient of Variance (CV) using the cv.model() function. 6.7 Exercise: Treatment Means The next step in a multiple treatment trial is to summarise the treatment means. Below you will learn how to calculate means and display them using a bar plot. 6.7.1 Case Study: Barley This is the same dataset we used in the previous exercise. library(tidyverse) barley_data = read.csv(&quot;data-unit-6/exercise_data/barley_crd.csv&quot;) head(barley_data) ## rep gen yield ## 1 S1 G01 0.05 ## 2 S1 G02 0.00 ## 3 S1 G03 0.00 ## 4 S1 G04 0.10 ## 5 S1 G05 0.25 ## 6 S1 G06 0.05 6.7.2 Calculating Treatment Means We can create a new dataset, containing treatment means, using another couple of functions from the tidyverse package: group_by() and summarise(). We will also use the pipe, %&gt;%, to feed the results of one line to the next. We start with barley %&gt;%, which feeds the barley dataset to the next line. There, we use the group_by() command to tell R we want to group the observations by gen (genetics). This means any summary calculations on the data will be done separately by level for the gen factor. Finally, we tell the data which variables to summarise. We use the summarise command to tell R to create a new variable, yield_mean, which is equal to the mean of observed yield values for each level of gen. barley_means = barley_data %&gt;% group_by(gen) %&gt;% summarise(yield_mean=mean(yield)) barley_means ## # A tibble: 10 x 2 ## gen yield_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 G01 4.2 ## 2 G02 4.77 ## 3 G03 7.34 ## 4 G04 9.57 ## 5 G05 14 ## 6 G06 21.8 ## 7 G07 24.2 ## 8 G08 34.8 ## 9 G09 37.2 ## 10 G10 49.3 We can see now that our values range from 4.20 to 49.33. Since the original data only had two decimal places, we should probably round these results. We can do that quickly using the mutate() and round() commands to our code. mutate tells R to create a new column. The round() function takes two arguments: the variable to round, and the number of decimal places to round to. In this case, we will create a new variable with the same name, yield_mean, as the old variable. The value of the new yield mean will be equal to the old yield mean, rounded to two decimal places. barley_means = barley_data %&gt;% group_by(gen) %&gt;% summarise(yield_mean=mean(yield)) %&gt;% mutate(yield_mean = round(yield_mean, 2)) barley_means ## # A tibble: 10 x 2 ## gen yield_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 G01 4.2 ## 2 G02 4.77 ## 3 G03 7.34 ## 4 G04 9.57 ## 5 G05 14 ## 6 G06 21.8 ## 7 G07 24.2 ## 8 G08 34.8 ## 9 G09 37.2 ## 10 G10 49.3 6.7.3 Plotting the Means We can use the ggplot() function in R to plot our results. The first line, the ggplot() statement, uses data = to define the data frame, and the aes() argument to declare the aesthetics to be plotted. Aesthetics are any information in the chart whose position or appearance are dependent on variables from the dataset. In other words, anything in the chart that might change with the value of an observation. In this case, the x-position of our data to be plotted will be defined by gen. The y-position will be defined by yield. The second line defines a geom, the type of geometry that will be used to represent the data. In this case, we will use geom_bar() to contstruct a bar plot. The final thing we need to do is to tell R whether the height of each bar should be determined by the number of observations, another statistical summary, or the value for y we declared in the initial ggplot statement. In this case, we want to use the value we declared above, yield_mean, to determine the top of the bars. The stats=identity tells R to do just that. ggplot(data=barley_means, aes(x=gen, y=yield_mean)) + geom_bar(stat=&quot;identity&quot;) There are all sorts of things we can do in R with our plots. Say we wanted to change the labels on the x or y axes. We just add those using the labs() option: ggplot(data=barley_means, aes(x=gen, y=yield_mean)) + geom_bar(stat=&quot;identity&quot;) + labs(x = &quot;Genotype&quot;, y = &quot;Yield Mean&quot;) We can change the bar color by adding the fill() command to geom_bar. ggplot(data=barley_means, aes(x=gen, y=yield_mean)) + geom_bar(stat=&quot;identity&quot;, fill=&quot;darkgreen&quot;) + labs(x = &quot;Genotype&quot;, y = &quot;Yield Mean&quot;) We can include the actual value above each bar by adding a second geometry, geom_text. The aes() argument to geom_text tells it what variable to use as the label (it also tells it where to place the label). In addition to the aes() argument, vjust=-0.03 tells R to adjust the label upward a few points so it doesnt touch the bar. The font size is increased with size = 3.5. ggplot(data=barley_means, aes(x=gen, y=yield_mean)) + geom_bar(stat=&quot;identity&quot;, fill=&quot;darkgreen&quot;) + labs(x = &quot;Genotype&quot;, y = &quot;Yield Mean&quot;) + geom_text(aes(label=yield_mean), vjust=-0.3, size=3.5) 6.7.4 Practice: Beet Data beet_data = read.csv(&quot;data-unit-6/exercise_data/beets_crd.csv&quot;) head(beet_data) ## plot fert yield ## 1 1 None 2.45 ## 2 2 P 6.71 ## 3 3 K 3.22 ## 4 4 PK 6.34 ## 5 5 PN 6.48 ## 6 6 KN 3.70 Calculate the yield mean by fertilizer level by completing the code below. Remember, you will need to uncomment (remove hashtags from) the code before you can run it. (Hint: you can add or remove multiple lines of comments by highlighting those lines and typing CTRL-SHIFT-C.) # beet_means = beet_data %&gt;% # group_by() %&gt;% # summarise() Create a simple bar plot using ggplot() and geom_bar(). # ggplot(data=beet_means, aes(x= , y= )) + # geom_bar(stat = ) 6.7.5 Practice: Potato Data potato_data = read.csv(&quot;data-unit-6/exercise_data/potato_crd.csv&quot;) head(potato_data) ## inf trt row col ## 1 9 F3 4 1 ## 2 12 O 4 2 ## 3 18 S6 4 3 ## 4 10 F12 4 4 ## 5 24 S6 4 5 ## 6 17 S12 4 6 Calculate the mean infection (inf) by treatment (trt) and assign it to the data frame potato_means. Use the potato_means data frame to create a simple bar plot. "],["multiple-treatment-designs.html", "Chapter 7 Multiple Treatment Designs 7.1 Randomized Complete Block Design 7.2 Factorial Design 7.3 Split-Plot Design 7.4 Linear Additive Model 7.5 Conclusion 7.6 Exercise: Randomized Complete Block Design 7.7 Exercise: Factorial ANOVA 7.8 Exercise: Split-Plot Design 7.9 Exercise: Experimental Design", " Chapter 7 Multiple Treatment Designs In the last unit, we were introduced to multiple-treatment experiments, using an example with which many of you are familiar: a hybrid research or demonstration trial. Recall how the analysis of variance worked: we compared two sources of variation to see how much of that variation each of them explained. There were two effects in our original trial: treatment and error \\[ Y_{ij}=\\mu + T_i + \\epsilon_{i(j)}\\] The Analysis of Variance (ANOVA) was used to calculate and compare these variances. First, the sums of squares from the treatment means and the error (the summed distributions of observations around each treatment mean) were calculated. By dividing the sum of squares by their degrees of freedom, the we obtained the treatment and error mean squares, also known as their variances. The F-value was derived from the ratio of the treatment variance to the error variance. Finally, the probability that the difference among treatments was zero, given the F-value we observed, was calculated using the F-distribution. This experimental design is known as a Completely Randomized Design. It is the simplest multiple-treatment design there is. In this unit, we will learn two other designs commonly used in trials: Randomized Complete Block Design Two-Way Factorial Design These designs, we will see, use additional sources of variation to either expand the number of treatments we can evaluate, or reduce the error (unexplained variation) in our trials so that we can better identify treatment effects. 7.1 Randomized Complete Block Design If you have participated in agronomic research, you have likely heard references to a Randomized Complete Block Design (RCBD). We first discussed blocking when we learned about side-by-side trials. When we block treatments, we force treatments to occur in closer proximity to each other than they likely would were they assigned at random. The best way to understand this is to look at a plot map. library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- ## v ggplot2 3.3.3 v purrr 0.3.4 ## v tibble 3.1.0 v dplyr 1.0.5 ## v tidyr 1.1.3 v stringr 1.4.0 ## v readr 1.4.0 v forcats 0.5.1 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() col = rep(1:4, each=4) row = rep(1:4, 4) block = rep(1:4, each=4) set.seed(5) crd = sample(col, 16, replace = FALSE) rcbd_list = list() for(i in c(1:4)){ set.seed(i) z = sample(1:4, 4, replace = FALSE) rcbd_list[[i]] = z } rcbd = do.call(c, rcbd_list) dataset = cbind(col, row, block, crd, rcbd) %&gt;% as.data.frame() %&gt;% gather(design, treatment, crd, rcbd) %&gt;% mutate(design = toupper(design)) %&gt;% mutate(block_effect = case_when(col==1 ~ 3, col==2 ~ 1, col==3 ~ -1, col==4 ~ -3)) dataset %&gt;% ggplot(aes(x=col, y=row, label=treatment)) + geom_tile(fill=&quot;grey&quot;, color=&quot;black&quot;) + geom_text(aes(label = treatment)) + facet_grid(. ~ design) In the plot above, the Completely Randomized Design (CRD) is shown on the left, and the Randomized Complete Block Design (RCBD) on the right. In the Completely Randomized Design, any treatment can occur anywhere in the the plot. Note that in the left plot, treatment 3 occurs twice in the first column of plots while treatment 2 does not occur at all. Treatment 2 occurs twice in the second column, but there is no treatment 1. In the Randomized Complete Block Design, each treatment must occur once, and only once, per column. In this case, the treatments are blocked on column. Why block? Lets suppose each column in the plot map above has a different soil type, with soils transitioning from more productive to less productive as columns increase from 1 to 4: dataset %&gt;% ggplot(aes(x=col, y=row, label=treatment)) + geom_tile(aes(fill=as.factor(col)), color=&quot;black&quot;) + geom_text(aes(label = treatment)) + scale_fill_manual(values = c(&quot;lightgreen&quot;, &quot;yellow2&quot;, &quot;orange1&quot;, &quot;tomato&quot;)) + facet_grid(. ~ design) + theme(legend.position = &quot;none&quot;) Note that treatment 3 occurs three times in the more productive soils of columns 1 and 2, and only once in the less productive soils of columns 3 and 4. Conversely, treatment 1 occurs three times in the less productive soils of columns 3 and 4, but only once in the more productive soil of columns 1 or 2. If the mean effect of treatment 3 is greater than the mean effect of treatment 1, how will we distinguish the effects of treatment and error? It is a moot question: we cant. Our linear model is additive: \\[ Y_{ij}=\\mu + T_i + \\epsilon_{i(j)}\\] The term additive is important: it means we assume that treatment and error effects do not interact  they independently add or subtract from the population mean. If the measured effect of treatment is dependent on plot error, the model fails. The plot on the right has blocked treatments according to column. Doing that allows us to remove the effect of soil type, which consistently varies from column to column, from the effect of error, which is random. Our linear model changes as well: \\[ Y_{ij}=\\mu + B_i + T_j + BT_{ij}\\] Where \\(Y_{ij}\\) is the individual value, \\(\\mu\\) is the population mean, \\(B_i\\) is the block effect, \\(T_j\\) is the treatment effect, and \\(BT_{ij}\\) is the interaction of block and treatment, also known as the error effect. 7.1.1 Case Study: Randomized Complete Block Design A field trial outside Goshen, Indiana, evaluated the effect of seed treatments on soybean yield. Treatments were as follows: - A: untreated - B: metalaxyl only - C: metalaxyl + insecticide - D: metalaxyl + insecticide + nematicide Treatments were arranged in a Randomized Complete Block Design. library(tidyverse) st = data.frame(obs=c(1:16)) set.seed(041383) mu = rep(c(68.3)) Block = rep(1:4, each=4) B = rep(c(-2.2, -0.8, 0.3, 1.2), each = 4) Treatment = rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;), 4) T = rep(c(-1.1, -0.3, 0.8, 1.6), 4) BT = rnorm(n=16, mean = 0, sd = 1) st_data = st %&gt;% cbind(Block, Treatment, mu, B, T, BT) %&gt;% mutate(BT = round(BT,1)) %&gt;% mutate(Y = mu+B+T+BT) %&gt;% group_by(Block) %&gt;% sample_n(4) %&gt;% ungroup() %&gt;% select(-obs) %&gt;% mutate(Block = as.factor(Block)) st_data$row = rep(1:4, 4) st_data %&gt;% mutate(plot_label = paste0(&quot;Treatment &quot;, Treatment, &quot;\\n&quot;, Y)) %&gt;% ggplot(aes(x=block, y=row, label=plot_label)) + geom_tile(fill=&quot;grey&quot;, color=&quot;black&quot;) + geom_text(aes(label = plot_label)) 7.1.1.1 Linear Additive Model In this example, the linear additive model is: \\[ Y_{ij}=\\mu + B_i + T_j + BT_{ij}\\] Or, with regard to our particular trial: \\[ Yield = Population \\space Mean + Block \\space Effect + Treatment \\space Effect + Block \\times Treatment \\space Interaction\\] We can see how the additive model works in the following table: st_effects = st_data %&gt;% select(Block, row, Treatment, mu, B, T, BT, Y) knitr::kable(st_effects) Block row Treatment mu B T BT Y 1 1 D 68.3 -2.2 1.6 -1.1 66.6 1 2 A 68.3 -2.2 -1.1 -0.8 64.2 1 3 C 68.3 -2.2 0.8 0.2 67.1 1 4 B 68.3 -2.2 -0.3 -1.5 64.3 2 1 C 68.3 -0.8 0.8 -0.4 67.9 2 2 A 68.3 -0.8 -1.1 -0.4 66.0 2 3 D 68.3 -0.8 1.6 -1.3 67.8 2 4 B 68.3 -0.8 -0.3 -0.1 67.1 3 1 C 68.3 0.3 0.8 1.0 70.4 3 2 B 68.3 0.3 -0.3 1.5 69.8 3 3 A 68.3 0.3 -1.1 1.3 68.8 3 4 D 68.3 0.3 1.6 -1.6 68.6 4 1 A 68.3 1.2 -1.1 -0.1 68.3 4 2 B 68.3 1.2 -0.3 2.0 71.2 4 3 D 68.3 1.2 1.6 0.4 71.5 4 4 C 68.3 1.2 0.8 0.1 70.4 In the first row of the table, we see that the observed yield, Y, is: \\[ Y = 68.3 + (-2.2) + (1.3) + (-1.1) = 66.3 \\] Similarly, in the fifth row: \\[ Y = 68.3 + (-0.8) + (0.5) + (-0.4) = 67.6 \\] 7.1.1.2 Analysis of Variance We can use the linear additive model above with R to create the proper linear model: model = aov(Y ~ Block + Treatment, st_data) anova(model) ## Analysis of Variance Table ## ## Response: Y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Block 3 56.250 18.7500 23.7175 0.0001313 *** ## Treatment 3 10.485 3.4950 4.4209 0.0359018 * ## Residuals 9 7.115 0.7906 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that in the above model, we only specify the Block and Treatment sources of variation. Any source of variation not included in the model and, if you will leftover from the model, is pooled into the Residuals, or error. In the model above, the interaction of Block and Treatment (BT) is not specified, for it is the source of any variation we observe from plot to plot. Two sources of variation are tested above: Block and Treatment. The F-value for both is calculated by dividing their Mean Square by the Residual (or Error) Mean Square. The probability that the difference among blocks or treatment is zero, given their observed F-value, is reported in the \\(Pr(&gt;F)\\) column. The Block effect is usually of less interest in analyzing table results. If it is insignificant, that may indicate we dont need to block in this location in the future, but in the vast majority of trials there will be at least some benefit to blocking. The most important effect of blocking is seen upon examining the Sum of Squares (Sum Sq) column. Here we can see just how much the Residual Sum of Squares was reduced by including blocks in the model. Had we not included the Block term in our model, our Residual Sum of Squares would have been about 63.4. Even given the greater residual degress of freedom (which would have included the three degrees of freedom that were assigned to Block, the residual mean square would have been about \\(64\\div12 = 5.3\\). Without even calculating F, we can see the error mean square would have been larger than the treatment mean square, meaning there was more variance within treatments than between them. The Treatment effect would not have been significant. 7.2 Factorial Design Agronomy is all about interactions. How do hybrids differ in their response to nitrogen? Response to fungicide? Does a fungicide increase yield more when it is sprayed on corn at V5 or VT? Does the effect of a starter fertilizer depend whether it is applied in-row or in a 2x2 band? How does a crop respond to popuation in 30-inch rows vs 15-inch rows? To grow a successful crop requires not a single input, but dozens, some of which we can manage and others we cant. So the issue of how different treatments interact is critical. It is also often more efficient and informative for us to study these interactions together in a single trial, than to study them separately in two or more trials. Before we go further, some nomenclature. In factorial design, treatments that differ in a single variable are called levels, and these levels together compose a factor. Here are some examples of levels and factors: multiple micronutrient products all applied at the V5 stage in corn  the product composition is the level, the timing the factor a single fungicide product, applied at V5, V10, or VT in corn  the timing is the level, the fungicide composition the factor a single adjuvant, tested with different nozzles  the nozzle is the level, the adjuvant composition the factor multiple hybrids, grown in rotation following soybean  the hybrid is the level, the crop rotation is the factor As you may have anticipated, a factorial design combines two or more factors, each containing two or more levels. For example: Factor Composition includes two levels of foliar micronutrient product: Mn and Zn Factor Timing\" includes two levels of timing: V5 and VT In a factorial designm, every level of factor Composition will occur with every level of factor Timing. We can visualize these treatment combinations the same way we might visualize a Punnet square in Mendalian genetics. The main effects are given on the axes and the particular treatment combinations are in the cells. factorial_trts_1 = data.frame(Timing = c(&quot;V5&quot;, &quot;VT&quot;, &quot;V5&quot;, &quot;VT&quot;), Composition = c(&quot;Mn&quot;, &quot;Mn&quot;, &quot;Zn&quot;, &quot;Zn&quot;), trt = c(&quot;Mn @ V5&quot;, &quot;Mn @ VT&quot;, &quot;Zn @ V5&quot;, &quot;Zn @ VT&quot;)) factorial_trts_1 %&gt;% ggplot(aes(x=Timing, y=Composition)) + geom_tile(color=&quot;black&quot;, fill=&quot;grey&quot;) + geom_text(aes(label = trt), size=6) + theme(axis.title = element_text(size=18), axis.text = element_text(size=14)) In another example: - Factor Composition consists of two adjuvant ingredients: guar (G) or an polyacrylamide (P) - Nozzles are Flat Fan (F) or AI nozzle (A) Our treatments in the factorial design, then, are: factorial_trts_2 = data.frame(Nozzle = c(&quot;FF&quot;, &quot;AI&quot;, &quot;FF&quot;, &quot;AI&quot;), Composition = c(&quot;G&quot;, &quot;G&quot;, &quot;P&quot;, &quot;P&quot;), trt = c(&quot;FF x G&quot;, &quot;AI x G&quot;, &quot;FF x P&quot;, &quot;AI x P&quot;)) factorial_trts_2 %&gt;% ggplot(aes(x=Nozzle, y=Composition)) + geom_tile(color=&quot;black&quot;, fill=&quot;grey&quot;) + geom_text(aes(label = trt), size=6) + theme(axis.title = element_text(size=18), axis.text = element_text(size=14)) 7.2.1 Case Study 1 Our case study is a clover forage trial conducted in New Zealand from 1992 to 1994. This dataset is publically available as part of the agridat package in R. For this first case study, we will focus on a subset of the data. The two factors were sulfur (S) and phosphorus (P) fertilizer. Sulfur was applied at 0, 11.25, or 22.5 kg/ha, while phosphorus was applied at 0, 40, and 80 kg/ha. Yield is reported in tons/hectare. Factorial trials are often nicknames by the number of levels of each factor. In this case, we have a \\(three-by-three\\), or \\(3\\times3\\) trial. We can visualize the factorial combinations as producing the following nine treatments: factorial_trts_3_main = data.frame(Sulfur = rep(c(0, 11.25, 22.5),3), Phosphorus = rep(c(0,80,160), each=3)) factorial_trts_3_combos = factorial_trts_3_main %&gt;% mutate(trt = paste0(&quot;S = &quot;, Sulfur, &quot;\\n&quot;, &quot;P = &quot;, Phosphorus)) factorial_trts_3_combos %&gt;% ggplot(aes(x=Sulfur, y=Phosphorus)) + geom_tile(color=&quot;black&quot;, fill=&quot;grey&quot;) + geom_text(aes(label = trt), size=6) + theme(axis.title = element_text(size=18), axis.text = element_text(size=14)) library(agridat) library(tidyverse) clover = sinclair.clover clover_dataset = clover %&gt;% filter(S %in% c(0, 11.25, 22.50)) %&gt;% filter(P %in% c(0, 80, 160)) %&gt;% mutate(S = as.factor(S), P = as.factor(P)) clover_data = data.frame(block = rep(1:4, each=9), treatment = rep(1:9, 4), sulfur = rep(clover_dataset$S,4), phosphorus = rep(clover_dataset$P,4), s_p_means = rep(clover_dataset$yield,4)) clover_effects = clover_data %&gt;% mutate(mu = mean(s_p_means)) %&gt;% group_by(sulfur) %&gt;% mutate(S = mean(s_p_means) - mu) %&gt;% ungroup() %&gt;% group_by(phosphorus) %&gt;% mutate(P = mean(s_p_means) - mu) %&gt;% ungroup() %&gt;% group_by(sulfur, phosphorus) %&gt;% mutate(SP = mean(s_p_means) - (mu + S + P)) %&gt;% ungroup() %&gt;% select(-treatment, -s_p_means) %&gt;% mutate(mu = round(mu,2), S = round(S,2), P = round(S,2), SP = round(SP,2)) set.seed(073020) clover_final = clover_effects %&gt;% mutate(Error = rnorm(36, 0, 0.4)) %&gt;% mutate(Error = round(Error, 1)) %&gt;% mutate(Yield = mu + S + P + SP + Error) When we deal with factorial designs, it is important to visualize the data. We can observe the data patterns using a line plot. We see clover yield increased with sulfur and phosphorus. We notice, however, the difference in yield between P=80 and P=160 is greater when S=0 than when S=11.25 or S=22.5. We also notice the difference betwee sulfur=0 and sulfur=22.5 is greater for P=160 than P=0. clover_final %&gt;% group_by(sulfur, phosphorus) %&gt;% summarise(Yield = mean(Yield)) %&gt;% ungroup() %&gt;% ggplot(aes(x=sulfur, y=Yield, group=phosphorus)) + geom_line(aes(color=phosphorus), size=2) + theme(axis.text = element_text(size=12), axis.title = element_text(size=14), legend.text = element_text(size=12), legend.title = element_text(size=14)) ## `summarise()` has grouped output by &#39;sulfur&#39;. You can override using the `.groups` argument. These differences are called interactions and are the most interesting part of a factorial design. In this case, it is no surprise that crop yield increases with sulfur fertiliztion in a sulfur soil. It is interesting, however, that the response to sulfur is appears to be dependent on phosphorus fertilization, and lends support to Liebigs Law of the Minimum, which states that addition of a particular nutrient will not increase yield if other nutrients are more deficient. 7.2.1.1 Linear Additive Model For our trial with two factors, the linear model is: \\[ Y_{ijk} = \\mu + S_i + P_j + SP_{ij} + \\epsilon_{ij(k)} \\] Each observed value, then, is the sum (additive value) of the population mean (\\(\\mu\\)), sulfur effect (\\(S\\)), phosphorus effect (\\(P\\)), the interaction of sulfur and phosphorus (\\(SP\\)), and the error (\\(\\epsilon\\)). As in previous models, error is the random source of variation due to environmental and instrumental inconsistency. \\(S_i\\) and \\(P_j\\) are described as main effects. \\(SP_{ij}\\) is the interaction. In the table below, the effects from this linear model are broken out: knitr::kable(clover_final) block sulfur phosphorus mu S P SP Error Yield 1 0 0 5.72 -2.06 -2.06 0.37 -0.1 1.87 1 0 80 5.72 -2.06 -2.06 -0.38 0.2 1.42 1 0 160 5.72 -2.06 -2.06 0.01 0.1 1.71 1 11.25 0 5.72 0.24 0.24 0.29 0.2 6.69 1 11.25 80 5.72 0.24 0.24 0.16 -0.4 5.96 1 11.25 160 5.72 0.24 0.24 -0.44 0.0 5.76 1 22.5 0 5.72 1.81 1.81 -0.66 0.6 9.28 1 22.5 80 5.72 1.81 1.81 0.23 0.2 9.77 1 22.5 160 5.72 1.81 1.81 0.43 -0.3 9.47 2 0 0 5.72 -2.06 -2.06 0.37 0.4 2.37 2 0 80 5.72 -2.06 -2.06 -0.38 0.0 1.22 2 0 160 5.72 -2.06 -2.06 0.01 0.2 1.81 2 11.25 0 5.72 0.24 0.24 0.29 1.0 7.49 2 11.25 80 5.72 0.24 0.24 0.16 0.1 6.46 2 11.25 160 5.72 0.24 0.24 -0.44 -0.4 5.36 2 22.5 0 5.72 1.81 1.81 -0.66 -0.2 8.48 2 22.5 80 5.72 1.81 1.81 0.23 -0.1 9.47 2 22.5 160 5.72 1.81 1.81 0.43 0.2 9.97 3 0 0 5.72 -2.06 -2.06 0.37 0.4 2.37 3 0 80 5.72 -2.06 -2.06 -0.38 -0.1 1.12 3 0 160 5.72 -2.06 -2.06 0.01 0.7 2.31 3 11.25 0 5.72 0.24 0.24 0.29 0.2 6.69 3 11.25 80 5.72 0.24 0.24 0.16 -0.6 5.76 3 11.25 160 5.72 0.24 0.24 -0.44 0.9 6.66 3 22.5 0 5.72 1.81 1.81 -0.66 0.4 9.08 3 22.5 80 5.72 1.81 1.81 0.23 -0.3 9.27 3 22.5 160 5.72 1.81 1.81 0.43 0.0 9.77 4 0 0 5.72 -2.06 -2.06 0.37 0.4 2.37 4 0 80 5.72 -2.06 -2.06 -0.38 -0.6 0.62 4 0 160 5.72 -2.06 -2.06 0.01 0.4 2.01 4 11.25 0 5.72 0.24 0.24 0.29 -0.2 6.29 4 11.25 80 5.72 0.24 0.24 0.16 0.2 6.56 4 11.25 160 5.72 0.24 0.24 -0.44 -0.1 5.66 4 22.5 0 5.72 1.81 1.81 -0.66 -0.3 8.38 4 22.5 80 5.72 1.81 1.81 0.23 0.2 9.77 4 22.5 160 5.72 1.81 1.81 0.43 -0.3 9.47 7.2.1.2 Analysis of Variance In R, we use the same approach as previous weeks. All terms from the linear model, except the population mean and error, are included in the model statement. library(broom) # define the model model = aov(Yield ~ sulfur + phosphorus + sulfur:phosphorus, data = clover_final) #run the anova anova_tab = tidy(model) knitr::kable(anova_tab) term df sumsq meansq statistic p.value sulfur 2 349.0468222 174.5234111 1227.117734 0.0000000 phosphorus 2 0.6720889 0.3360444 2.362812 0.1133374 sulfur:phosphorus 4 5.7705111 1.4426278 10.143477 0.0000381 Residuals 27 3.8400000 0.1422222 NA NA In the table, we see that the main effect of sulfur is significant at the \\(P \\le 0.05\\) level. The phosphorus effect is not significant. The interaction (sulfur:phosphorus) effect is also significant at the \\(P\\le0.05\\) level. When an interaction is significant, we should examine the effect of one factor independently at each level of the other. We can group analyses of one factor by levels of another factor using the group_by command in R. In this case we will tell R to run the analysis of variance for the sulfur effect separately for each level of P. library(broom) slice_1 = clover_final %&gt;% group_by(phosphorus) %&gt;% do(tidy(aov(.$Yield ~ .$sulfur))) %&gt;% as.data.frame() %&gt;% mutate(term = gsub(&quot;[.][$]&quot;, &quot;&quot;, term)) knitr::kable(slice_1) phosphorus term df sumsq meansq statistic p.value 0 sulfur 2 90.33447 45.1672333 264.8242 0 0 Residuals 9 1.53500 0.1705556 NA NA 80 sulfur 2 145.58927 72.7946333 671.9505 0 80 Residuals 9 0.97500 0.1083333 NA NA 160 sulfur 2 118.89360 59.4468000 402.2716 0 160 Residuals 9 1.33000 0.1477778 NA NA We see that the effect of sulfur is significant at each level of phosphorus. We can group analyses of phosphorusby each level of sulfur. library(broom) slice_2 = clover_final %&gt;% group_by(sulfur) %&gt;% do(tidy(aov(.$Yield ~ .$phosphorus))) %&gt;% as.data.frame() %&gt;% mutate(term = gsub(&quot;[.][$]&quot;, &quot;&quot;, term)) knitr::kable(slice_2) sulfur term df sumsq meansq statistic p.value 0 phosphorus 2 2.869267 1.4346333 17.331141 0.0008196 0 Residuals 9 0.745000 0.0827778 NA NA 11.25 phosphorus 2 1.782067 0.8910333 3.734249 0.0659399 11.25 Residuals 9 2.147500 0.2386111 NA NA 22.5 phosphorus 2 1.791267 0.8956333 8.507335 0.0084258 22.5 Residuals 9 0.947500 0.1052778 NA NA Whoa, what is going on here! We can now see the phosphorus effect is significant at S=0 and S=22.5, and almost significant at S=11.25. If we look at the line plot above, we see that phosphorus increases yield when S=0 and S=11.25, but decreases yield when S=22.5. The positive and negative effects cancelled each out when we looked at the overall analysis of variance  the interaction masked the sulfur effect so that its significance was not reflected in the results. This demonstrates why it is so important to investigate interactions before making inferences about treatments. If we concluded from the first analysis of variance that sulfur affected yield, we would have been accurate. But if we had not analyzed the phosphorus effect separately by sulfur level, we would have erroneously concluded it did not affect yield. 7.2.2 Case Study 2 For the second case study, we are going to look at an experiment where turnips were planted at different densities in different row spacings. There were 5 densities and 4 row widths. This trial also blocked treatments, so it combines the randomized complete block and factorial designs. 7.2.2.1 Linear Additive Model The linear additive model for this trial is: \\[Y_{ijk} = B_i + D_j + S_k + DS_{jk} + BDS_{ijk}\\] In this model, \\(Y_{ijk}\\) is yield, \\(B_i\\) is block, \\(D_j\\) is density, \\(S_k\\) is row spacing, \\(DS_{jk}\\) is the interaction of planting density and row spacing, and \\(BDS_{ijk}\\) is the interaction of block, density, and row width, which is used as the residual or error term in this model. We can see the additive effects of the factor levels and their interactions in the table below. library(agridat) turnip = mead.turnip turnip_data = turnip %&gt;% filter(!spacing %in% c(32)) %&gt;% filter(density %in% c(8,20,32)) %&gt;% mutate(spacing = as.factor(spacing), density = as.factor(density)) turnip_effects = turnip_data %&gt;% mutate(mu = mean(yield)) %&gt;% group_by(block) %&gt;% mutate(B = mean(yield) - mu) %&gt;% ungroup() %&gt;% group_by(spacing) %&gt;% mutate(S = mean(yield) - mu) %&gt;% ungroup() %&gt;% group_by(density) %&gt;% mutate(D = mean(yield) - mu) %&gt;% ungroup() %&gt;% group_by(spacing, density) %&gt;% mutate(SD = mean(yield) - (mu + B + S + D)) %&gt;% ungroup() %&gt;% mutate(Error = yield - (mu + B + S + D + SD)) %&gt;% mutate(mu = round(mu,2), B = round(B,2), S = round(S,2), D = round(D,2), SD = round(SD,2), Error = round(Error, 2)) turnip_final = turnip_effects %&gt;% mutate(yield = if_else(spacing==16 &amp; density==32, yield-0.4, yield)) %&gt;% mutate(yield = if_else(spacing==8 &amp; density==32, yield+0.5, yield)) %&gt;% mutate(yield = if_else(spacing==4 &amp; density==32, yield+0.3, yield))%&gt;% mutate(yield = if_else(spacing==16 &amp; density==8, yield+3.2, yield)) #turnip_data knitr::kable(turnip_final) yield block spacing density mu B S D SD Error 2.40 B1 4 8 2.67 -0.26 0.02 -0.17 0.29 -0.16 2.56 B1 4 20 2.67 -0.26 0.02 0.11 0.18 -0.16 2.78 B1 4 32 2.67 -0.26 0.02 0.06 0.30 -0.31 2.33 B1 8 8 2.67 -0.26 0.08 -0.17 0.32 -0.32 2.56 B1 8 20 2.67 -0.26 0.08 0.11 0.21 -0.26 3.03 B1 8 32 2.67 -0.26 0.08 0.06 0.25 -0.27 5.21 B1 16 8 2.67 -0.26 -0.10 -0.17 0.17 -0.30 2.56 B1 16 20 2.67 -0.26 -0.10 0.11 0.38 -0.25 1.90 B1 16 32 2.67 -0.26 -0.10 0.06 0.23 -0.30 2.60 B2 4 8 2.67 0.07 0.02 -0.17 -0.03 0.04 2.89 B2 4 20 2.67 0.07 0.02 0.11 -0.15 0.17 3.06 B2 4 32 2.67 0.07 0.02 0.06 -0.02 -0.03 2.63 B2 8 8 2.67 0.07 0.08 -0.17 -0.01 -0.02 2.69 B2 8 20 2.67 0.07 0.08 0.11 -0.11 -0.13 3.48 B2 8 32 2.67 0.07 0.08 0.06 -0.08 0.18 5.57 B2 16 8 2.67 0.07 -0.10 -0.17 -0.16 0.06 3.04 B2 16 20 2.67 0.07 -0.10 0.11 0.06 0.23 2.31 B2 16 32 2.67 0.07 -0.10 0.06 -0.10 0.11 2.67 B3 4 8 2.67 0.19 0.02 -0.17 -0.16 0.11 2.71 B3 4 20 2.67 0.19 0.02 0.11 -0.27 -0.01 3.44 B3 4 32 2.67 0.19 0.02 0.06 -0.15 0.35 2.98 B3 8 8 2.67 0.19 0.08 -0.17 -0.13 0.33 3.21 B3 8 20 2.67 0.19 0.08 0.11 -0.24 0.39 3.39 B3 8 32 2.67 0.19 0.08 0.06 -0.21 0.09 5.76 B3 16 8 2.67 0.19 -0.10 -0.17 -0.29 0.25 2.83 B3 16 20 2.67 0.19 -0.10 0.11 -0.07 0.02 2.40 B3 16 32 2.67 0.19 -0.10 0.06 -0.22 0.20 As we did before, we visually inspect the data for insights into how they may interact. turnip_final %&gt;% group_by(density, spacing) %&gt;% summarise(yield = mean(yield)) %&gt;% ungroup() %&gt;% ggplot(aes(x=spacing, y=yield, group=density)) + geom_line(aes(color=density), size=2) ## `summarise()` has grouped output by &#39;density&#39;. You can override using the `.groups` argument. The plot gives critical insight into these data. Increasing spacing from 4 to 8 to 16 seems to cause a slight increase in yield where density=20. But where density=8, yield seems to increase rapidly between row spacing 8 and row spacing 16. Where density = 32, yield increases slighty with row spacing from 4 to 8, and then decreases markedly from 8 to 16. The mean yield, averaged across row spacings, changes little across planting densities  even though the yields change dramatically within each of the individual row spacings. If we did not visually examine the data above, and instead relied on the ANOVA to alert us to an affect, we could miss this very important insight. 7.2.2.2 Analysis of Variance Our analysis of variance is similar to that we ran for the first case study, except for it now includes the block term. model = aov(yield ~ block + density + spacing + density:spacing, data = turnip_final) anova_tab = anova(model) knitr::kable(anova_tab) Df Sum Sq Mean Sq F value Pr(&gt;F) block 2 0.9770963 0.4885481 25.06684 1.17e-05 density 2 3.3854519 1.6927259 86.85182 0.00e+00 spacing 2 2.6353852 1.3176926 67.60929 0.00e+00 density:spacing 4 16.3880593 4.0970148 210.21312 0.00e+00 Residuals 16 0.3118370 0.0194898 NA NA The planting density and plant spacing main effects were significant, as was their interacton. Was the spacing effect significant at each level of density? We can slice the data to find this out. library(broom) slice_1 = turnip_final %&gt;% group_by(density) %&gt;% do(tidy(aov(.$yield ~ .$spacing))) %&gt;% as.data.frame() %&gt;% mutate(term = gsub(&quot;[.][$]&quot;, &quot;&quot;, term)) knitr::kable(slice_1) density term df sumsq meansq statistic p.value 8 spacing 2 16.9677556 8.4838778 125.0694513 0.0000129 8 Residuals 6 0.4070000 0.0678333 NA NA 20 spacing 2 0.0182000 0.0091000 0.1341523 0.8770078 20 Residuals 6 0.4070000 0.0678333 NA NA 32 spacing 2 2.0374889 1.0187444 12.8701572 0.0067549 32 Residuals 6 0.4749333 0.0791556 NA NA We can see above that the effect of spacing on yield is only significant at \\(P\\le0.05\\) density=8 and density=32. If we examining the effect of density separately for each level of spacing: library(broom) slice_2 = turnip_final %&gt;% group_by(spacing) %&gt;% do(tidy(aov(.$yield ~ .$density))) %&gt;% as.data.frame() %&gt;% mutate(term = gsub(&quot;[.][$]&quot;, &quot;&quot;, term)) knitr::kable(slice_2) spacing term df sumsq meansq statistic p.value 4 density 2 0.4540667 0.2270333 4.347447 0.0680698 4 Residuals 6 0.3133333 0.0522222 NA NA 8 density 2 0.6872889 0.3436444 3.670979 0.0909484 8 Residuals 6 0.5616667 0.0936111 NA NA 16 density 2 18.6321556 9.3160778 135.037365 0.0000103 16 Residuals 6 0.4139333 0.0689889 NA NA We similarly see that density is only significant at \\(P\\le0.05\\) where spacing=16. In this trial, both main (density and spacing) effects are significant. But if dont explore and explain our analysis further, we might miss how the both the magnitude of and the rank rank of row spacing effects on yield changes with plant density. 7.2.3 Discussing Interactions In factorial experiments, there are three kinds of interaction that may occur. We can have no interaction, a spreading interaction, or a crossover interaction. 7.2.3.1 No Interaction Where there is no interaction, the treatments effects are simply additive  the observed value of each observation in the plot above is the sum of the effect of the level of Factor A plus the effect of the level of Factor B. The difference between levels of Factor A are consistent across levels of Factor B, and vice versa. If we plot the data using a line plot, it will look like this: factA = rep(c(1,2,3), 2) Y = c(1:6) factB = rep(c(&quot;T1&quot;, &quot;T2&quot;), each=3) dataset = cbind(factA, Y, factB) %&gt;% as.data.frame() %&gt;% mutate(factA=as.numeric(factA), Y=as.numeric(Y)) dataset %&gt;% ggplot(aes(x=factA, y=Y, group=factB)) + geom_line(aes(color=factB), size=2) + labs(x=&quot;Factor A&quot;, legend=&quot;Factor B&quot;, color=&quot;Factor B&quot;) In a bar plot, it should look like this: dataset %&gt;% ggplot(aes(x=factA, y=Y, group=factB)) + geom_bar(stat=&quot;identity&quot;, aes(fill=factB), position = &quot;dodge&quot;) + labs(x=&quot;Factor A&quot;, legend=&quot;Factor B&quot;, fill=&quot;Factor B&quot;) Finally, there will be no change in the ranking of levels within factors. Rank is the order of levels according to their obseved effects, from least to greatest. For factor A, the ranking of levels within Factor A is \\(1 &gt; 2 &gt; 3\\), while within Factor B, level T2 always ranks higher than level T1. 7.2.3.2 Spreading Interaction In a spreading interaction, the ranking of levels within factors does not change, but the difference between them does. factA = rep(c(1,2,3), 2) Y = c(1,2,3,4,6,8) factB = rep(c(&quot;T1&quot;, &quot;T2&quot;), each=3) dataset = cbind(factA, Y, factB) %&gt;% as.data.frame() %&gt;% mutate(factA=as.numeric(factA), Y=as.numeric(Y)) dataset %&gt;% ggplot(aes(x=factA, y=Y, group=factB)) + geom_line(aes(color=factB), size=2) + labs(x=&quot;Factor A&quot;, legend=&quot;Factor B&quot;, color=&quot;Factor B&quot;) In the above plot, we can see the levels of Factor A still rank \\(1&lt;2&lt;3\\) in their effect on the observed value Y, for both level T1 and level T2 of Factor B. We also note that the levels of Factor B rank \\(T1&lt;T2\\) at each level of Factor A. In this spreading interaction, however, the difference between T1 and T2 of factor B increases as the levels of Factor A increase. Similarly, the differences among levels of Factor A are greater for level T2 than level T1 of Factor B. We saw a spreading interaction before in Case Study 1. The effect of sulfur increased with the level of phosphorus, and vice versa. 7.2.3.3 Crossover Interaction A crossover interaction is similar to a spreading interacton in that the differences between levels within one factor change with the levels of a second factor, but different in that the ranking of levels changes as well. In addition, as we saw above, crossover reactions can mask the effects of factor levels. factA = rep(c(1,2,3), 2) Y = c(1,2,3,4,3,2) factB = rep(c(&quot;T1&quot;, &quot;T2&quot;), each=3) dataset = cbind(factA, Y, factB) %&gt;% as.data.frame() %&gt;% mutate(factA=as.numeric(factA), Y=as.numeric(Y)) dataset %&gt;% ggplot(aes(x=factA, y=Y, group=factB)) + geom_line(aes(color=factB), size=2) + labs(x=&quot;Factor A&quot;, legend=&quot;Factor B&quot;, color=&quot;Factor B&quot;) In the plot above, the ranking of levels within Factor B is \\(T2&gt;T1\\) for levels 1 and 2 of Factor A, but \\(T1&gt;T2\\) for level 3 of Factor A. In other words, whether T2 is greater than T1 depends on the level of Factor A. In addition, the levels of Factor B behave differently in response to the levels of Factor A. Level T1 of Factor B increases with the level of Factor B, while level T2 decreases. We observed a crossover reaction in Case Study 2 above, where the effect of the widest row spacing on yield was greater than the narrow row spacings at the lowest planting density, but was less than the narrow spacings at the greatest planting density. 7.2.3.4 Discussing Interactions Interactions are exciting. Spreading interactions show us how the proper combination of of management practices or inputs can have a combined effect that is greater than the individual effect of inputs. Conversely, crossover interactions show us how the advantage of one practice can be lost with the mismanagement of a second practice. This is the essence of agronomy. When we identify spreading interactions, we learn how to build more productive cropping systems, as opposed to one-off solutions. Dont get me wrong  there are some wonderful one-off solutions. But almost every input or practice can be made more effective by understanding how it interacts with other practices. In addition, trials can be designed to test how the effect of that input interacts with different environmental conditions, such as temperature and precipitation. Interactions should always be highlighted when discussing the findings of an Analysis of Variance. The essense of an interaction is that the effect of one factor is dependent on the level of a second factor. If you dont discuss the interaction between factors, you dont completely describe the effect of either factor. As we saw above, crossover interactions can mask the effects of factors. In the second case study, had we simply concluded from the main effect that yield did not differ with plant density, we would have failed to report what the data actually show: yield differs profoundly with plant density  but that row spacing affects the direction of that difference. 7.3 Split-Plot Design The Split-Plot Design is a special kind of factorial experiment, in which one factor is nested within another. Lets jump to an illustration. library(agricolae) main_factor = c(&quot;A&quot;,&quot;B&quot;) sub_factor = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;) trt_rand = design.split(main_factor, sub_factor, r=2, design = &quot;rcbd&quot;, seed=2) plot_map = trt_rand$book plot_map %&gt;% ggplot(aes(x=plots, y=splots), group=main_factor) + geom_tile(aes(fill=main_factor), color=&quot;black&quot;) + geom_text(aes(label=sub_factor)) In the experiment show above, there are two factors: the main factor and the sub-factor. The main factor has two levels, while the sub-factor has 5 levels. The design is a randomized complete block design. As with any randomized complete block design, all treatments must occur once within each block. Main factor levels are indicated by color. Sub-factor levels are indicated by letter. In the above plot map, the main factors occur once in Block 1 (plots 101 and 102) and Block 2 (plots 103 and 104). Within each occurrence of the main factor, all five levels of the sub-factor are nested. The levels of the main factor are randomized within each block, and the levels of the subfactor are randomized within each main plot. So now that we know what a split-plot design looks like, we will address the question: why would we want to do this? The first answer has to do with practicality. Say the first factor is composed of fungicide treatments that we are going to apply with a Hagge applicator with a 100 ft boom. The second factor is composed of treatments (hybrid, in-furrow fertilizer, etc) that can be apply in 20-foot widths. We can apply our treatments much more easily if we use a split-plot design and nest the second factor (the sub-factor) within the first factor (the main factor). The second answer is that a thoughtfully-designed split-plot experiment will be more sensitive to differences among levels of the sub-factor than a randomized complete block trial. This is because, no matter how hard we try, plots that are closer together are more likely to be similar than plots that are further apart. By putting the sub-factor treatments close together, we can better estimate and test treatment effects. The greater sensitivity to subfactor treatments, however, comes at a cost: we sacrifice some of our ability to detect differences among levels of our main factor. Sometimes, however, the levels of the main plot factor are so markedly different that we know we will detect differences among them, even if they are placed further apart. Plus, we may be more interested in the interaction between the main factor and sub-factor, and our ability to estimate and test this interaction, too, is enhanced by the split plot design. ###Case Study: Habenero Peppers If you are a chili pepper fan like me, you enjoy habanero chilies, also called Scotch bonnets in strict moderation. They are hotter than the jalapeno, but not devastating like the Carolina Reaper. In this study, habanero peppers were grown in pots of soils characterized by their color. They were harvested at two stages of ripening, green and orange, and their polyphenol concentrations measured. Soil was the main factor and harvest time the subfactor. habanero = read.csv(&quot;data-unit-7/habenero.csv&quot;) head(habanero) ## block soil harvest_stage total_polyphenols ## 1 R1 red green 109.84958 ## 2 R1 red orange 186.71777 ## 3 R1 brown green 130.53991 ## 4 R1 brown orange 207.00041 ## 5 R1 black green 97.86705 ## 6 R1 black orange 212.19618 7.4 Linear Additive Model \\[ Y_{ijk} = \\mu + M_{j} + Error(B_i + BM_{ij}) + S_{k} + MS_{ik} + Error(BS_{ik}+BMS_{ijk})\\] Bear with me. We can break this model down in to two sections. Lets start with the first three terms. These focus on explaining the observed variance among the main plots. \\(B_i\\) is the effect of block \\(i\\) \\(M_j\\) is the effect of level \\(j\\) of the main factor \\(Error(BM_{ij})\\) is the error (unexplained variance) associated with the block \\(i\\) and level \\(j\\) of the main factor. When we calculate the F-value for the main factor, we use this error term. The second three terms focus on explaining the observed variance among subplots. \\(S_k\\) is the effect of level \\(k\\) of the sub-factor \\(MS_{ik}\\) is the interaction between level \\(i\\) of the main factor and level \\(k\\) of the subfactor \\(Error(BS_{ik}+BMS_{ijk})\\) is the unexplained variance associated with the given levels of the sub factor and the main factor - sub-factor interaction. It is used in testing the significance of both those effects Ugh. What a dumpster fire. Lets look at the ANOVA table for our habenero trial to make more sense of this. habanero_model = aov(total_polyphenols ~ soil + harvest_stage + soil:harvest_stage + Error(block:soil), data=habanero) ## Warning in aov(total_polyphenols ~ soil + harvest_stage + soil:harvest_stage + : ## Error() model is singular summary(habanero_model) ## ## Error: block:soil ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## soil 2 1322.7 661.3 8.691 0.00791 ** ## Residuals 9 684.9 76.1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## harvest_stage 1 42120 42120 176.72 3.2e-07 *** ## soil:harvest_stage 2 5938 2969 12.46 0.00256 ** ## Residuals 9 2145 238 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We have two ANOVA tables. The top table evaluates the effect of soil in our main plots. Looking at the degrees of freedom, we can see we have 3 soils - 1 = 2 degrees of freedom for soil and (4 blocks - 1) x 3 = 9 degrees of freedom for the error. The F-value is the ratio of the mean square for soil and the mean square for the main factor error (residuals). The second table evalulates the effect of of the harvest stage and the interaction between soil and harvest stage in the subplots. There are 2 stages - 1 = 1 degree of freedom for harvest stage and *(3 soils - 1) x (2 stages - 1) = 2 degrees of freedom for the interaction. The F-values for the harvest stage and interaction effects are the ratios of their mean squares to the mean square for the subplot error (residuals). We analyze the ANOVA results the same as we would for any factorial trial. Looking at our table, we see significant results for soil, harvest stage, and their interaction. So lets go straight to the interaction plot. habanero %&gt;% group_by(soil, harvest_stage) %&gt;% summarise(total_polyphenols = mean(total_polyphenols)) %&gt;% ungroup() %&gt;% ggplot(aes(x=harvest_stage, y=total_polyphenols, group = soil)) + geom_line(aes(color=soil)) ## `summarise()` has grouped output by &#39;soil&#39;. You can override using the `.groups` argument. We can see the the total polyphenol concentration was affected by harvest stage in each soil. Orange habaneros had greater total polyphenos than green ones. Furthermore, we can see that black soils produced the fewest total polyphenols when habaneros where harvested green, but the most when they were harvested orange. Looking at our interaction tables, we see soil has a significant effect on total polyphenols at both harvest stages. library(broom) slice_1 = habanero %&gt;% group_by(harvest_stage) %&gt;% do(tidy(aov(.$total_polyphenols ~ .$block + .$soil))) %&gt;% as.data.frame() %&gt;% mutate(term = gsub(&quot;[.][$]&quot;, &quot;&quot;, term)) knitr::kable(slice_1) harvest_stage term df sumsq meansq statistic p.value green block 3 312.1119 104.03731 0.4324894 0.7375426 green soil 2 5006.9642 2503.48208 10.4071271 0.0112036 green Residuals 6 1443.3275 240.55458 NA NA orange block 3 79.2278 26.40927 0.1592069 0.9199513 orange soil 2 2253.3551 1126.67757 6.7921174 0.0287564 orange Residuals 6 995.2810 165.88017 NA NA And that harvest stage had a significant effect at each level of soil. library(broom) slice_2 = habanero %&gt;% group_by(soil) %&gt;% do(tidy(aov(.$total_polyphenols ~ .$block + .$harvest_stage))) %&gt;% as.data.frame() %&gt;% mutate(term = gsub(&quot;[.][$]&quot;, &quot;&quot;, term)) knitr::kable(slice_2) soil term df sumsq meansq statistic p.value black block 3 79.03859 26.34620 0.0555253 0.9798439 black harvest_stage 1 32872.72483 32872.72483 69.2801410 0.0036344 black Residuals 3 1423.46960 474.48987 NA NA brown block 3 214.87947 71.62649 0.5937286 0.6605111 brown harvest_stage 1 7062.17490 7062.17490 58.5400106 0.0046367 brown Residuals 3 361.91529 120.63843 NA NA red block 3 390.95481 130.31827 1.0869202 0.4734925 red harvest_stage 1 8122.82365 8122.82365 67.7484520 0.0037542 red Residuals 3 359.69045 119.89682 NA NA 7.5 Conclusion Experimental design of multiple treatment experiments plays a critical role in the quality of our data and the inferences that can be made. In most cases, you will probably run randomized complete block design experiments with a single factor, where blocking will reduce the error among plots within each block. The factorial design allows greater efficiency if you are interested in studying two factors. It is also the only way to study the interaction between two factors. Finally, where one factor is known to be more subtle (and therefore, harder to test signficance for) than a second factor, or where differences in equipment size make it more practical to nest one factor inside the other, the split-plot design can be very useful. 7.6 Exercise: Randomized Complete Block Design While the lecture is long this week, fortunately running the analyses in R is very fast. We start the exercies with the Randomized Complete Block Design. To run a Randomized Complete Block Design, we will again use same analysis of variance approach we did last week. We only need two lines of code: the first to define the model we wish to test, and the second to tell R to summarise the results. 7.6.1 Case Study Barley varieties were compared using a Randomized Complete Block Design. Yield is in grams per plot. Five varieties were grown in three replicates. library(tidyverse) barley = read.csv(&quot;data-unit-7/exercise_data/barley_besag.csv&quot;) head(barley) ## block gen yield ## 1 1 G37 10.92 ## 2 1 G55 12.07 ## 3 1 G58 9.51 ## 4 1 G63 9.23 ## 5 1 G09 8.03 ## 6 2 G37 10.24 7.6.2 ANOVA For this analysis of variance, we need two lines of code. The first is the model statement: barley_model = aov(yield ~ block + gen, data=barley) In the above code: barley_model defines the object to which the ANOVA results will be saved aov tells R to run an analysis of variance yield ~ block + gen tells R our linear additive model. When using R, we leave out mu and the error term  R knows to calculate these. data=barley tells R to fit our ANOVA model to the barley dataset Reviewing our results is simple: summary(barley_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## block 1 2.611 2.611 7.476 0.023066 * ## gen 4 19.758 4.940 14.142 0.000642 *** ## Residuals 9 3.144 0.349 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In our result above, we see the effect of barley variety (gen) is highly significant. 7.6.3 Plotting the Results Although we get into the presentation of treatment means in more detail late in the course, lets take a quick look at our treatment means. We will use ggplot. barley_plot = ggplot(barley, aes(x=gen, y=yield)) + geom_bar(stat = &quot;summary&quot;, fun=&quot;mean&quot;) In the plot above: * barley is the object to which we are saving the plot * ggplot() starts the ggplot function * ggplot(barley, aes(x=gen, y=yield)) tells R that our plot will be based on the barley dataset. Anything in ggplot that occurs in aes() brackets is an aesthetic. An aesthetic is any value that is based on a variable from our plot and changes in value from treatment to treatment. x=gen tells R the horizontal position of whatever we draw (points, parts, etc.) is determined by the gen variable. y=yield tells R the vertical position (height) of our object is based on the yield variable. * geom_bar tells R we want to draw bars with our data. * With geom bar, we can also calculate summary statistics on the fly. stat=summary tells R we are going to calculate a summary statistic. fun=mean tells R to use the mean as that statistic. Now we can display the plot at any time by running barley_plot: barley_plot This is so drab! While I dont want you to get bogged down in formatting plots, lets tweak a few details just so you can see the power of ggplot to make publication-quality plots. Lets change the bar color by adding the fill command to our code: barley_plot = ggplot(barley, aes(x=gen, y=yield)) + geom_bar(stat = &quot;summary&quot;, fun=&quot;mean&quot;, fill=&quot;darkolivegreen&quot;) barley_plot Lets draw a black line around the outside of the bars by adding color to our geom_bar statement. barley_plot = ggplot(barley, aes(x=gen, y=yield)) + geom_bar(stat = &quot;summary&quot;, fun=&quot;mean&quot;, fill=&quot;darkolivegreen&quot;, color=&quot;black&quot;) barley_plot Lets make the outline a little thicker using the size command: barley_plot = ggplot(barley, aes(x=gen, y=yield)) + geom_bar(stat = &quot;summary&quot;, fun=&quot;mean&quot;, fill=&quot;darkolivegreen&quot;, color=&quot;black&quot;, size=1.5) barley_plot The axis titles are little hard to read. We can change text formatting, legend formatting, and other objects using the theme function. In the theme function below, lets change the axis.title size using the element_text command and setting the size to 18. barley_plot = ggplot(barley, aes(x=gen, y=yield)) + geom_bar(stat = &quot;summary&quot;, fun=&quot;mean&quot;, fill=&quot;darkolivegreen&quot;, color=&quot;black&quot;) + theme(axis.title = element_text(size=18)) barley_plot Lets similarly increase the size of the axis.text (tick labels) to 14pt. barley_plot = ggplot(barley, aes(x=gen, y=yield)) + geom_bar(stat = &quot;summary&quot;, fun=&quot;mean&quot;, fill=&quot;darkolivegreen&quot;, color=&quot;black&quot;) + theme(axis.title = element_text(size=18), axis.text = element_text(size=14)) barley_plot Finally, lets rename our axis titles using the lab() command. barley_plot = ggplot(barley, aes(x=gen, y=yield)) + geom_bar(stat = &quot;summary&quot;, fun=&quot;mean&quot;, fill=&quot;darkolivegreen&quot;, color=&quot;black&quot;) + theme(axis.title = element_text(size=18), axis.text = element_text(size=14)) + labs(x=&quot;variety&quot;, y=&quot;Yield (tons ha-1)&quot;) barley_plot Like I said, I dont want you to get too bogged down in formatting your plots. And, yes, a simple plot like this could be made just as quickly in Excel. But building the plot in R Studio has multiple advantages. First, you dont have to cut and paste your data to Excel. Second, you dont have to copy your completed chart back to R or your report document. You can include it right in your R Notebook with your data. That way, if you are working on your creative component, you can have one document, instead of multiple documents all over the place. Third, if you are working on the multiple plots, you can coordinate them by reusing your theme code from plot to plot, rather than having to click and change each field in each plot. Finally, and most importantly, you have way more options for creating your plot in R than just about any software I know! 7.6.4 Practice Navy bean varieties were evaluated in a randomized complete block trial . beans = read.csv(&quot;data-unit-7/exercise_data/besag_beans.csv&quot;) head(beans) ## block gen yield ## 1 R1 Maris 350 ## 2 R1 Dwarf 230 ## 3 R1 Minica 355 ## 4 R1 Stella 370 ## 5 R1 Topless 280 ## 6 R1 Metissa 185 Run the analysis in R. Your output should match the following: Df Sum Sq Mean Sq F value Pr(&gt;F) block 23 294256 12794 2.324 0.00182 ** gen 5 719695 143939 26.147 &lt; 2e-16 *** Residuals 115 633063 5505 Signif. codes: 0  0.001  0.01  0.05 . 0.1   1 What do you conclude from the analysis of variance? 7.7 Exercise: Factorial ANOVA For the second exercise this week, we will work with factorial experimental designs. Remember, a factorial design has two or more factors; within each factor are two or more levels, which we commonly refer to as treatments. For a factorial design, our analysis becomes a little more complex, but not terribly so. Factorial experiments may have Completely Randomized Designs or Randomized Complete Block Designs. The latter is more common. 7.7.1 Case Study: Biochar For our case study, we will use a dataset inspired from a recent paper: Ahmad, M.; Wang, X.; Hilger, T.H.; Luqman, M.; Nazli, F.; Hussain, A.; Zahir, Z.A.; Latif, M.; Saeed, Q.; Malik, H.A.; Mustafa, A. Evaluating Biochar-Microbe Synergies for Improved Growth, Yield of Maize, and Post-Harvest Soil Characteristics in a Semi-Arid Climate. Agronomy 2020, 10, 1055. This trial, in Pakistan, looked at the factorial effects of biochar soil amendments and and seed innoculum on maize growth. I am not an expert on biochar  it is basically an amendment made from burned crop residues  a charcoal of sorts. More information can be found in the paper above, which is open-source. library(tidyverse) biochar = read.csv(&quot;data-unit-7/exercise_data/biochar_innoculum.csv&quot;) head(biochar) ## Block Innoculation Amendment Yield ## 1 B1 Inoculated Control 7.426576 ## 2 B1 Inoculated Egyptian.acacia.biochar..0.1.. 6.848225 ## 3 B1 Inoculated Egyptian.acacia.biochar..0.2.. 8.130492 ## 4 B1 Inoculated FYM.biochar..0.1.. 6.349240 ## 5 B1 Inoculated FYM.biochar..0.2.. 8.463009 ## 6 B1 Inoculated Wheat.straw.biochar..0.1.. 7.647091 I tend to skip over the treatments, so lets discuss them in greater detail. There are 2 levels of the Innoculation factor: innoculated, or not Innoculated. There are 7 levels of the factor biochar: one control plus 6 biochar treatments of different sources and rates. There are 6 blocks. Yield is reported in tons per hectare. 7.7.2 ANOVA We use the same two lines of code we have used for other analyses of variance. First, our model statement. Before we enter that, lets review the linear additive model for this factorial experiment: Yield = mu + Block + Innoculation + Amendment + Innoculation*Amendment + Error This includes the effect of Block (which is assumed, by nature of the design, to not interact with the Innoculation or Amendment factors), Innoculation, Amendment, and their interaction, Innoculation*Amendment. Having identified the linear additive model, we can write out model statement in R. biochar_model = aov(Yield ~ Block + Innoculation + Amendment + Innoculation:Amendment, data = biochar) Just the same as for other trials, the statement above defines an object, biochar model, and assigns it the output of an analysis of variance (aov), based on our linear model and the dataset biochar. We can look at the ANOVA table by using the summary() function. summary(biochar_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Block 5 26.98 5.396 8.782 2.09e-06 *** ## Innoculation 1 4.24 4.244 6.907 0.010703 * ## Amendment 6 19.10 3.183 5.181 0.000208 *** ## Innoculation:Amendment 6 4.44 0.740 1.205 0.315335 ## Residuals 65 39.94 0.614 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Looking first at our main effects, we see the effects of both factors, Innoculation aren Amendment, are significant at the P=0.05 level. The interaction is not significant. 7.7.3 Interaction Plots As we learned in the lecture, it is important when analyzing factorial design experiments to plot the interactions between the data. Although line plots are normally used in the final presentation of qualitative factors like products and practices, they are useful to visualize the nature of interactions. The first step in creating our plot is to create a dataset of means. To do this, we need to group our data using the group_by() function, calculate the means for each combination of Innoculation and Amendment, and ungroup() the data. We can link these functions with %&gt;% grouped_data = biochar %&gt;% # this feeds the biochar dataset to the next line group_by(Innoculation, Amendment) %&gt;% # group by the interactions of Innoculation and Amendment summarise(Yield = mean(Yield)) %&gt;% # calculate the means for the interactions ungroup() # ungroup the data for further processing ## `summarise()` has grouped output by &#39;Innoculation&#39;. You can override using the `.groups` argument. grouped_data ## # A tibble: 14 x 3 ## Innoculation Amendment Yield ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Inoculated Control 8.38 ## 2 Inoculated Egyptian.acacia.biochar..0.1.. 7.84 ## 3 Inoculated Egyptian.acacia.biochar..0.2.. 8.05 ## 4 Inoculated FYM.biochar..0.1.. 8.33 ## 5 Inoculated FYM.biochar..0.2.. 9.19 ## 6 Inoculated Wheat.straw.biochar..0.1.. 8.75 ## 7 Inoculated Wheat.straw.biochar..0.2.. 9.57 ## 8 Un-Inoculated Control 7.45 ## 9 Un-Inoculated Egyptian.acacia.biochar..0.1.. 7.54 ## 10 Un-Inoculated Egyptian.acacia.biochar..0.2.. 8.13 ## 11 Un-Inoculated FYM.biochar..0.1.. 8.16 ## 12 Un-Inoculated FYM.biochar..0.2.. 8.20 ## 13 Un-Inoculated Wheat.straw.biochar..0.1.. 8.86 ## 14 Un-Inoculated Wheat.straw.biochar..0.2.. 8.62 We can now feed our grouped data into ggplot to create our line plot. The aes() argument tells R that the position of the points connected by our lines will be determined by their level of the Innoculation factor and their Yield. It further tells R that lines should be drawn between points with the same level of the Amendment factor. grouped_data %&gt;% ggplot(aes(x=Innoculation, y=Yield, group=Amendment)) We need to add one more line telling R what shapes (geometries) to draw. We add the geom_line argument. In it we include an aes() argument to associate line color to with level of Innoclulation. grouped_data %&gt;% ggplot(aes(x=Innoculation, y=Yield, group=Amendment)) + geom_line(aes(color=Amendment)) Our plot above reflects our ANOVA results. Maize yield with most amendments decreased when the crop was un-innoculated. The maize response was flat or even slightly increased for a few levels of Amendment, but the interaction only had a P-value of about 0.32, so the interaction was not significant. Similarly, the ranking of Amendments changed somewhat with level of Innoculation, but not enough to be significant. 7.7.4 Testing Factors Individually Sometimes we may want to know whether one factor has a significant effect at each level of the other factor. To do this requires three lines of code. The last is a little tricky, but we will get there. The first couple of lines should look familiar. We take the biochar data.frame, and we use group_by to group it by Innoculation level. The third line requires further explation. What we ae doing is telling R to run a separate analysis of variance for Amendment at each level of Innnoculation. *aov(.\\(Yield ~ .\\)Block + .\\(Amendment))* is our anova model, but it looks different than the ones we have specified before. Normally we would just write: *aov(Yield ~ Block + Amendment, data = biochar)*. We can&#39;t do that here, though, because the model is using the data groups fed by the group_by() function just above. Instead, the &quot;.\\)\" tells R that each term in the model is a column from the data that is being fed to it. tidy tells R to return the anova as a data.frame() This allows us to stack the ANOVAs for different levels of Innoculant on top of each other. tidy is part of the broom package, so we use library(broom) to load that package before we begin. Finally, do() is the magical command that tells R to, well, do something  anything  with the groups of data that are fed to it. So when we put it all together, the third line tells R to do something with the two levels of Innoculant, specifically to create tidy data.frames of the analyses of variance for both of them. library(broom) biochar %&gt;% group_by(Innoculation) %&gt;% do(tidy(aov(.$Yield ~ .$Block + .$Amendment))) ## # A tibble: 6 x 7 ## # Groups: Innoculation [2] ## Innoculation term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Inoculated .$Block 5 22.1 4.42 11.6 0.00000272 ## 2 Inoculated .$Amendment 6 14.0 2.33 6.10 0.000291 ## 3 Inoculated Residuals 30 11.5 0.382 NA NA ## 4 Un-Inoculated .$Block 5 13.3 2.67 4.00 0.00669 ## 5 Un-Inoculated .$Amendment 6 9.58 1.60 2.39 0.0521 ## 6 Un-Inoculated Residuals 30 20.0 0.667 NA NA We can see from the output that the effect of Amendment is significant at both levels of Innoculant. What about the other way around? We simply take the code above and reverse the positions of Amendment and Innoculation. biochar %&gt;% group_by(Amendment) %&gt;% do(tidy(aov(.$Yield ~ .$Block + .$Innoculation))) ## # A tibble: 21 x 7 ## # Groups: Amendment [7] ## Amendment term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Control .$Block 5 2.26 0.452 1.27 0.401 ## 2 Control .$Innocula~ 1 2.61 2.61 7.32 0.0425 ## 3 Control Residuals 5 1.78 0.357 NA NA ## 4 Egyptian.acacia.biochar..~ .$Block 5 4.01 0.801 1.68 0.292 ## 5 Egyptian.acacia.biochar..~ .$Innocula~ 1 0.266 0.266 0.556 0.489 ## 6 Egyptian.acacia.biochar..~ Residuals 5 2.39 0.478 NA NA ## 7 Egyptian.acacia.biochar..~ .$Block 5 4.67 0.934 0.779 0.604 ## 8 Egyptian.acacia.biochar..~ .$Innocula~ 1 0.0225 0.0225 0.0188 0.896 ## 9 Egyptian.acacia.biochar..~ Residuals 5 5.99 1.20 NA NA ## 10 FYM.biochar..0.1.. .$Block 5 12.3 2.47 3.30 0.108 ## # ... with 11 more rows We see the effect of Innoculant on yield is only significant for the Control and Wheat Straw Biochar 2X rate. It is insignificant with all other levels of Amendment. 7.7.5 Bar Plots Similar to the other exercise, we can also create a bar plot of the treatment means. We have two factors, however, so we will need to tweak our code. The first thing to do is to pick one of the factors to be plotted along the X axis. Lets go with the Amendment factor. We can create a simple bar graph for our Amendment treatments ggplot(data=biochar, aes(x=Amendment, y=Yield)) + geom_bar(stat=&quot;summary&quot;, fun=&quot;mean&quot;) The treatment names are running together a little, so lets add a theme argument with angle = to pitch those at a 45 degree angle to the axis and \"hjust=1) to right-justify them from the axis. ggplot(data=biochar, aes(x=Amendment, y=Yield)) + geom_bar(stat=&quot;summary&quot;, fun=&quot;mean&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) We can plot the innoculum effect the same way, just substituting Innoculation for Amendment in our code above. ggplot(data=biochar, aes(x=Innoculation, y=Yield)) + geom_bar(stat=&quot;summary&quot;, fun=&quot;mean&quot;) Of course, in a factorial experiment, we may want to see both treatments together. In the plot below, we will plot Amendment along the X axis, Yield on the Y axis, and group the bars by level of Innoculation. Our plot starts out similar to the Amendment plot above. But in the first line we need to add the argument group=Innoculation. This tells R we will group our bars by the level of innoculation. To our second line, we need to add an aesthetics argument (aes). Why? Because we want the color of the bar to change with the valuable of a variable, in this case the level of innoculation. Our complete argmument reads aes(fill = Innoculation) ggplot(data=biochar, aes(x=Amendment, y=Yield, group=Innoculation)) + # add group statement to this line geom_bar(stat=&quot;summary&quot;, fun=&quot;mean&quot;, aes(fill=Innoculation)) + # add fill statement to this line theme(axis.text.x = element_text(angle = 45, hjust = 1)) Ok, weve got different colored bars  but they are stacked! We want them side-by-side. So we need to add one more argument to our second line: position=dodge. This tells R to position the bars so they dodge each other. Its a weird choice of words, but technically it works. ggplot(data=biochar, aes(x=Amendment, y=Yield, group=Innoculation)) + # add group statement to this line geom_bar(stat=&quot;summary&quot;, fun=&quot;mean&quot;, aes(fill=Innoculation), position=&quot;dodge&quot;) + # add fill statement to this line theme(axis.text.x = element_text(angle = 45, hjust = 1)) And Voila, we have our plot. Whew, those color bars are high-contrast, which is great for accessibility. If we want to change the colors to something closer to our preferred palate, however, ggplot(data=biochar, aes(x=Amendment, y=Yield, group=Innoculation)) + # add group statement to this line geom_bar(stat=&quot;summary&quot;, fun=&quot;mean&quot;, aes(fill=Innoculation), position=&quot;dodge&quot;) + # add fill statement to this line theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_fill_manual(values = c(&quot;darkgreen&quot;, &quot;orange&quot;)) 7.7.6 Practice For practice we have a dataset inspired by Plant population and row spacing effects on corn: Plant growth, phenology, and grain yield (2020), published by Brad J. Bernhard Frederick E. Below in Agronomy Journal. The article is open source. In this study, corn was grown with two row spacings and four plant populations. The trial was a complete random block design. corn = read.csv(&quot;data-unit-7/exercise_data/corn_pop_spacing.csv&quot;) head(corn) ## block pop spacing yield ## 1 1 94000 76 16.90595 ## 2 1 94000 51 16.72022 ## 3 1 109000 76 15.69209 ## 4 1 109000 51 18.51509 ## 5 1 124000 76 16.26873 ## 6 1 124000 51 17.26152 We need to do a couple of things to our dataset before we can run an analysis of variance. We need to reformat the data for block, pop, and spacing. Although I did not intend to make this part of the exercise, Im glad it is in here. ANOVA is based on named effects. But if we look under the words block, pop, and spacing, we see the expression . The problem is that we are using numbers to represent our treatments. If we run the analysis of variance on these data as-is, our table will look like this: Df Sum Sq Mean Sq F value Pr(&gt;F) block 1 17.808 17.808 24.850 3.17e-05 * pop 1 2.028 2.028 2.831 0.10402 spacing 1 5.550 5.550 7.745 0.00971 pop:spacing 1 1.318 1.318 1.839 0.18633 Residuals 27 19.349 0.717 If you see nothing wrong, be comforted this has happened to me many times. But look again, particularly at the degrees of freedom. Do you see it now? We have four blocks in this trial, so we should have three degrees of freedom. We have four populations, so again we should have three degrees of freedom. Our degrees of freedom for population should be equal to the population degrees of freedom (4-1 = 3) times the spacing degrees of freedom (2-1 = 1), or three degrees. What is going on is R thingks we want to create a regression model, and it will just have to be patient because we dont get to that until Unit 9! To tell R to run this analysis correctly, lets tell it to treat block, pop, and spacing as the factors they are, rather than as integers. We are going to use the as.factor command to tell R these variables are factors. We use the ominous sounding command mutate to change our variables. corn_fixed = corn %&gt;% # the &quot;%&gt;%&quot; tells R to take the corn dataset and use it in the next line of code mutate(block = as.factor(block), # mutate tells R to change the variables according to the formula given pop = as.factor(pop), spacing = as.factor(spacing)) 7.7.6.1 ANOVA Now, run your ANOVA on the new dataset, corn_fixed. The degrees of freedom will be correct. Your ANOVA output should look like: Df Sum Sq Mean Sq F value Pr(&gt;F) block 3 18.791 6.264 12.841 5.53e-05 pop 3 6.860 2.287 4.688 0.01169 spacing 1 5.550 5.550 11.379 0.00287 pop:spacing 3 4.609 1.536 3.150 0.04645 * Residuals 21 10.243 0.488 Signif. codes: 0  0.001  0.01  0.05 . 0.1   1 7.7.6.2 Test Factors Individually Test the effect of spacing at each level of pop. Your results should look like: pop term df sumsq meansq statistic p.value 1 94000 .\\(block 3 2.20 0.733 5.93 0.0889 2 94000 .\\)spacing 1 0.336 0.336 2.72 0.198 3 94000 Residuals 3 0.371 0.124 NA NA 4 109000 .\\(block 3 6.00 2.00 1.74 0.330 5 109000 .\\)spacing 1 4.65 4.65 4.05 0.138 6 109000 Residuals 3 3.45 1.15 NA NA 7 124000 .\\(block 3 5.88 1.96 3.01 0.195 8 124000 .\\)spacing 1 3.72 3.72 5.71 0.0968 9 124000 Residuals 3 1.95 0.651 NA NA 10 139000 .\\(block 3 8.70 2.90 18.0 0.0201 11 139000 .\\)spacing 1 1.46 1.46 9.07 0.0571 12 139000 Residuals 3 0.482 0.161 NA NA Test the effects of pop at each level of spacing. Your results should look like: spacing term df sumsq meansq statistic p.value 1 51 .\\(block 3 8.24 2.75 5.25 0.0228 2 51 .\\)pop 3 8.06 2.69 5.13 0.0243 3 51 Residuals 9 4.71 0.523 NA NA 4 76 .\\(block 3 11.3 3.76 7.03 0.00984 5 76 .\\)pop 3 3.41 1.14 2.13 0.167 6 76 Residuals 9 4.81 0.534 NA NA 7.8 Exercise: Split-Plot Design The split-plot design is similar to the factorial design but, as we learned in the lecture, has two error terms: one for the main factor, and the second for the sub-factor. This requires we use different R code than for the factorial design. 7.8.1 Case Study: Corn-Soybean Systems Trial A cropping systems trial was conducted in Wisconsin to investigate the combined effects of tillage and in-furrow fungicide on soybean yield. This dataset was inspired by the following article, which is open-source: Potratz, DJ, Mourtzinis, S, Gaska, J, Lauer, J, Arriaga, FJ, Conley, SP. Striptill, other management strategies, and their interactive effects on corn grain and soybean seed yield. Agronomy Journal. 2020; 112: 72 80. https://doi.org/10.1002/agj2.20067 The trial was a split-plot design with two levels of tillage (no-till, NT, and strip-till, ST) as the main factor and two levels of fungicide (Fungicide and No Fungicide) as the sub-factor. There were four replications. Main factor levels were blocked. library(tidyverse) soybean = read.csv(&quot;data-unit-7/exercise_data/tillage_fungicide_soybean.csv&quot;) head(soybean) ## block tillage fungicide yield ## 1 B1 NT Fungicide 3.698314 ## 2 B1 NT No Fungicide 3.452027 ## 3 B1 ST Fungicide 4.059679 ## 4 B1 ST No Fungicide 3.867225 ## 5 B2 NT Fungicide 4.045291 ## 6 B2 NT No Fungicide 3.902678 7.8.2 ANOVA Lets first construct our linear model: Y = mu + T + Error(B + BT) + F + TF + Error(BF + BTF) So our anova output should include include the effects of tillage (T), fungicide (F), and their interaction (TF). We also need to tell R to use the interaction of block and tillage to test the main factor tillage effect. We code this in R as follows. soybean_model = aov(yield ~ tillage + fungicide + tillage:fungicide + Error(block:tillage), data=soybean) ## Warning in aov(yield ~ tillage + fungicide + tillage:fungicide + ## Error(block:tillage), : Error() model is singular soybean_model = with(soybean, sp.plot(block, tillage, fungicide, yield)) ## ## ANALYSIS SPLIT PLOT: yield ## Class level information ## ## tillage : NT ST ## fungicide : Fungicide No Fungicide ## block : B1 B2 B3 B4 ## ## Number of observations: 16 ## ## Analysis of Variance Table ## ## Response: yield ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## block 3 2.17303 0.72434 90.9367 0.001920 ** ## tillage 1 0.38181 0.38181 47.9344 0.006177 ** ## Ea 3 0.02390 0.00797 ## fungicide 1 0.26756 0.26756 23.4352 0.002878 ** ## tillage:fungicide 1 0.00017 0.00017 0.0147 0.907315 ## Eb 6 0.06850 0.01142 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## cv(a) = 2.1 %, cv(b) = 2.5 %, Mean = 4.289329 Of particular note is the new Error term we have added. This tells R which error to use for testing the tillage effect. The ANOVA output is created using the summary() function. summary(soybean_model) ## Length Class Mode ## ANOVA 5 anova list ## gl.a 1 -none- numeric ## gl.b 1 -none- numeric ## Ea 1 -none- numeric ## Eb 1 -none- numeric As we saw in lecture, the ANOVA output for a split-plot experiment includes two tables. The top table tests the main factor effect. At the top of that table, it specifies we are using the block:tillage interaction as our error term. The bottom table tests the sub-factor effect and the interaction between the main factor and the sub-factor. We can see tillage did not have an effect. Fungicide did affect yield. 7.8.3 Interaction Plot Although the interaction is not singificant, we can still draw a line plot to visualize the relationship between tillage and fungicide effects. First, we need to create a summary dataset with mean yields for the interactions of tillage and fugicide. soybean_group = soybean %&gt;% # build new dataframe, soybean_group, on summary of soybean dataframe group_by(tillage, fungicide) %&gt;% # summarise based on interaction of tillage and fungicide summarise(yield = mean(yield)) %&gt;% # summarise by means across replications ungroup() # ungroup for further processing ## `summarise()` has grouped output by &#39;tillage&#39;. You can override using the `.groups` argument. Then we create our line plot. We can start the plot with the ggplot() function. The aes() argument tells R to position the data points according to their tillage level and yield. It also tells R to group the points by fungicide level. soybean_group %&gt;% ggplot(aes(x=tillage, y=yield, group=fungicide)) Next we add line geometries to our plot using geom_line(). We use another aes() argument to tell R to differentiate line color by level of fungicide. soybean_group %&gt;% ggplot(aes(x=tillage, y=yield, group=fungicide)) + geom_line(aes(color=fungicide)) 7.8.4 Bar plot We can create a bar plot just as we did for the factorial trial. First, we tell ggplot with the aes() argument to position the bars according to their tillage level and their yield, and to group means with the same fungicide level together so they are the same color. soybean %&gt;% ggplot(aes(x=tillage, y=yield, group=fungicide)) Then we add our geometry with the geom_bar() argument. We will add an aes() argument to tell R that bar color should be matched to fungicide level. We also need to use the stat=summary argument that the statistic to be plotted is a summary statistic to be calculated. The we need to tell it with fun=mean that the statistic to be calculated is the mean. soybean %&gt;% ggplot(aes(x=tillage, y=yield, group=fungicide)) + geom_bar(aes(fill=fungicide), stat = &quot;summary&quot;, fun=&quot;mean&quot;) Whoops! We need to add a final argument to geom_bar(), to tell it to plot the two levels of fungicide next to each other, instead of stacking them. We add position=dodge accordingly. soybean %&gt;% ggplot(aes(x=tillage, y=yield, group=fungicide)) + geom_bar(aes(fill=fungicide), stat = &quot;summary&quot;, fun=&quot;mean&quot;, position=&quot;dodge&quot;) Finally, if we want, we can add a couple of theme() arguments to increase the font size of the axis titles and labels. soybean %&gt;% ggplot(aes(x=tillage, y=yield, group=fungicide)) + geom_bar(aes(fill=fungicide), stat = &quot;summary&quot;, fun=&quot;mean&quot;, position=&quot;dodge&quot;) + theme(axis.title = element_text(size=18), axis.text = element_text(size=14)) # element_text() tells R we are modifying the font, as opposed to the position of the axis title 7.8.5 Practice The same study above produced a corn dataset. The main factor was rotation and the sub-factor was fungicide. corn = read.csv(&quot;data-unit-7/exercise_data/rotation_fungicide_corn.csv&quot;) head(corn) ## block rotation fungicide yield ## 1 B1 CS Fungicide 12.95823 ## 2 B1 CS No Fungicide 12.40752 ## 3 B1 CC Fungicide 11.49778 ## 4 B1 CC No Fungicide 11.60836 ## 5 B2 CS Fungicide 14.19937 ## 6 B2 CS No Fungicide 13.69898 7.8.5.1 ANOVA Create an analysis of variance for the data above. Your results should look like: Error: block:rotation Df Sum Sq Mean Sq F value Pr(&gt;F) rotation 1 4.99 4.987 0.896 0.38 Residuals 6 33.39 5.565 Error: Within Df Sum Sq Mean Sq F value Pr(&gt;F) fungicide 1 0.1309 0.1309 64.19 0.000202 rotation:fungicide 1 0.3492 0.3492 171.19 1.23e-05 Residuals 6 0.0122 0.0020 Signif. codes: 0  0.001  0.01  0.05 . 0.1   1 7.8.5.2 Test Factors Individually First, test the effect of rotation at each level of fungicide. Your results should look like: fungicide term df sumsq meansq statistic p.value 1 Fungicide .\\(block 3 16.6 5.52 6385. 0.00000333 2 Fungicide .\\)rotation 1 3.99 3.99 4612. 0.00000703 3 Fungicide Residuals 3 0.00259 0.000865 NA NA 4 No Fungicide .\\(block 3 16.8 5.61 6068. 0.00000359 5 No Fungicide .\\)rotation 1 1.35 1.35 1458. 0.0000395 6 No Fungicide Residuals 3 0.00277 0.000925 NA NA Second, test the effect of fungicide at each level of rotation. Your results should look like: rotation term df sumsq meansq statistic p.value 1 CC .\\(block 3 16.7 5.58 4075. 0.00000652 2 CC .\\)fungicide 1 0.0262 0.0262 19.2 0.0220 3 CC Residuals 3 0.00410 0.00137 NA NA 4 CS .\\(block 3 16.7 5.56 2049. 0.0000183 5 CS .\\)fungicide 1 0.454 0.454 167. 0.000997 6 CS Residuals 3 0.00814 0.00271 NA NA 7.8.5.3 Create a line plot to view the interaction between rotation and fungicide. 7.9 Exercise: Experimental Design Now that you have been introoduced to four powerful experimental designs  completely randomized, randomized complete block, factorial, and split-plot, how can you use R to create your plot layout and randomize your treatments? Of course, R has a package for that. We will use the agricolae() packages and its multiple tools for experimental design. 7.9.1 Completely Randomized Design We have a hemp trial with five varieties: China Cat, Eyes, Scarlet, Fire, and Tower. How do we randomize it? We use the design.crd() function. library(agricolae) # First, we need to define the varieties as a vector: varieties = c(&quot;China Cat&quot;, &quot;Eyes&quot;, &quot;Scarlet&quot;, &quot;Fire&quot;, &quot;Tower&quot;) # Then we define the number of replications we want: reps = 6 # One more thing we can do is provide a &quot;seed&quot;. A seed tells R where to start in randomizing the plot. This is often useful in case we need to recreate a set of random numbers. In case we need to recreate this plot map from scratch, we will provide a seed. seed_no = 910 # then we just feed these to the design.crd argument crd = design.crd(trt=varieties, r=reps, seed = seed_no) design.crd outputs a list (a collection of R objects). To view the treatment assignments in our new plot plan, crd, we add $book to the end of it. crd$book ## plots r varieties ## 1 101 1 Fire ## 2 102 1 Eyes ## 3 103 2 Eyes ## 4 104 1 Scarlet ## 5 105 2 Scarlet ## 6 106 2 Fire ## 7 107 3 Eyes ## 8 108 3 Fire ## 9 109 4 Eyes ## 10 110 1 Tower ## 11 111 4 Fire ## 12 112 2 Tower ## 13 113 5 Fire ## 14 114 1 China Cat ## 15 115 5 Eyes ## 16 116 2 China Cat ## 17 117 3 China Cat ## 18 118 3 Scarlet ## 19 119 3 Tower ## 20 120 4 Scarlet ## 21 121 4 Tower ## 22 122 6 Eyes ## 23 123 6 Fire ## 24 124 5 Tower ## 25 125 5 Scarlet ## 26 126 4 China Cat ## 27 127 6 Tower ## 28 128 5 China Cat ## 29 129 6 China Cat ## 30 130 6 Scarlet 7.9.2 Randomized Complete Block Design What if we want to block our treatments? For this, we use the design.rcbd() function. The values we feed to it will be identical to the completely randomized design above. # First, we need to define the varieties as a vector: varieties = c(&quot;China Cat&quot;, &quot;Eyes&quot;, &quot;Scarlet&quot;, &quot;Fire&quot;, &quot;Tower&quot;) # Then we define the number of replications we want: reps = 6 # One more thing we can do is provide a &quot;seed&quot;. A seed tells R where to start in randomizing the plot. This is often useful in case we need to recreate a set of random numbers. In case we need to recreate this plot map from scratch, we will provide a seed. seed_no = 910 rcbd = design.rcbd(trt = varieties, r=reps, seed = seed_no) We can review our treatment assignments using rcbd$book. rcbd$book ## plots block varieties ## 1 101 1 China Cat ## 2 102 1 Fire ## 3 103 1 Scarlet ## 4 104 1 Eyes ## 5 105 1 Tower ## 6 201 2 Fire ## 7 202 2 Eyes ## 8 203 2 Tower ## 9 204 2 Scarlet ## 10 205 2 China Cat ## 11 301 3 Fire ## 12 302 3 China Cat ## 13 303 3 Scarlet ## 14 304 3 Tower ## 15 305 3 Eyes ## 16 401 4 Fire ## 17 402 4 Scarlet ## 18 403 4 China Cat ## 19 404 4 Eyes ## 20 405 4 Tower ## 21 501 5 China Cat ## 22 502 5 Scarlet ## 23 503 5 Tower ## 24 504 5 Eyes ## 25 505 5 Fire ## 26 601 6 Tower ## 27 602 6 Scarlet ## 28 603 6 Fire ## 29 604 6 Eyes ## 30 605 6 China Cat We can save this data.frame to our local directory  for use in our spreadsheet  using the write.csv() function. # the write.csv arguments are (name of data.frame, filename, row.names=FALSE) write.csv(rcbd$book, &quot;field_book.csv&quot;, row.names = FALSE) 7.9.3 Factorial Design We use the design.ab() function to create our plot layout. The factorial experimental design is slightly different. We cannot give R named treatments. We can only tell it how many treatments are in each factor. Lets say we wanted to run a factorial experiment with three levels of hemp variety and two levels of soil amdendments. We would call this a 3 x 2 factorial. One additonal change is we can tell R whether to arrange our treatments in a completely randomized design or a randomized complete block design. We will use the design = rcbd argment to tell R to create a randomized complete block design. (If we wanted to create a completely randomized design, we would use design=crd instead # the line below tells R that we have three levels of the first factor and two levels of the second factor. factors = c(3,2) reps = 4 seed_no=910 fact = design.ab(trt = factors, r = reps, seed = seed_no, design = &quot;rcbd&quot;) We can again review our treatment assignments by using $book. fact$book ## plots block A B ## 1 101 1 1 1 ## 2 102 1 2 1 ## 3 103 1 1 2 ## 4 104 1 2 2 ## 5 105 1 3 1 ## 6 106 1 3 2 ## 7 107 2 2 2 ## 8 108 2 1 1 ## 9 109 2 2 1 ## 10 110 2 3 1 ## 11 111 2 3 2 ## 12 112 2 1 2 ## 13 113 3 2 2 ## 14 114 3 1 1 ## 15 115 3 1 2 ## 16 116 3 3 1 ## 17 117 3 2 1 ## 18 118 3 3 2 ## 19 119 4 2 2 ## 20 120 4 3 1 ## 21 121 4 2 1 ## 22 122 4 3 2 ## 23 123 4 1 2 ## 24 124 4 1 1 7.9.4 Split-Plot Design Lets say we are going to use equipment to spread our compost. The applicator creates a 6-meter swath, while our three varieties are being grown in plots 2 meters wide. We deside to conduct a split plot trial with soil amendment as our main factor and variety as our sub-factor. For the split plot design, will define the levels of our two factors, varieties and amendments, by name # First, we need to define the varieties as a vector: varieties = c(&quot;Eyes&quot;, &quot;Scarlet&quot;, &quot;Fire&quot;) # Second, we define our amendment treatments amendments = c(&quot;control&quot;, &quot;compost&quot;) # # Then we define the number of replications we want: reps = 4 # One more thing we can do is provide a &quot;seed&quot;. A seed tells R where to start in randomizing the plot. This is often useful in case we need to recreate a set of random numbers. In case we need to recreate this plot map from scratch, we will provide a seed. seed_no = 910 # The design.split() function is similar to the factorial design.ab function above. The main difference is we now define a &quot;trt1&quot; and a &quot;trt2&quot;. trt1 is the factor you want in the main plots; trt2 is the factor you want in the subplots. The other arguments are the same. # spdesign = design.split(trt1 = amendments, trt2 = varieties, r=4, seed = seed_no, design = &quot;rcbd&quot;) Our treatment assignments are below: spdesign$book ## plots splots block amendments varieties ## 1 101 1 1 control Scarlet ## 2 101 2 1 control Eyes ## 3 101 3 1 control Fire ## 4 102 1 1 compost Eyes ## 5 102 2 1 compost Fire ## 6 102 3 1 compost Scarlet ## 7 103 1 2 control Fire ## 8 103 2 2 control Eyes ## 9 103 3 2 control Scarlet ## 10 104 1 2 compost Fire ## 11 104 2 2 compost Eyes ## 12 104 3 2 compost Scarlet ## 13 105 1 3 compost Eyes ## 14 105 2 3 compost Fire ## 15 105 3 3 compost Scarlet ## 16 106 1 3 control Eyes ## 17 106 2 3 control Fire ## 18 106 3 3 control Scarlet ## 19 107 1 4 compost Scarlet ## 20 107 2 4 compost Fire ## 21 107 3 4 compost Eyes ## 22 108 1 4 control Fire ## 23 108 2 4 control Scarlet ## 24 108 3 4 control Eyes 7.9.5 Practice We now have a new hemp trial comparing varieties Chicago, New York, and Detroit. The critics say theyre all on the same street (whatever), but we would like to compare them. 7.9.5.1 Completely Randomized Design Create a completely randomized design with 4 replicates. Please use seed_no=123 so our results will match. varieties = c(&quot;Chicago&quot;, &quot;New York&quot;, &quot;Detroit&quot;) reps=4 seed_no=123 Your plan should look like: plots r varieties 1 101 1 New York 2 102 1 Detroit 3 103 1 Chicago 4 104 2 Chicago 5 105 2 Detroit 6 106 3 Chicago 7 107 3 Detroit 8 108 2 New York 9 109 4 Detroit 10 110 3 New York 11 111 4 Chicago 12 112 4 New York 7.9.5.2 Randomized Complete Block Design Create a randomized complete block design using the same three varieties as above. Please again use seed_no=123 so our results will match. varieties = c(&quot;Chicago&quot;, &quot;New York&quot;, &quot;Detroit&quot;) reps=4 seed_no=123 Your plan should look like: plots block varieties 1 101 1 Detroit 2 102 1 Chicago 3 103 1 New York 4 201 2 Detroit 5 202 2 Chicago 6 203 2 New York 7 301 3 New York 8 302 3 Chicago 9 303 3 Detroit 10 401 4 New York 11 402 4 Detroit 12 403 4 Chicago 7.9.5.3 Factorial Design Now lets take our 3 levels of hemp variety and treat each with two levels of foliar spray (control and kelp). We will arrange our treatments in blocked (design=rcbd) factorial design. We will use 2 reps (only to keep our practice data set small  more reps would be better!). Please again use seed_no=123 factors = c(3,2) reps=2 seed_no=123 Your results should look like: plots block A B 1 101 1 1 1 2 102 1 3 2 3 103 1 2 2 4 104 1 1 2 5 105 1 3 1 6 106 1 2 1 7 107 2 2 2 8 108 2 1 2 9 109 2 3 2 10 110 2 1 1 11 111 2 3 1 12 112 2 2 1 7.9.5.4 Split Plot Design Now lets suppose we had a 9-meter spray boom and variety plots that were 3-meters wide. Lets design a split plot trial with foliar spray as the main factor and hemp variety as the subfactor. We will block the main plots (design=rcbd). Please again use seed_no=123 and fill in the remaining code. varieties = c(&quot;Chicago&quot;, &quot;New York&quot;, &quot;Detroit&quot;) foliar = c(&quot;control&quot;, &quot;kelp&quot;) reps=2 seed_no=123 Your results should look like: plots splots block foliar varieties 1 101 1 1 control Chicago 2 101 2 1 control New York 3 101 3 1 control Detroit 4 102 1 1 kelp New York 5 102 2 1 kelp Chicago 6 102 3 1 kelp Detroit 7 103 1 2 control New York 8 103 2 2 control Detroit 9 103 3 2 control Chicago 10 104 1 2 kelp New York 11 104 2 2 kelp Chicago 12 104 3 2 kelp Detroit "],["means-separation-and-data-presentation.html", "Chapter 8 Means Separation and Data Presentation 8.1 Case Study 8.2 Least Significant Difference 8.3 LSD Output in R 8.4 Comparisonwise versus Experimentwise Error 8.5 Tukeys Honest Significant Difference 8.6 Linear Contrast 8.7 Means Presentation 8.8 Exercise: LSD and Tukeys HSD 8.9 Exercise: Linear Contrasts 8.10 Exercise: Means Tables", " Chapter 8 Means Separation and Data Presentation The previous two units focused on the design and analysis of effects in multiple treatment files. Our focus was to determine whether treat ent effects explained more of the variation among individuals in a population than error (or residual) effects, which are based on unexplained differences among individuals. In the first half of this unit, we will learn three common tools used for testing the differences between treatments. This is often the key purpose of a research trial. We know going in that some treatments will be different. But we dont how they will rank, and whether the difference between them will be great enough to infer one is better than another. In the second half, we will learn how to present treatment means in tables and plots. Proper data allows the reader to not only grasp results, but even incorporate some of your findings into their own work 8.1 Case Study Our sample dataset is inspired by Salas, M.C.; Montero, J.L.; Diaz, J.G.; Berti, F.; Quintero, M.F.; Guzmn, M.; Orsini, F. Defining Optimal Strength of the Nutrient Solution for Soilless Cultivation of Saffron in the Mediterranean. Agronomy 2020, 10, 1311. Saffron is a spice made from the anthers of the saffron flow. It has a nuanced, sweet, complex flavor, and is used in dishes from Cornish saffron rolls to Spanish paella to Iranian tahdig. It comes from the anthers of the field-grown saffron flower and must be hand picked, making it very expensive. In this study, saffron was grown hydroponically in 15-L pots filled with perlite, with four nutrient concentrations as defined by electroconductivity (EC): low (EC 2.0), medium (EC 2.5), high (EC 3.0), and very high (EC 4.0). The effect of the solutions on corm production (needed to propagate the crop) was measured. library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- ## v ggplot2 3.3.3 v purrr 0.3.4 ## v tibble 3.1.0 v dplyr 1.0.5 ## v tidyr 1.1.3 v stringr 1.4.0 ## v readr 1.4.0 v forcats 0.5.1 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() saffron = read.csv(&quot;data-unit-8/saffron.csv&quot;) head(saffron) ## plot block rate corm_number ## 1 1 1 1X 246 ## 2 2 1 4X 240 ## 3 3 1 3X 254 ## 4 4 1 2X 285 ## 5 5 2 4X 233 ## 6 6 2 3X 251 8.2 Least Significant Difference Perhaps the most straightforward method of means separation is the infamous Least Significant Difference test. Not to be confused with the psychadelic parties in Marin County, California, the LSD test is as simple as this: calculate the least significant difference any treatment differences that are equal to or greater than the least significant difference are  you guessed it  significant The least significant difference is calculated as follows: \\[LSD_{df, \\alpha} = t_{df, \\alpha} \\cdot SED\\] Where \\(LSD_{df, \\alpha}\\) is the least significant difference, given the degrees of freedom associated with the error effect and the desired signifance. Does this formula look vaguely familiar? Does it remind you of what you were doing back in Unit 4? Great, because this is the same formula we used to calculate the distance between confidence limits and the sample mean back when we learned t-tests. Back then, we saw how the confidence interval was used to test the probability our observed difference between treatments was different from zero. Recall if zero fell outside our confidence interval, we inferred the two treatments were different. Similarly, if the difference between two treatments is greater than the least significant difference, we infer the treatments are significantly different. In R, we use a function, LSD.test(), which is part of the agricolae package, to calculate the LSD. First, however, lets run an analysis of variance on our t data. The experiment was a randomized complete block design, so our linear additive model is: \\[ Y_{ij} = \\mu +B_i + T_j + BT_{ij}\\] Where \\(Y_{ij}\\) is the number of corms, \\(\\mu\\) is the overall population mean for the trial, \\(B_i\\) is the block effect, \\(T_j\\) is the treatment effect, and \\(BT_{ij}\\) is the block effect. Our analysis of variance result is below. The effect of fertilization rate is highly significant. And this brings us to an important rule for using the LSD test. We only use the LSD test to separate means if the treatment effect is significant in our analysis of variance. Doing otherwise can lead to errors, as we will discuss below. saffron$block = as.factor(saffron$block) saffron_model = aov(corm_number ~ block + rate, saffron) summary(saffron_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## block 3 85 28.4 2.465 0.129 ## rate 3 4473 1490.9 129.333 1.02e-07 *** ## Residuals 9 104 11.5 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 8.3 LSD Output in R Now that we know the effect of fertilization rate is highly significant, we want to know how the individual treatments rank, and whether they are significantly different from one another. The results of our LSD test are below. library(agricolae) lsd = LSD.test(saffron_model, &quot;rate&quot;) lsd ## $statistics ## MSerror Df Mean CV t.value LSD ## 11.52778 9 257.125 1.32047 2.262157 5.43101 ## ## $parameters ## test p.ajusted name.t ntr alpha ## Fisher-LSD none rate 4 0.05 ## ## $means ## corm_number std r LCL UCL Min Max Q25 Q50 Q75 ## 1X 249.25 4.272002 4 245.4097 253.0903 246 255 246.00 248.0 251.25 ## 2X 284.75 2.629956 4 280.9097 288.5903 281 287 284.00 285.5 286.25 ## 3X 254.25 2.872281 4 250.4097 258.0903 251 258 253.25 254.0 255.00 ## 4X 240.25 5.439056 4 236.4097 244.0903 233 246 238.25 241.0 243.00 ## ## $comparison ## NULL ## ## $groups ## corm_number groups ## 2X 284.75 a ## 3X 254.25 b ## 1X 249.25 b ## 4X 240.25 c ## ## attr(,&quot;class&quot;) ## [1] &quot;group&quot; Lets unpack this piece by peace. The output from the LSD test is in a list of tables. 8.3.1 Statistics Table Lets start with the $statistics table. This explains how our LSD was calculated: MSerror is the error or residual mean square. It should match that value from the ANOVA table above. Recall that MSerror is an estimate the variance within treatments  that is, the variation among plots unexplained by our model. Therefore, its square root is the standard deviation of the observations within each treatment. DF is the degrees of freedom, which is used to calculate our t.value Mean is just that  the overall mean of the trial. CV is the coefficient of variance. By dividing the standard deviation by the mean, and multiplying by 100, we arrive at this value. Recall from Unit 6 that the CV is a measure of the quality control of the trial: how consistent were our experimental units? The t-value is based on the degrees of freedom and \\(\\alpha\\), the desired p-value (often 0.05) to be used to to test significance. LSD is the product of the t-value and the standard error of the difference, which can be derived from MSerror and the number of replicates. This is a lot to absorb, I realize. The most important two statistics for you to understand from this table are the CV and LSD. The other numbers are intermediate values, although if you list the LSD in a report you should also report the degrees of freedom used to calculate the t-value. 8.3.2 Means Table The $means table explains the distribution of observations within a treatment level around their sample mean. Most of these concepts we discussed in Unit 4. The statistics are: corm_number: These are our sample means for each level of treatment std: the standard error of the sample mean. This is unique to the sample mean for each treatment. r: the number of replicates per treatment level LCL and UCL: the lower confidence limit and upper confidence limit for each mean. These are calculated just as we did in Unit 4. The remainder of the statistics show the minimum and maximum values, and the quartiles. 8.3.3 Groups Table Often, the $groups table is the most interesting, for it tests the differences among levels of a treatment. The treatment means among levels are ranked from greatest to least. corm_number: again, the sample mean for each level of treatment group: this groups treatments that are statistically equivalent. Any means followed by the same letter are considered equal. In the table above, the mean corm numbers associated with the 1X and 3X rates of fertilizer are considered equal. Any means not followed by the same letter are consisdered statistically different at the p-level chosen to test those differences. The 2X rate produced significantly greater corm numbers than the other fertilizer rates. The 4X rate produced statistically lesser corm numbers than the other fertilizer rates. 8.4 Comparisonwise versus Experimentwise Error It is important LSD tests not be used indiscriminatly to separate means. The problem is that each time we test the difference between two means, we have a 5% chance of committing a type I error. This is the probability of committing a Type I error in that comparison. This is known as the comparisonwise error rate. The probability of committing a type I error across all the comparisons is known as the experimentwise error rate. If we only conduct one comparison, our experimentwise error rate is equal to the comparisonwise error rate. But if we conduct two comparisons, our experimentwise error rate is equal to the probability that comparisonwise errors will not occur in both comparison. We can demonstrate this with some simple calculations. Before we start our comparisons, there is a 100% chance we have not committed an experimentwise Type I error. We cold express this as: \\[ \\text{Experimentwise Error} = 1 \\] If we once comparison, there is 5% probability of a comparisonwise Type I error  and a 95% chance the error will not occur. We can express this as \\[ \\text{Experimentwise Error} = 1 - 0.95 = 0.05 \\] If we have two comparisons, there is a 95% probability a comparisonwise Type I error wont occur in the first comparison  and a 95% probability it wont occur in the second comparison. But the probability it doesnt occur in both comparisons is 0.95 * 0.95: \\[ \\text{Experimentwise Error} = 1 - 0.95 \\times 0.95 = 1 - 0.9025 = 0.0975 \\] Now the Experimentwise error rate is 0.0975, or 9.75%. What about three comparisons? \\[ \\text{Experimentwise Error} = 1 - 0.95 \\times 0.95 \\times 0.95 = 1 - 0.8573 = 0.1427 \\] The Experimentwise error rate is now 0.1427, or 14.27%. Finally, what if we had 10 comparisons? \\[ \\text{Experimentwise Error} = 1 - 0.95^{10} = 1 - 0.5987 = 0.4013\\] Our experimentwise error rate is now about 0.40, or 40%. As the number of our comparisons increases, so does the probability of an experimentwise error. How can we avoid this? The first method, mentioned above, is to not use the LSD test unless the ANOVA shows a significant treatment effect. We call this approach the F-protected LSD test. The second approach is to use a multiple range test that increases its minimum significant difference for comparing treatments as the number of treatments increases. 8.5 Tukeys Honest Significant Difference If we are going to be comparing many treatments, it is better to use a minimum significant difference, like Tukeys Honest Significant Different (HSD) Test. Tukeys HSD is calculated very similarly to the least significant difference: \\[ \\text{Tukey&#39;s HSD} = Q_{\\alpha, df, k} \\text{ } \\cdot SED \\] The difference in Tukeys HSD is that we use Q, the studentized range distribution (you can just call it Q in this course) in place of the t-distribution. Q differs from t in that its value is determined not only by \\(\\alpha\\), the desired probability of a Type I error, and \\(df\\), the degrees of freedom associated with the error mean square, but also \\(k\\), the number of treatments. The plot below includes four Q distribution curves, associated with 3, 5, 6, and 10 treatments. Observe how the distributions shift to the right as the number of treatments increases. This means that the minimum difference for significance also increases with the number of treatments. tukey_list = list() for(k in c(3:10)){ x_ptukey &lt;- seq(0, 8, by = 0.005) # Specify x-values for ptukey function y_ptukey &lt;- ptukey(x_ptukey, nmeans = k, df = 3*k) # Apply ptukey function tukey_df = data.frame(x_ptukey) %&gt;% cbind(y_ptukey) %&gt;% mutate(change = y_ptukey - lag(y_ptukey)) %&gt;% mutate(midpoint = (x_ptukey + lag(x_ptukey))/2) %&gt;% mutate(no_trt = k) %&gt;% as.data.frame() i=k-2 tukey_list[[i]] = tukey_df } tukey_df_all = bind_rows(tukey_list) tukey_df_all %&gt;% filter(no_trt %in% c(3,5,8,10)) %&gt;% mutate(no_trt = as.factor(no_trt)) %&gt;% ggplot(aes(x=midpoint, y=change, group=no_trt)) + geom_point(aes(color=no_trt)) ## Warning: Removed 4 rows containing missing values (geom_point). The Tukey test output below is very similar to the LSD test output. The $statistics section is identical to that of the LSD output, except it now reports the minimum significant difference instead of the least significant difference. In the $parameters section, the Studentized Range value (Q) is given in place of the t-value. The $groups section can be interpreted the same for the minimum significant difference as for the least significant difference. tukey = HSD.test(saffron_model, &quot;rate&quot;, group=TRUE) tukey ## $statistics ## MSerror Df Mean CV MSD ## 11.52778 9 257.125 1.32047 7.494846 ## ## $parameters ## test name.t ntr StudentizedRange alpha ## Tukey rate 4 4.41489 0.05 ## ## $means ## corm_number std r Min Max Q25 Q50 Q75 ## 1X 249.25 4.272002 4 246 255 246.00 248.0 251.25 ## 2X 284.75 2.629956 4 281 287 284.00 285.5 286.25 ## 3X 254.25 2.872281 4 251 258 253.25 254.0 255.00 ## 4X 240.25 5.439056 4 233 246 238.25 241.0 243.00 ## ## $comparison ## NULL ## ## $groups ## corm_number groups ## 2X 284.75 a ## 3X 254.25 b ## 1X 249.25 b ## 4X 240.25 c ## ## attr(,&quot;class&quot;) ## [1] &quot;group&quot; Unlike the LSD test, the Tukey test does not need to be protected by first examing whether the Analysis of Variance treatment effect is significant. That said, the Tukey test is unlikely to indicate a significant difference between treatments without the ANOVA treatment effect being significant as well. 8.6 Linear Contrast The last type of means comparison we will learn in this lesson is the linear contrast. Unlike the LSD and Tukey tests, linear contrasts may be used to separate two groups of treatments. While a lot of math may be introduced to the curious, in a linear contrast the statistician defines two groups of treatments through the use of coefficients; R then calculates their means and standard errors, and compares them using a t-test. There are multiple ways we can use a linear contrast, and our saffron dataset is a great way to introduce them. 8.6.1 Coefficients Recall how a t-test between two treatments works. We calculate \\(t\\) as: \\[ t = \\frac{(\\bar{x}_1-\\bar{x}_2) - (\\mu_1-\\mu_2)}{SED}\\] Where \\(\\bar{x}_1 -\\bar{x}_2\\) is the difference between sample means, \\((\\mu_1-\\mu_2)\\) is the hypothesized difference between treatments (usually zero), and SED is the standard error of the difference. For most situations, we could simplify the above equation to: \\[ t = \\frac{\\bar{x}_1-\\bar{x}_2}{SED}\\] What about if we are comparing more than two treatments? For our saffron example, what if we wanted to calculate the difference between the mean of the two lower rates (1X and 2X) and the mean of the two higher rates (3X and 4X)? Lets call this difference \\(L\\). We would calculate this difference as: \\[ L = \\frac{\\bar{x}_{1X} + \\bar{x}_{2X}}{2} - \\frac{\\bar{x}_{3X} + \\bar{x}_{4X}}{2} \\] All we are doing above is 1) calculating the means for the two groups and 2) subtracting the mean of the 3X and 4X rates from the mean of the 1X and 2X rates. Now lets express this same formula a little differently. What we are doing in the above equation is multiplying each number by \\(1/2\\): \\[ L = \\frac{1}{2}(\\bar{x}_{1X} + \\bar{x}_{2X}) - \\frac{1}{2}(\\bar{x}_{3X} + \\bar{x}_{4X})\\] In addition (bear with me!), when we subtract the mean of treatments 3X and 4X, it is the equivalent of adding the negative value of their mean to the mean of treatments 1X and 2X: \\[ L = \\frac{1}{2}(\\bar{x}_{1X} + \\bar{x}_{2X}) + (- \\frac{1}{2}(\\bar{x}_{3X} + \\bar{x}_{4X}))\\] Finally, we can arrange the equation above as: \\[ L = \\frac{1}{2}\\bar{x}_{1X} + \\frac{1}{2}\\bar{x}_{2X} - \\frac{1}{2}\\bar{x}_{3X} - \\frac{1}{2}\\bar{x}_{4X} + \\] The reason for this tortuous algebra flashback is to show you where the contrast coefficients come from. Each of the \\(\\frac{1}{2}\\)s in the equation above is a contrast coefficient. Lets demonstrate this with our saffron data. Our saffron treatment means are: saffron_means = saffron %&gt;% group_by(rate) %&gt;% summarise(corm_number = mean(corm_number)) %&gt;% ungroup() saffron_means ## # A tibble: 4 x 2 ## rate corm_number ## &lt;chr&gt; &lt;dbl&gt; ## 1 1X 249. ## 2 2X 285. ## 3 3X 254. ## 4 4X 240. Now lets add in a column with our coefficients: saffron_coefficients = saffron_means %&gt;% mutate(coefficient = c(1/2, 1/2, -1/2, -1/2)) saffron_coefficients ## # A tibble: 4 x 3 ## rate corm_number coefficient ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1X 249. 0.5 ## 2 2X 285. 0.5 ## 3 3X 254. -0.5 ## 4 4X 240. -0.5 We see that R has converted these to decimals. We then create a new column, mean_x_coefficient, that is the product of the original mean and the coefficient. We see these products are approximately half the value of the original sample mean (some may be less than half because of rounding errors). saffron_products = saffron_coefficients %&gt;% mutate(mean_x_coefficient = corm_number * coefficient) saffron_products ## # A tibble: 4 x 4 ## rate corm_number coefficient mean_x_coefficient ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1X 249. 0.5 125. ## 2 2X 285. 0.5 142. ## 3 3X 254. -0.5 -127. ## 4 4X 240. -0.5 -120. Finally, we can sum the mean_x_coefficient column to get the total difference among treatments. total_difference = sum(saffron_products$mean_x_coefficient) paste(&quot;total difference = &quot;, total_difference, sep=&quot;&quot;) ## [1] &quot;total difference = 19.75&quot; One critical rule about requirements is their sum must always equal zero. Otherwise you are not completely identifying two groups to compare. Using coefficients that do not sum to zero can also suggest you are weighting one group unfairly compared to another. ###Contrast Calculations To calculate \\(t\\), of course, we must divide \\(L\\) by the standard error of the difference: \\[ t = \\frac{L}{SED}\\] So how is the standard error of the difference calculated? Well, we know that the error mean square from our ANOVA is equal to the mean variance within treatments. And we know that if we take the square root of a variance, we get the standard deviation. And if we divide the standard deviation by the number of observations in each sample mean, we get the standard error. In a paired t-test, where we are simply calculating a standard error for one set of numbers, the differences between each pair, the standard error of the difference is calculated the same as the standard error: \\[ SED = SE = \\frac{s}{\\sqrt{n}} \\] Where \\(s\\) is the standard deviation and \\(n\\) is the number of observations (reps) per treatment. This formula is equal to that below, where we use the variance, \\(s^2\\), in place of the standard deviation. \\[ SED = \\sqrt{\\frac{s^2}{n}} \\] When we work with two-treatment trials where the treatments are not paired or blocked, we account for the variance and number replicates individually. For treatment levels 1 and 2\", the standard errror of the difference becomes \\[ SED = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}} \\] In most trials, however, we assume that the variances and number of replications are equal among treatments. So we can also express the above trial as: \\[ SED = \\sqrt\\frac{2 \\cdot s^2}{n}\\] Recall that in the analsis of variance for a multiple treatment trial, the error mean squares is the mean variance within treatments. So the equation above is equivalent to: \\[ SED = \\sqrt\\frac{2\\cdot EMS}{n}\\] In a linear contrast, however, our standard error of the difference must be scaled according to the coefficients we use, since we are no longer comparing two treatments, but multiple. So our equation becomes: \\[ SED = c \\cdot \\sqrt{\\frac{EMS}{n}} \\] Where \\(c\\) is the square root of the sum of the squared constants is the For our example above, this sum, \\(c\\), would be: \\[ c = s\\sqrt{\\sum (\\frac{1}{2})^2 + (\\frac{1}{2})^2 + (- \\frac{1}{2})^2 + (- \\frac{1}{2})^2} \\] Which is equal to: \\[ c = \\sqrt{\\frac{1}{4} + \\frac{1}{4} +\\frac{1}{4} +\\frac{1}{4}} = \\sqrt{1} = 1\\] We can now calculate the standard error of the difference for our contrast. First, lets go back to our analysis of variance. saffron_model = aov(corm_number ~ rate, saffron) summary(saffron_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## rate 3 4473 1490.9 94.66 1.28e-08 *** ## Residuals 12 189 15.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can see from the ANOVA output thtat our error mean square is 15.75. We know from the design itself that we had 4 replicates So our final standard error of the difference for our contrast is: \\[ SED = 1 \\cdot \\sqrt{\\frac{15.75}{4}} = 1 \\cdot \\sqrt{3.9375} = 1.984 \\] The t-value for our test, would then be: \\[ t = \\frac{L}{SED} = \\frac{19.75}{1.984} = 9.954 \\] The probability of a t-value of 10, given the 12 degrees of freedom associated with the error sum of squares, would be: pt(9.954,12, lower.tail = FALSE) ## [1] 1.882084e-07 8.6.2 Linear Contrasts with R We can automate linear contrast testing, of course, using R and the glht() function of the multcomp package. First, we need to define a matrix (table) of contrast coefficients for R. To get the coefficients in the correct order, lets double check the order in which the rate levels are listed in R. It is important to make sure our treatment is classified as a factor. We can do this using the as.factor() function saffron$rate = as.factor(saffron$rate) We can then list the order of the levels in treatment rate: levels(saffron$rate) ## [1] &quot;1X&quot; &quot;2X&quot; &quot;3X&quot; &quot;4X&quot; We can see they follow the order 1X, 2X, 3X, 4X. We will therefore form a matrix, K, with the appropriate coefficients. K = matrix(c(1/2, 1/2, -1/2, -1/2),1) K ## [,1] [,2] [,3] [,4] ## [1,] 0.5 0.5 -0.5 -0.5 We are now ready to run our contrast. First, we need to slightly alter our ANOVA model by adding a zero (0) between the tilde (~) and the treatment name. This is one of of those one-off ideosyncrasities of R. library(multcomp) ## Loading required package: mvtnorm ## Loading required package: survival ## Loading required package: TH.data ## Loading required package: MASS ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select ## ## Attaching package: &#39;TH.data&#39; ## The following object is masked from &#39;package:MASS&#39;: ## ## geyser saffron_model_for_contrast = aov(corm_number ~ 0 + rate, saffron) low_vs_high = glht(saffron_model_for_contrast, linfct=K) summary(low_vs_high) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: aov(formula = corm_number ~ 0 + rate, data = saffron) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 == 0 19.750 1.984 9.953 3.77e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) And there we have it, our contrast in seconds. Now lets interpret this. Estimate is the difference between groups. Since we subtracted the mean of the 3x and 4X rates from the mean of the 1X and 2X rates, the positive estimate value indicates the lower rates produced greater corm numbers than the higher rates. Std. Error is the standard error of the difference, as we calculated above. The t-value is equal to the estimate divided by the standard error of the difference. Finally, Pr(&gt;|t|) is the probability of observing the t-value by chance. In this case, it is \\(3.77 \\times 10^{-7}\\), very close to zero. We conclude the lower two rates, as a group, produce greater corms than the upper two rates as a group. We can quickly as other questions of our data. For example, do the middle two rates (2X and 3X) produce a greater corm number than the lowest (1X and highest (4X) rates? Again, lets examine the order of our rate levels. levels(saffron$rate) ## [1] &quot;1X&quot; &quot;2X&quot; &quot;3X&quot; &quot;4X&quot; In order to subtract the mean corm number of rates 1X and 4X from the mean corm number of rates 2X and 3X, we will need to calculate the difference as: \\[ L = (-\\frac{1}{2})\\bar{x}_{1X} + (\\frac{1}{2})\\bar{x}_{2X} + (\\frac{1}{2})\\bar{x}_{3X} + (- \\frac{1}{2})\\bar{x}_{4X} + \\] So our contrasts coefficients would be \\(-\\frac{1}{2}\\), \\(\\frac{1}{2}\\), \\(\\frac{1}{2}\\), \\(-\\frac{1}{2}\\). Our contrast maxtrix is then: K = matrix(c(-1/2, 1/2, 1/2, -1/2),1) K ## [,1] [,2] [,3] [,4] ## [1,] -0.5 0.5 0.5 -0.5 We are now ready to run our contrast. First, we need to slightly alter our ANOVA model by adding a zero (0) between the tilde (~) and the treatment name. This is one of of those one-off ideosyncrasities of R. library(multcomp) saffron_model_for_contrast = aov(corm_number ~ 0 + rate, saffron) low_high_vs_middle = glht(saffron_model_for_contrast, linfct=K) summary(low_high_vs_middle) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: aov(formula = corm_number ~ 0 + rate, data = saffron) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 == 0 24.750 1.984 12.47 3.14e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) We see this estimated difference is even greater than the previous contrast. The significant t-value suggests there may be a parabolic (curved) response of corm number to nutrient solution rate. Indeed, if we plot the mean, we can see a parabolic response saffron_means %&gt;% ggplot(aes(x=rate, y=corm_number)) + geom_point(size = 3, color=&quot;blue&quot;) 8.7 Means Presentation Something I have always overlooked in teaching this course is also something that is most basic: once we have separated our means, how do we present them to others? There are two ways to present means: in a table, or in a plot. Both have advantages and disadvantages. In a table, you provide raw statistics (treatment means, standard errors, and perhaps, means groupings from an LSD or Tukey test). A reader can use these values to recalculate your statistics, say, if they wanted to separate the means at p=0.10, p=0.1, or p=0.001. A figure, one the otherhand, allows the reader to quickly grasp treatment differences, or patterns among treatments. In addition, they are colorful and  lets face it  more inviting than a dense table of numbers. Whichever format we use, however, we need to present our treatment means and some reference for the reader to gauge whether those means are statistically equal or different. 8.7.1 Means Tables Tables can be tricky, especially when many types of measures are included. If we are reporting results from a factorial experiment, we may be tempted to list the levels of one factor down rows, and the other factor across columns. I would generally discourage this, however, unless required to fit the table neatly into a publication space. Generally, the long form of means presentation is best. In the long form of data, cases (individual observations or treatment means) are listed down the rows. Measurements from each cases are listed across rows. For our saffron data, our table would start like this: final_table = saffron_means knitr::kable(final_table) rate corm_number 1X 249.25 2X 284.75 3X 254.25 4X 240.25 To this table, we may wish to add the standard error of the difference. Remember, the standard error of the difference for an LSD or Tukey test is equal to: \\[ SED = \\sqrt{\\frac{(2 \\cdot EMS)}{n}} \\] So for our saffron trial, where the error mean square is 15.75 (we can get this from either the ANOVA table or the LSD output)and the number of replications is 4, the standard error of the difference is: \\[ SED = \\sqrt{\\frac{(2 \\cdot 15.75)}{4}} = \\sqrt{7.875} = 2.80 \\] We can add this below the means final_table = saffron_means %&gt;% mutate(rate = as.character(rate)) %&gt;% rbind(c(&quot;SED&quot;, 2.80)) knitr::kable(final_table) rate corm_number 1X 249.25 2X 284.75 3X 254.25 4X 240.25 SED 2.8 It is also important to indicate the number of replications of the treatments. We can add another row to the table with N. final_table = saffron_means %&gt;% mutate(rate = as.character(rate)) %&gt;% rbind(c(&quot;SED&quot;, 2.80)) %&gt;% rbind(c(&quot;N&quot;, 4)) knitr::kable(final_table) rate corm_number 1X 249.25 2X 284.75 3X 254.25 4X 240.25 SED 2.8 N 4 We should add the LSD, to make it easy for readers to compare treatments. We will want also, to include the \\(\\alpha\\) which was used to calcualate the LSD. That way, the reader will know whether the risk of a Type I error  that the LSD will separate treatments that are not truely different  is 5% or some other probability. final_table = saffron_means %&gt;% mutate(rate = as.character(rate)) %&gt;% rbind(c(&quot;SED&quot;, 2.80)) %&gt;% rbind(c(&quot;N&quot;, 4)) %&gt;% rbind(c(&quot;LSD (0.05)&quot;, 6.11)) knitr::kable(final_table) rate corm_number 1X 249.25 2X 284.75 3X 254.25 4X 240.25 SED 2.8 N 4 LSD (0.05) 6.11 We might want to include the p-value from the ANOVA table, so the reader knows that the LSD is protected. The p-value for the saffron trial is \\(1.28 \\times 10^{-8}\\). This is an extremely small number. It is acceptable for us to simplify this in the table, indicating that the probability of F was less than 0.001. final_table = saffron_means %&gt;% mutate(rate = as.character(rate)) %&gt;% rbind(c(&quot;SED&quot;, 2.80)) %&gt;% rbind(c(&quot;N&quot;, 4)) %&gt;% rbind(c(&quot;LSD (0.05)&quot;, 6.11)) %&gt;% rbind(c(&quot;Pr&lt;F&quot;, &quot;&lt;0.001&quot;)) knitr::kable(final_table) rate corm_number 1X 249.25 2X 284.75 3X 254.25 4X 240.25 SED 2.8 N 4 LSD (0.05) 6.11 Pr&lt;F &lt;0.001 Finally, if imperial (pounds, acres, etc) or metric units were used to measure the response variable, it is important to indicate that in the table. Indicating those in parentheses after the variable name is appropriate. 8.7.2 Plotting Means When we work with categorical treatments (that is, treatments levels that are defined by words), or even numerical variables treated as categorical variables (as in the saffron example) we should use a bar plot, not a line plot, to visualize the data. An easy way to determine whether a bar plot should be used is this: if you are using an LSD or Tukeys test to separate your means, you should use a bar plot. A line plot, which suggests treatment levels are numerically related to (higher or lower than) each other should be used to fit regression models, where the analysis defines a continuous relationship between Y and X. A basic bar plot will has a bar representing each treatment mean. The treatment level is indicated along the x (horizontal) axis. The sample mean is indicated along the y (vertical) axis. The bar height indicates the sample mean. saffron_means %&gt;% ggplot(aes(x=rate, y=corm_number)) + geom_bar(stat=&quot;identity&quot;, color=&quot;black&quot;, fill = &quot;darkolivegreen&quot;) + theme(axis.text = element_text(size=12), axis.title = element_text(size=16)) This plot, however, does not provide the viewer any sense of whether corm number is significantly different among treatments. For that purpose, we can add error bars. saffron_means %&gt;% ggplot(aes(x=rate, y=corm_number)) + geom_bar(stat=&quot;identity&quot;, color=&quot;black&quot;, fill = &quot;darkolivegreen&quot;) + geom_errorbar(aes(ymin=corm_number-1.81, ymax=corm_number+1.81), width=0.4, size=1) + theme(axis.text = element_text(size=12), axis.title = element_text(size=16)) These error bars stretch from one standard error of the mean below the sample mean to one standard error of the mean above the sample mean. As a general rule of thumb, the least significant difference is approximately 2 standard errors of the mean. If the error bars from two treatment levels dont overlap, then the two treatments are likely different. We can see in the plot that the corm number for the 1X and 3X fertilizer rates are very similar. Alternatively, we could set the error bar height equal to the least significant difference itself. saffron_means %&gt;% ggplot(aes(x=rate, y=corm_number)) + geom_bar(stat=&quot;identity&quot;, color=&quot;black&quot;, fill = &quot;darkolivegreen&quot;) + geom_errorbar(aes(ymin=corm_number-5.63, ymax=corm_number+5.63), width=0.4, size=1) + theme(axis.text = element_text(size=12), axis.title = element_text(size=16)) This allows the viewer to use the actual LSD to separate treatment means. If the range of the error bar from one treatment does not include the mean of another treatment, then the two treatments are not equal. Significant differences among treatments can also be indicated by including letter groupings from an LSD or Tukeys test. lsd$groups %&gt;% rownames_to_column(var=&quot;rate&quot;) %&gt;% ggplot(aes(x=rate, y=corm_number)) + geom_bar(stat=&quot;identity&quot;, color=&quot;black&quot;, fill = &quot;darkolivegreen&quot;) + geom_text(aes(x=rate, y=corm_number+15, label=groups), size=6) + theme(axis.text = element_text(size=12), axis.title = element_text(size=16)) We will learn more about graphics in the exercises this unit. A very comprehensive resource for creating plots in R is R Graphics Cookbook, by Winston Chang. An online copy is available at https://r-graphics.org/. Print copies can also be purchased from common booksellers. This book explains not only how to create plots, but how to adjust labels, legends, axis lables, and so on. 8.8 Exercise: LSD and Tukeys HSD In the lecture, we learned two ways of grouping treatment means: by Least Significant Difference\" and by Tukeys HSD (Honest Significant Difference. Both of these tests can be run very quicly in R. 8.8.1 Case Study: Common Bean The yield of organically-grown common bean was assessed following four crops: conventional broccoli, green_manure, fallow, and organic broccoli. Yield is in tons per hectare. We wish to separate the treatment means. common_bean = read.csv(&quot;data-unit-8/exercise_data/common_bean.csv&quot;) head(common_bean) ## block prev_crop fresh_weight ## 1 1 conv_broccoli 52.15924 ## 2 1 green_manure 49.82191 ## 3 1 organic_broccoli 36.80926 ## 4 1 fallow 54.05342 ## 5 2 conv_broccoli 52.79726 ## 6 2 fallow 47.68364 First, we need to create our linear model and run our analysis of variance. The trial is a randomized complete block design, so our linear model is: fresh_weight = mu + block + prev_crop + error Our model statement in R is: common_bean_model = aov(fresh_weight ~ block + prev_crop, data=common_bean) Our analysis of variance is: summary(common_bean_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## block 1 2.4 2.37 0.179 0.6802 ## prev_crop 3 428.9 142.98 10.835 0.0013 ** ## Residuals 11 145.1 13.20 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see the effect of previous crop is significant, so now we want to separate the treatment means. 8.8.2 Least Significant Difference Both the least significant test and Tukeys Honest Significant Test will be run using functions from the agricolae package. We load the agricolae package to our session using the library() command library(agricolae) After this. We can run the least significant test using tne LSD.test() function. This argument takes two arguments: a linear model, and the treatment we wish to separate (which must be given in quotes). We can simply reference the common_bean_model created above. lsd_common_bean = LSD.test(common_bean_model, &quot;prev_crop&quot;) lsd_common_bean ## $statistics ## MSerror Df Mean CV t.value LSD ## 13.19522 11 48.04057 7.561363 2.200985 5.653408 ## ## $parameters ## test p.ajusted name.t ntr alpha ## Fisher-LSD none prev_crop 4 0.05 ## ## $means ## fresh_weight std r LCL UCL Min Max ## conv_broccoli 52.79032 1.242884 4 48.79276 56.78788 51.68091 54.52386 ## fallow 50.53382 2.718281 4 46.53625 54.53138 47.68364 54.05342 ## green_manure 49.52910 5.135442 4 45.53153 53.52666 44.06679 56.34669 ## organic_broccoli 39.30905 3.723475 4 35.31149 43.30662 36.10887 44.26354 ## Q25 Q50 Q75 ## conv_broccoli 52.03966 52.47825 53.22891 ## fallow 48.93562 50.19911 51.79730 ## green_manure 46.92744 48.85145 51.45311 ## organic_broccoli 36.63416 38.43190 41.10679 ## ## $comparison ## NULL ## ## $groups ## fresh_weight groups ## conv_broccoli 52.79032 a ## fallow 50.53382 a ## green_manure 49.52910 a ## organic_broccoli 39.30905 b ## ## attr(,&quot;class&quot;) ## [1] &quot;group&quot; The test results are returned in a list, which in R is a collection of objects grouped under one name (lsd_common_bean in our code above). We can reference different parts of this list the same way you would reference columns within a data frame: using the dollar sign to separate the list name and the object. For example, if we just wanted to look at our $statistics, we would type: lsd_common_bean$statistics ## MSerror Df Mean CV t.value LSD ## 13.19522 11 48.04057 7.561363 2.200985 5.653408 We can reference the means groupings, as well: lsd_common_bean$groups ## fresh_weight groups ## conv_broccoli 52.79032 a ## fallow 50.53382 a ## green_manure 49.52910 a ## organic_broccoli 39.30905 b This selectivity is handy if we are writing a report and just want to reference part of the output. In the grouping above, we see that common bean fresh weight was statistically similar following conventional broccoli, fallow, and green manure. Common bean fresh weight following organic broccoli, however, was statistically less than following the other three treatments. 8.8.3 Tukey HSD We can separate the means, very similarly, using Tukeys Honest Significant Difference. We use the HSD.test() function. The first two arguments are the same as for the LSD.test(): the linear model and treatment (in quotes) we wish to separate. We add a third argument, group = TRUE. This tells the HSD.test to return a letter-grouping of treatment means. common_bean_hsd = HSD.test(common_bean_model, &quot;prev_crop&quot;, group = TRUE) common_bean_hsd ## $statistics ## MSerror Df Mean CV MSD ## 13.19522 11 48.04057 7.561363 7.730267 ## ## $parameters ## test name.t ntr StudentizedRange alpha ## Tukey prev_crop 4 4.256143 0.05 ## ## $means ## fresh_weight std r Min Max Q25 Q50 ## conv_broccoli 52.79032 1.242884 4 51.68091 54.52386 52.03966 52.47825 ## fallow 50.53382 2.718281 4 47.68364 54.05342 48.93562 50.19911 ## green_manure 49.52910 5.135442 4 44.06679 56.34669 46.92744 48.85145 ## organic_broccoli 39.30905 3.723475 4 36.10887 44.26354 36.63416 38.43190 ## Q75 ## conv_broccoli 53.22891 ## fallow 51.79730 ## green_manure 51.45311 ## organic_broccoli 41.10679 ## ## $comparison ## NULL ## ## $groups ## fresh_weight groups ## conv_broccoli 52.79032 a ## fallow 50.53382 a ## green_manure 49.52910 a ## organic_broccoli 39.30905 b ## ## attr(,&quot;class&quot;) ## [1] &quot;group&quot; The HSD.test output, like the lsd output is a list. We can again reference individual tables within it: common_bean_hsd$groups ## fresh_weight groups ## conv_broccoli 52.79032 a ## fallow 50.53382 a ## green_manure 49.52910 a ## organic_broccoli 39.30905 b 8.8.4 Practice: Apple Apple yield was measured after treatment with four biostimulants. The design was a randomized complete block. The yield was measured in tons per hectare. apple = read.csv(&quot;data-unit-8/exercise_data/apple.csv&quot;) 8.8.4.1 Analysis of Variance Run the analysis of variance. Your results should be: Df Sum Sq Mean Sq F value Pr(&gt;F) block 1 0.40 0.40 0.076 0.78800 foliar_trt 3 162.10 54.03 10.345 0.00156 ** Residuals 11 57.45 5.22 Signif. codes: 0  0.001  0.01  0.05 . 0.1   1 8.8.4.2 Least Significant Difference Run the Least Significant Difference test on the treatment means. Your results should look like: $statistics MSerror Df Mean CV t.value LSD 5.222995 11 47.96522 4.764676 2.200985 3.55682 $parameters test p.ajusted name.t ntr alpha Fisher-LSD none foliar_trt 4 0.05 $means yield std r LCL UCL Min Max Q25 Q50 Q75 conv 45.70984 1.678378 4 43.19478 48.22489 44.44665 48.15233 44.71817 45.12018 46.11185 conv+calcium 44.08293 1.274706 4 41.56788 46.59798 42.34804 45.30697 43.58313 44.33835 44.83815 conv+seaweed 51.90854 2.073312 4 49.39349 54.42359 50.18903 54.78184 50.51099 51.33165 52.72920 conv+si+zn 50.15958 3.246956 4 47.64452 52.67463 46.88179 54.60709 48.55789 49.57471 51.17639 $comparison NULL $groups yield groups conv+seaweed 51.90854 a conv+si+zn 50.15958 a conv 45.70984 b conv+calcium 44.08293 b attr(,class) [1] group 8.8.4.3 Tukeys HSD Run the Honest Significant Difference test on the treatment means. Your results should look like: $statistics MSerror Df Mean CV MSD 5.222995 11 47.96522 4.764676 4.863468 $parameters test name.t ntr StudentizedRange alpha Tukey foliar_trt 4 4.256143 0.05 $means yield std r Min Max Q25 Q50 Q75 conv 45.70984 1.678378 4 44.44665 48.15233 44.71817 45.12018 46.11185 conv+calcium 44.08293 1.274706 4 42.34804 45.30697 43.58313 44.33835 44.83815 conv+seaweed 51.90854 2.073312 4 50.18903 54.78184 50.51099 51.33165 52.72920 conv+si+zn 50.15958 3.246956 4 46.88179 54.60709 48.55789 49.57471 51.17639 $comparison NULL $groups yield groups conv+seaweed 51.90854 a conv+si+zn 50.15958 ab conv 45.70984 bc conv+calcium 44.08293 c attr(,class) [1] group 8.8.5 Practice: Wheat Treatment with Mildew Four foliar treatments against powdery mildew in wheat were tested in Rothemsted, England, for their effect on grain yield. The same fungicide was tested with four timings: 0 (none), 1 (early), 2 (late), R (repeated). The treatment was a randomized complete block design. Yield was measured in tons per hectare. wheat_mildew = read.csv(&quot;data-unit-8/exercise_data/mildew.csv&quot;) head(wheat_mildew) ## plot trt block yield ## 1 1 T2 B1 5.73 ## 2 2 R B1 6.08 ## 3 3 T0 B1 5.26 ## 4 4 T1 B1 5.89 ## 5 5 T0 B2 5.37 ## 6 6 T2 B2 5.95 8.8.5.1 Analysis of Variance Run the analysis of variance on the response of wheat yield to fungicide treatment. Your results should look like: Df Sum Sq Mean Sq F value Pr(&gt;F) block 8 5.085 0.6356 17.53 2.79e-08 trt 3 3.129 1.0432 28.77 4.05e-08 Residuals 24 0.870 0.0363 Signif. codes: 0  0.001  0.01  0.05 . 0.1   1 8.8.5.2 Least Significant Difference Run the Least Significant Difference test on the treatment means. Your results should look like: $statistics MSerror Df Mean CV t.value LSD 0.03625532 24 5.801944 3.281802 2.063899 0.1852542 $parameters test p.ajusted name.t ntr alpha Fisher-LSD none trt 4 0.05 $means yield std r LCL UCL Min Max Q25 Q50 Q75 R 5.942222 0.4649403 9 5.811228 6.073217 5.06 6.54 5.76 6.01 6.18 T0 5.310000 0.4519956 9 5.179006 5.440994 4.38 5.82 5.16 5.26 5.73 T1 5.867778 0.4602928 9 5.736783 5.998772 5.04 6.45 5.59 5.89 6.26 T2 6.087778 0.3347304 9 5.956783 6.218772 5.63 6.48 5.76 6.14 6.43 $comparison NULL $groups yield groups T2 6.087778 a R 5.942222 ab T1 5.867778 b T0 5.310000 c attr(,class) [1] group 8.8.5.3 Tukeys Honest Significant Difference Test Run the HSD test on the treatment means. Your results should look like: $statistics MSerror Df Mean CV MSD 0.03625532 24 5.801944 3.281802 0.2476109 $parameters test name.t ntr StudentizedRange alpha Tukey trt 4 3.901262 0.05 $means yield std r Min Max Q25 Q50 Q75 R 5.942222 0.4649403 9 5.06 6.54 5.76 6.01 6.18 T0 5.310000 0.4519956 9 4.38 5.82 5.16 5.26 5.73 T1 5.867778 0.4602928 9 5.04 6.45 5.59 5.89 6.26 T2 6.087778 0.3347304 9 5.63 6.48 5.76 6.14 6.43 $comparison NULL $groups yield groups T2 6.087778 a R 5.942222 a T1 5.867778 a T0 5.310000 b 8.9 Exercise: Linear Contrasts Linear contrasts are different from least significant difference (LSD) or Tukeys honest significant difference (HSD) tests in that they can test groups of treatments. As we learned in the lecture, the simplest way to think of a linear constrast is as a t-test between two groups. We also learned in the lecture that we use contrast coefficients to define these groups. The coefficients for one group should add up to 1; the coefficients of the other group should add up to -1. That way, we subtract the average value for treatments in one group from the average value for treatments in the other group. 8.9.1 Case Study: Winter Canola Cultivar Trial. Six winter canola cultivars were tested in a randomized complete block trial in Manitoba. Yield is in kg / ha. canola = read.csv(&quot;data-unit-8/exercise_data/canola_gd.csv&quot;) head(canola) ## block cultivar yield ## 1 1 Bob 1508.424 ## 2 2 Bob 1962.072 ## 3 3 Bob 1364.861 ## 4 4 Bob 1832.715 ## 5 1 Donna 1212.190 ## 6 2 Donna 1629.680 8.9.1.1 ANOVA First, lets run our analysis of variance canola_model = aov(yield ~ block + cultivar, data = canola) summary(canola_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## block 1 18600 18600 0.171 0.684822 ## cultivar 5 4551035 910207 8.344 0.000388 *** ## Residuals 17 1854462 109086 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 8.9.1.2 Least Significant Difference Test Next, lets run an LSD test. library(agricolae) lsd_canola = LSD.test(canola_model, &quot;cultivar&quot;) lsd_canola ## $statistics ## MSerror Df Mean CV t.value LSD ## 109086 17 2296.983 14.37894 2.109816 492.7357 ## ## $parameters ## test p.ajusted name.t ntr alpha ## Fisher-LSD none cultivar 6 0.05 ## ## $means ## yield std r LCL UCL Min Max Q25 ## Bill 2296.268 326.6421 4 1947.851 2644.685 1865.903 2614.081 2142.888 ## Bob 1667.018 277.4666 4 1318.601 2015.435 1364.861 1962.072 1472.533 ## Donna 1833.282 514.0721 4 1484.865 2181.699 1212.190 2358.635 1525.308 ## Jerry 2669.161 176.6376 4 2320.745 3017.578 2545.158 2923.794 2552.218 ## Mickey 2406.509 263.5791 4 2058.092 2754.926 2130.413 2656.553 2207.614 ## Phil 2909.658 275.1842 4 2561.242 3258.075 2565.683 3218.583 2776.782 ## Q50 Q75 ## Bill 2352.545 2505.925 ## Bob 1670.570 1865.055 ## Donna 1881.152 2189.126 ## Jerry 2603.847 2720.790 ## Mickey 2419.535 2618.430 ## Phil 2927.183 3060.060 ## ## $comparison ## NULL ## ## $groups ## yield groups ## Phil 2909.658 a ## Jerry 2669.161 ab ## Mickey 2406.509 b ## Bill 2296.268 bc ## Donna 1833.282 cd ## Bob 1667.018 d ## ## attr(,&quot;class&quot;) ## [1] &quot;group&quot; 8.9.1.3 Contrasts Contrasts should be based on specific questions we want to ask of the data. In this case, there are two questions we would like to answer with contrasts. First, cultivars Donna and Bob are from a line of canola called PITB. Sometimes they perform well, but other times their performance can be rather discordant. Our first question is, do cultivars from the PITB line perform worse than cultivars from other lines. Cultivars Mickey and Bill are from the FOTM line of cultivars. They regularly perform better than cultivars with PITB pedigrees. But in bad seasons they have a field performance that has been likened to sneakers in a drier. Meanwhile, cultivars Phil and Jerry, from the DKSTR line, under appropriate growing conditions, have a performance potential that is almose unlimited. Our second question, then, is whether cultivars from the DKSTR outperform those from the FOTM line. 8.9.1.4 Figuring Out Which Coefficients to Use To answer these questions, we need to define our coefficients. The order of the coefficients must reflect the order of the factor levels in R. To determine this order we use the levels() function. levels(canola$cultivar) ## NULL What coefficients do we use, then? Our first hypothesis is that Bob and Donna perform worse than others. So we will assign a -1 to both those cultivars. Coefficients: 0 -1 -1 0 0 0 Since we are comparing against the other four cultivars, we assign them +1: Coefficients: +1 -1 -1 +1 +1 +1 The coefficients of each group must sum to 1. So we will divide the coefficients for Bob and Donna by 2: Coefficients: +1 -1/2 -1/2 +1 +1 +1 And we will divide the coefficients for the other cultivars by 4: Coefficients: +1/4 -1/2 -1/2 +1/4 +1/4 +1/4 Now, lets check our math. The coefficients for Bob and Donna should sum to -1 Coefficients: 0 -1/2 -1/2 0 0 0 = -1 The coefficients for the other four cultivars should sum to 1. Coefficients: +1/4 0 0 +1/4 +1/4 +1/4 = 1 Finally, all the coefficients together should sum to zero: Coefficients: +1/4 -1/2 -1/2 +1/4 +1/4 +1/4 = 0 8.9.1.5 Telling R the Coefficients to Use Running the linear coefficient in R requires we define a coefficient matrix. A matrix is a slightly more primative version of the data.frame  still akin to a table, but its rows and columns tend to be defined by numbers instead of names. To create a coefficient matrix in R, we need to define it as follows: K = matrix(c(insert coefficients here),1) c(coefficients) tells R to fill in the coefficients across columns. The ,1 that follows tells R to put those values in the first row. That second part of the matrix, the ,1 is really important. In creating this lesson, I spent at least a half-hour trying to figure out why one of my contrasts would not work. It was because I had written it: K_bd_vs_others = matrix(c(+1/4, -1/2, -1/2, +1/4, +1/4, +1/4)) Instead of: K_bd_vs_others = matrix(c(+1/4, -1/2, -1/2, +1/4, +1/4, +1/4), 1) K_bd_vs_others = matrix(c(+1/4, -1/2, -1/2, +1/4, +1/4, +1/4),1) K_bd_vs_others ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.25 -0.5 -0.5 0.25 0.25 0.25 Above is our coefficient matrix for Bob and Donna versus the others. Now lets create our second matrix, for Mickey and Bill versus Phil and Jerry. Lets check the order of the factor levels again. levels(canola$cultivar) ## NULL Our hypothesis is that Phil and Jerry perform better than Mickey and Bill, so we will assign them the initial coefficients of 1 So we will assign a -1 to both those cultivars. Coefficients: 0 0 0 1 0 1 We then assign Mickey and Bill the coefficients -1: Coefficients: -1 0 0 +1 -1 +1 Donna and Bob are assigned the coefficient zero since they are not involved in this comparison. The coefficients of each group must sum to 1. So we will divide the coefficients for Jerry and Phil by 2: Coefficients: -1 0 0 +1/2 -1 +1/2 We do the same with Mickey and Bill: Coefficients: -1/2 0 0 +1/2 -1/2 +1/2 Now, lets check our math. The coefficients for Jerry and Phil should sum to 1 Coefficients: -0 0 0 +1/2 0 +1/2 = 1 The coefficients for Mickey and Bill should sum to -1. Coefficients: -1/2 0 0 0 -1/2 0 = -1 Finally, all the coefficients together should sum to zero: Coefficients: -1/2 0 0 +1/2 -1/2 +1/2 = 0 Our correlation coefficient for our second contrast, then, is: K_jp_vs_mb = matrix(c(-1/2, 0, 0, +1/2, -1/2, +1/2),1) K_jp_vs_mb ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -0.5 0 0 0.5 -0.5 0.5 8.9.2 Running the Contrast Defining the contrast coefficients is difficult. Running the contrast afterwards is relatively easy. We use the ghlt() function from the multicomp() package. glht stands for generalized linear hypothesis test. glht() requires two arguments: a linear model, and the coefficient contrast we created above. The linear model is a little different than what we defined for the ANOVA above. It only contains the terms 0 and the treatment name: canola_contrast_model = aov(yield ~ 0 + cultivar, data=canola) The reason for this is because our contrast is isolating the effect of cultivar. The zero forces our contrast to calculate the actual difference between the two groups means (which we may like to report). Now we can run our contrast library(multcomp) bd_vs_others = glht(canola_contrast_model, linfct = K_bd_vs_others) We will use summary() to summarise these results. summary(bd_vs_others) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: aov(formula = yield ~ 0 + cultivar, data = canola) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 == 0 820.2 139.7 5.872 1.47e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) Ok, lets review our results. The Estimate, 820.2 kg / ha, is the difference between the group means. Since we subracted the mean of Bob and Donna from the mean of the other four cultivars, we concluded that Bob and Donna as a group yielded less than the other four cultivars as a group. The Estimate was divided by Std. Error, the standard error of the difference, to produce the t value of 5.872. The probability of observing a t-value of this size or greater, if in fact the true difference between groups was zero, is very small, 1.47 x 10^-5. The difference between groups is significant. We conclude that cultivars from the PITB line yielded worse than other cultivars as a group. We can now run the second contrast, Jerry and Phil versus Mickey and Bill. We again use the canola_contrast_model as our linear model, and this time, the coefficient matrix K_jp_vs_mb jp_vs_mb = glht(canola_contrast_model, linfct=K_jp_vs_mb) summary(jp_vs_mb) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: aov(formula = yield ~ 0 + cultivar, data = canola) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 == 0 438.0 161.3 2.716 0.0142 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) The mean of Jerry and Phill was 438.0 kg/ha greater than the mean of Mickey and Bill. (Note: when you look at the Estimate, always ask yourself whether the number, based on treatment means, makes sense. The first time I ran this contrast, for example, the estimate was greater than 2000. This caused me to review my contrast coefficients, where I discovered one of the coefficients was positive when it should have been negative.) Our t value again has a probability of occuring by chance of less than P=0.05, so the difference between the mean of Jerry and the mean of Phil and Mickey and Bill is significant. We conclude that cultivars from the DKSTR line yielded better than cultivars from the FOTM line. 8.9.3 Practice: Corn Nitrogen Source and Timing This is a really powerful example of how contrasts can be used, not just to test significances, but generate much broader understandings than either the ANOVA or LSD/HSD alone would allow. Our practice data set is from a nitrogen management trial was conducted near Whitehouse, Ohio, to compare 7 nitrogen management strategies in corn. All treatments (other than the control) provided 200 units (lbs) total N during the growing season. n_source_timing = read.csv(&quot;data-unit-8/exercise_data/corn_nitrogen_source_timing.csv&quot;) head(n_source_timing) ## trt yield block B Error ## 1 control 147.9109 B1 -4 3.9108674 ## 2 aa_pre 178.1029 B1 -4 -1.8970790 ## 3 aa_post 173.5400 B1 -4 5.5400054 ## 4 aa_pre_post 184.2516 B1 -4 -2.7483991 ## 5 uan_pre 170.4471 B1 -4 -0.5529385 ## 6 uan_post 165.4751 B1 -4 -1.5249071 Lets look at the seven levels of nitrogen treatment: levels(n_source_timing$trt) ## NULL Treatment abbreviations are: aa = anhydrous amonia, uan = urea ammonium nitrate, pre = pre-plant, and post = post-emergence (sidedress). 8.9.3.1 ANOVA First, lets look at our analysis of variance: corn_nitrogen_model = aov(yield ~ block + trt, data=n_source_timing) summary(corn_nitrogen_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## block 3 283 94.3 2.414 0.1 ## trt 6 3421 570.1 14.591 4.81e-06 *** ## Residuals 18 703 39.1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Our nitrogen treatments are significantly different. What does the least significant test tell us differences among treatment means? lsd = LSD.test(corn_nitrogen_model, &quot;trt&quot;) lsd$groups ## yield groups ## aa_pre_post 191.2750 a ## aa_pre 181.7686 b ## uan_pre_post 179.3731 b ## uan_pre 175.9014 bc ## aa_post 174.7789 bc ## uan_post 167.9677 c ## control 153.2038 d We have four questions: 1) is the mean yield of the six nitrogen treatments significantly different from the untreated control? is the mean yield of preplant nitrogen treatments significantly different from the mean of postemergence nitrogen treatments? is the mean yield of anhydrous ammonia treatments significantly different from the mean of urea ammonium nitrate treatments? is the mean yield of split application treatments significantly different from the mean of single application treatments? 8.9.3.2 Nitrogen Treatments vs Control Lets walk through this one together. What contrast coefficients do we use? Lets first double-check the order of factor levels (treatments): levels(n_source_timing$trt) ## NULL So we will assign -1 to the control treatment and 1 to all other treatments: coefficients: 1 1 1 -1 1 1 1 The coefficients assigned to each group should add up to an absolute value. Since there are six nitrogen treatments, we need to divide each of their coefficients by six. coefficients: 1/6 1/6 1/6 -1 1/6 1/6 1/6 Now, if we check, we will see the coefficients assigned to the six nitrogen treatments sum to 1, the control coefficient is zero, and the sum of all coefficients is 0. Now lets define our first contrast coefficient matrix by filling in the matrix below: K_n_vs_control = matrix(c(1/6, 1/6, 1/6, -1, 1/6, 1/6, 1/6), 1) # don&#39;t forget the &quot;, 1&quot; after the coefficients! We need to define our nitrogen contrast model. Remember, the model only has 0 and the factor name on the right side of the equation. Complete our equation below. n_contrast_model = aov(yield ~ 0 + trt, data=n_source_timing) Finally, run the contrast using the glht() and summary() functions below: n_vs_control = glht(n_contrast_model, linfct=K_n_vs_control) summary(n_vs_control) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: aov(formula = yield ~ 0 + trt, data = n_source_timing) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 == 0 25.307 3.701 6.838 9.26e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) What do you conclude? 8.9.3.3 Preplant vs Postemergence Treatments Compare the two pre-plant treatments (\"_pre) to the two post-emergence (_post\") treatments. Your results should look like: K_pre_vs_post = matrix(c(1/2,-1/2,0,0,1/2,-1/2,0),1) pre_vs_post = glht(n_contrast_model, K_pre_vs_post) summary(pre_vs_post) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: aov(formula = yield ~ 0 + trt, data = n_source_timing) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 == 0 -7.462 3.427 -2.178 0.041 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) Simultaneous Tests for General Linear Hypotheses Fit: aov(formula = yield ~ 0 + trt, data = n_source_timing) Linear Hypotheses: Estimate Std. Error t value Pr(&gt;|t|) 1 == 0 -7.462 3.427 -2.178 0.041 * Signif. codes: 0  0.001  0.01  0.05 . 0.1   1 (Adjusted p values reported  single-step method) 8.9.3.4 Anhydrous Ammonian vs Urea Ammonium Nitrate Compare the three anhydrous ammonia (aa_) treatments to the three urea ammonium nitrate (\"_uan\") treatments. Your results should look like: K_aa_vs_uan = matrix(c(1/3,1/3,1/3,0,-1/3,-1/3,-1/3),1) aa_vs_uan = glht(n_contrast_model, K_aa_vs_uan) summary(aa_vs_uan) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: aov(formula = yield ~ 0 + trt, data = n_source_timing) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 == 0 8.193 2.798 2.929 0.00803 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) Simultaneous Tests for General Linear Hypotheses Fit: aov(formula = yield ~ 0 + trt, data = n_source_timing) Linear Hypotheses: Estimate Std. Error t value Pr(&gt;|t|) 1 == 0 8.193 2.798 2.929 0.00803 ** Signif. codes: 0  0.001  0.01  0.05 . 0.1   1 (Adjusted p values reported  single-step method) 8.9.3.5 Split vs Single Applications Compare the two split application (\"_pre_post) treatments to the four single-application (_pre\" or \"_post\") treatments. Your results should look like: K_split_vs_single = matrix(c(-1/4, -1/4, 1/2,0,-1/4,-1/4,1/2),1) split_vs_single = glht(n_contrast_model, K_split_vs_single) summary(split_vs_single) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: aov(formula = yield ~ 0 + trt, data = n_source_timing) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 == 0 10.220 2.967 3.444 0.00243 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) Simultaneous Tests for General Linear Hypotheses Fit: aov(formula = yield ~ 0 + trt, data = n_source_timing) Linear Hypotheses: Estimate Std. Error t value Pr(&gt;|t|) 1 == 0 10.220 2.967 3.444 0.00243 ** Signif. codes: 0  0.001  0.01  0.05 . 0.1   1 (Adjusted p values reported  single-step method) attr(,class) [1] group 8.10 Exercise: Means Tables Rarely have I seen a statistics course that addresses how to present your summarise your data for others to use. Sure, you learn about tests and P-values. But how do you summarise your data for a report publication or presentation? I dont want to go too deep in the weeds, but I want you to be aware how to organize your data, and how R can be used to present it. 8.10.1 Case Study: Corn Nitrogen Source and Timing This is the same dataset we used for practice in the linear contrasts exercise. Seven N fertilizer treatments (6 combinations of source and timing, plus one unfertilized control) were tested in a randomized complete block trial near Whitehouse, Ohio. corn_n = read.csv(&quot;data-unit-8/exercise_data/corn_nitrogen_source_timing.csv&quot;) head(corn_n) ## trt yield block B Error ## 1 control 147.9109 B1 -4 3.9108674 ## 2 aa_pre 178.1029 B1 -4 -1.8970790 ## 3 aa_post 173.5400 B1 -4 5.5400054 ## 4 aa_pre_post 184.2516 B1 -4 -2.7483991 ## 5 uan_pre 170.4471 B1 -4 -0.5529385 ## 6 uan_post 165.4751 B1 -4 -1.5249071 8.10.1.1 Calculating Means We can quicly calculate means using the LSD.test() function from the agricolae package. corn_n_model = aov(yield ~ block + trt, data=corn_n) summary(corn_n_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## block 3 283 94.3 2.414 0.1 ## trt 6 3421 570.1 14.591 4.81e-06 *** ## Residuals 18 703 39.1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 library(agricolae) lsd = LSD.test(corn_n_model, &quot;trt&quot;) lsd ## $statistics ## MSerror Df Mean CV t.value LSD ## 39.07036 18 174.8955 3.573922 2.100922 9.285785 ## ## $parameters ## test p.ajusted name.t ntr alpha ## Fisher-LSD none trt 7 0.05 ## ## $means ## yield std r LCL UCL Min Max Q25 ## aa_post 174.7789 3.277009 4 168.2129 181.3449 170.7412 177.8955 172.8403 ## aa_pre 181.7686 7.217879 4 175.2026 188.3347 174.6550 191.2908 177.2409 ## aa_pre_post 191.2750 5.197261 4 184.7089 197.8410 184.2516 196.7370 189.6756 ## control 153.2038 7.242153 4 146.6378 159.7699 147.6271 163.0634 147.8399 ## uan_post 167.9677 1.734666 4 161.4016 174.5337 165.4751 169.4693 167.5775 ## uan_pre 175.9014 12.448641 4 169.3354 182.4675 162.5066 191.5746 168.4619 ## uan_pre_post 179.3731 5.336127 4 172.8070 185.9391 171.7472 184.1562 178.2194 ## Q50 Q75 ## aa_post 175.2394 177.1780 ## aa_pre 180.5644 185.0921 ## aa_pre_post 192.0556 193.6550 ## control 151.0624 156.4262 ## uan_post 168.4631 168.8533 ## uan_pre 174.7623 182.2018 ## uan_pre_post 180.7945 181.9482 ## ## $comparison ## NULL ## ## $groups ## yield groups ## aa_pre_post 191.2750 a ## aa_pre 181.7686 b ## uan_pre_post 179.3731 b ## uan_pre 175.9014 bc ## aa_post 174.7789 bc ## uan_post 167.9677 c ## control 153.2038 d ## ## attr(,&quot;class&quot;) ## [1] &quot;group&quot; We can create a separate table with just the $groups section: trt_means=lsd$groups trt_means ## yield groups ## aa_pre_post 191.2750 a ## aa_pre 181.7686 b ## uan_pre_post 179.3731 b ## uan_pre 175.9014 bc ## aa_post 174.7789 bc ## uan_post 167.9677 c ## control 153.2038 d One thing we need to fix are the row_names. These are our treatment names and we want them in a column for our final table. The rownames_to_column() argument quickly fixes this (and is a trick it took me quite a while to figure out!). The rownames_to_column function takes one argument, vars = \"\", which tells R what to name the new column. table_w_trt_column = trt_means %&gt;% rownames_to_column(var = &quot;trt&quot;) table_w_trt_column ## trt yield groups ## 1 aa_pre_post 191.2750 a ## 2 aa_pre 181.7686 b ## 3 uan_pre_post 179.3731 b ## 4 uan_pre 175.9014 bc ## 5 aa_post 174.7789 bc ## 6 uan_post 167.9677 c ## 7 control 153.2038 d 8.10.1.2 Additional Statistics to Include It would be good to add some additional statistics to this, particularly the standard error of the difference and the number of replicatons. We can calculate the standard error of the difference from the Error Mean Square given in the LSD output. First, lets start by retrieving the error mean square from the $statistics section of our lsd output. This is a case where we can use multiple dollar signs to drill down through the list and dataframe within the list, to get to the individual statistic. ems = lsd$statistics$MSerror ems ## [1] 39.07036 Here is another important trick. Analyses of variance and LSD outputs rarely report the standard error of the difference. But we can quickly calculate it if we know the error mean square and the number of replicates: SED = sqrt(2 *EMS / r) Where r is the number of replicates. We will set r as a variable and call it into the equation (that way we can call it into our table later as well.) r = 4 sed = sqrt(2 * ems/r) sed ## [1] 4.419862 Now we can add the standard error of the difference and the number of replicates to the bottom of the column yield. We will put the name of the statistic in the treatment column and the value of the statistic in the yield column. All we need to do is use the add_row() function and tell R what the value of treatment and yield are in this new row. table_w_stats = table_w_trt_column %&gt;% add_row(trt=&quot;SED&quot;, yield=sed) %&gt;% add_row(trt=&quot;Number of Reps&quot;, yield=r) table_w_stats ## trt yield groups ## 1 aa_pre_post 191.274962 a ## 2 aa_pre 181.768650 b ## 3 uan_pre_post 179.373088 b ## 4 uan_pre 175.901428 bc ## 5 aa_post 174.778906 bc ## 6 uan_post 167.967676 c ## 7 control 153.203813 d ## 8 SED 4.419862 &lt;NA&gt; ## 9 Number of Reps 4.000000 &lt;NA&gt; 8.10.1.3 Making a Presentation-Quality Table At this point, our table content is complete. If we are going to include this in a document or presentation, however, we need to spruce it up a little. While there are multiple packages to create tables in R, I think that the gt table is one you are most likely to use. Well start loading the gt package, then using the gt() function to flow in the means table we have created above. This will create a preliminary table. library(gt) gt_tbl = gt(table_w_stats) gt_tbl html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #xbpnlipkwx .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #xbpnlipkwx .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xbpnlipkwx .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #xbpnlipkwx .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #xbpnlipkwx .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xbpnlipkwx .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xbpnlipkwx .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #xbpnlipkwx .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #xbpnlipkwx .gt_column_spanner_outer:first-child { padding-left: 0; } #xbpnlipkwx .gt_column_spanner_outer:last-child { padding-right: 0; } #xbpnlipkwx .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #xbpnlipkwx .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #xbpnlipkwx .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #xbpnlipkwx .gt_from_md > :first-child { margin-top: 0; } #xbpnlipkwx .gt_from_md > :last-child { margin-bottom: 0; } #xbpnlipkwx .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #xbpnlipkwx .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #xbpnlipkwx .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xbpnlipkwx .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #xbpnlipkwx .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xbpnlipkwx .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #xbpnlipkwx .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #xbpnlipkwx .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xbpnlipkwx .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xbpnlipkwx .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #xbpnlipkwx .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xbpnlipkwx .gt_sourcenote { font-size: 90%; padding: 4px; } #xbpnlipkwx .gt_left { text-align: left; } #xbpnlipkwx .gt_center { text-align: center; } #xbpnlipkwx .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #xbpnlipkwx .gt_font_normal { font-weight: normal; } #xbpnlipkwx .gt_font_bold { font-weight: bold; } #xbpnlipkwx .gt_font_italic { font-style: italic; } #xbpnlipkwx .gt_super { font-size: 65%; } #xbpnlipkwx .gt_footnote_marks { font-style: italic; font-size: 65%; } trt yield groups aa_pre_post 191.274962 a aa_pre 181.768650 b uan_pre_post 179.373088 b uan_pre 175.901428 bc aa_post 174.778906 bc uan_post 167.967676 c control 153.203813 d SED 4.419862 NA Number of Reps 4.000000 NA 8.10.1.3.1 Choose Decimal Places One of the first things we notice is that the means in our middle column, yield, has have way too many decimal places. We can correct this with the fmt_number() function. This function takes two arguments. First, columns - vars(yield) tells R which column to format. The second argument,decimals=1\", tells the number of decimal places to round to. gt_tbl %&gt;% fmt_number(columns = vars(yield), decimals = 1) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #fcsgpmleww .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #fcsgpmleww .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #fcsgpmleww .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #fcsgpmleww .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #fcsgpmleww .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fcsgpmleww .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #fcsgpmleww .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #fcsgpmleww .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #fcsgpmleww .gt_column_spanner_outer:first-child { padding-left: 0; } #fcsgpmleww .gt_column_spanner_outer:last-child { padding-right: 0; } #fcsgpmleww .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #fcsgpmleww .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #fcsgpmleww .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #fcsgpmleww .gt_from_md > :first-child { margin-top: 0; } #fcsgpmleww .gt_from_md > :last-child { margin-bottom: 0; } #fcsgpmleww .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #fcsgpmleww .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #fcsgpmleww .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #fcsgpmleww .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #fcsgpmleww .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #fcsgpmleww .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #fcsgpmleww .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #fcsgpmleww .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fcsgpmleww .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #fcsgpmleww .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #fcsgpmleww .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #fcsgpmleww .gt_sourcenote { font-size: 90%; padding: 4px; } #fcsgpmleww .gt_left { text-align: left; } #fcsgpmleww .gt_center { text-align: center; } #fcsgpmleww .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #fcsgpmleww .gt_font_normal { font-weight: normal; } #fcsgpmleww .gt_font_bold { font-weight: bold; } #fcsgpmleww .gt_font_italic { font-style: italic; } #fcsgpmleww .gt_super { font-size: 65%; } #fcsgpmleww .gt_footnote_marks { font-style: italic; font-size: 65%; } trt yield groups aa_pre_post 191.3 a aa_pre 181.8 b uan_pre_post 179.4 b uan_pre 175.9 bc aa_post 174.8 bc uan_post 168.0 c control 153.2 d SED 4.4 NA Number of Reps 4.0 NA 8.10.1.3.2 Adjust Table Width Our table looks a little skinny, so lets adjust the width. We can do this with the cols_width function. The everything() argument tells R to set each of the columns to the same width.~200px\" sets the width at 200px. gt_tbl %&gt;% fmt_number(columns = vars(yield), decimals = 1) %&gt;% cols_width(everything() ~ &quot;200px&quot;) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #olwxawjigw .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #olwxawjigw .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #olwxawjigw .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #olwxawjigw .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #olwxawjigw .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #olwxawjigw .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #olwxawjigw .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #olwxawjigw .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #olwxawjigw .gt_column_spanner_outer:first-child { padding-left: 0; } #olwxawjigw .gt_column_spanner_outer:last-child { padding-right: 0; } #olwxawjigw .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #olwxawjigw .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #olwxawjigw .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #olwxawjigw .gt_from_md > :first-child { margin-top: 0; } #olwxawjigw .gt_from_md > :last-child { margin-bottom: 0; } #olwxawjigw .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #olwxawjigw .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #olwxawjigw .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #olwxawjigw .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #olwxawjigw .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #olwxawjigw .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #olwxawjigw .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #olwxawjigw .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #olwxawjigw .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #olwxawjigw .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #olwxawjigw .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #olwxawjigw .gt_sourcenote { font-size: 90%; padding: 4px; } #olwxawjigw .gt_left { text-align: left; } #olwxawjigw .gt_center { text-align: center; } #olwxawjigw .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #olwxawjigw .gt_font_normal { font-weight: normal; } #olwxawjigw .gt_font_bold { font-weight: bold; } #olwxawjigw .gt_font_italic { font-style: italic; } #olwxawjigw .gt_super { font-size: 65%; } #olwxawjigw .gt_footnote_marks { font-style: italic; font-size: 65%; } trt yield groups aa_pre_post 191.3 a aa_pre 181.8 b uan_pre_post 179.4 b uan_pre 175.9 bc aa_post 174.8 bc uan_post 168.0 c control 153.2 d SED 4.4 NA Number of Reps 4.0 NA 8.10.1.3.3 Align Columns We use the cols_align() function to align our columns. The first argument to this function, cols_align =, specifies whether the column(s) should be aligned to the left, center, or right. The second argument, columns = vars() tells R which columns to which the alignment should be applied. gt_tbl %&gt;% fmt_number(columns = vars(yield), decimals = 1) %&gt;% cols_width(everything() ~ &quot;200px&quot;) %&gt;% cols_align(align = &quot;left&quot;, columns = vars(trt)) %&gt;% cols_align(align = &quot;center&quot;, columns = vars(yield, groups)) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #cwukhqbugc .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #cwukhqbugc .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #cwukhqbugc .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #cwukhqbugc .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #cwukhqbugc .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #cwukhqbugc .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #cwukhqbugc .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #cwukhqbugc .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #cwukhqbugc .gt_column_spanner_outer:first-child { padding-left: 0; } #cwukhqbugc .gt_column_spanner_outer:last-child { padding-right: 0; } #cwukhqbugc .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #cwukhqbugc .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #cwukhqbugc .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #cwukhqbugc .gt_from_md > :first-child { margin-top: 0; } #cwukhqbugc .gt_from_md > :last-child { margin-bottom: 0; } #cwukhqbugc .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #cwukhqbugc .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #cwukhqbugc .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #cwukhqbugc .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #cwukhqbugc .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #cwukhqbugc .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #cwukhqbugc .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #cwukhqbugc .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #cwukhqbugc .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #cwukhqbugc .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #cwukhqbugc .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #cwukhqbugc .gt_sourcenote { font-size: 90%; padding: 4px; } #cwukhqbugc .gt_left { text-align: left; } #cwukhqbugc .gt_center { text-align: center; } #cwukhqbugc .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #cwukhqbugc .gt_font_normal { font-weight: normal; } #cwukhqbugc .gt_font_bold { font-weight: bold; } #cwukhqbugc .gt_font_italic { font-style: italic; } #cwukhqbugc .gt_super { font-size: 65%; } #cwukhqbugc .gt_footnote_marks { font-style: italic; font-size: 65%; } trt yield groups aa_pre_post 191.3 a aa_pre 181.8 b uan_pre_post 179.4 b uan_pre 175.9 bc aa_post 174.8 bc uan_post 168.0 c control 153.2 d SED 4.4 NA Number of Reps 4.0 NA 8.10.1.3.4 Assign Formal Column Headers While it is great to use simple column names when we are processing our data, when we go to present a table we should use more formal column headings. We use the function cols_label(). We specify the current column name and desired name in our argument. For example, the current name of the left column is trt, but we want to rename it Treatment. The second argument is a little fancier. We want to rename our column so that it includes Yield and the measurement units, bu/acre. But we want to use the more scientifically accepted practice of foregoing the forward-slash / and instead write bu acre-1 where the -1 is in superscript. To get a superscript -1, we need to use a couple of html arguments:  specifices the following text is superscript; ends the superscript text. So our text argument is Yield (bu acre-1). To have R render it in html, however, we need to place our text string inside the html() function. gt_tbl %&gt;% fmt_number(columns = vars(yield), decimals = 1) %&gt;% cols_width(everything() ~ &quot;200px&quot;) %&gt;% cols_align(align = &quot;left&quot;, columns = vars(trt)) %&gt;% cols_align(align = &quot;center&quot;, columns = vars(yield, groups)) %&gt;% cols_label(trt = &quot;Treatment&quot;, yield = html(&quot;Yield (bu acre&lt;sup&gt;-1&lt;/sup&gt;)&quot;), groups = &quot;LSD Groupings&quot;) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #bryeodlgip .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #bryeodlgip .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #bryeodlgip .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #bryeodlgip .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #bryeodlgip .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #bryeodlgip .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #bryeodlgip .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #bryeodlgip .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #bryeodlgip .gt_column_spanner_outer:first-child { padding-left: 0; } #bryeodlgip .gt_column_spanner_outer:last-child { padding-right: 0; } #bryeodlgip .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #bryeodlgip .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #bryeodlgip .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #bryeodlgip .gt_from_md > :first-child { margin-top: 0; } #bryeodlgip .gt_from_md > :last-child { margin-bottom: 0; } #bryeodlgip .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #bryeodlgip .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #bryeodlgip .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #bryeodlgip .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #bryeodlgip .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #bryeodlgip .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #bryeodlgip .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #bryeodlgip .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #bryeodlgip .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #bryeodlgip .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #bryeodlgip .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #bryeodlgip .gt_sourcenote { font-size: 90%; padding: 4px; } #bryeodlgip .gt_left { text-align: left; } #bryeodlgip .gt_center { text-align: center; } #bryeodlgip .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #bryeodlgip .gt_font_normal { font-weight: normal; } #bryeodlgip .gt_font_bold { font-weight: bold; } #bryeodlgip .gt_font_italic { font-style: italic; } #bryeodlgip .gt_super { font-size: 65%; } #bryeodlgip .gt_footnote_marks { font-style: italic; font-size: 65%; } Treatment Yield (bu acre-1) LSD Groupings aa_pre_post 191.3 a aa_pre 181.8 b uan_pre_post 179.4 b uan_pre 175.9 bc aa_post 174.8 bc uan_post 168.0 c control 153.2 d SED 4.4 NA Number of Reps 4.0 NA 8.10.1.3.5 Table Aesthetics At this point, our table is essentially complete. But we would like tweak a couple of things. The first thing we want to do is remove those NAs and just leave those cells blank. The fmt_missing() function will take care of that for us. The first argument, columns = TRUE, tells R to search the entire table for occurrences of NA. The second argument, missing_text = \", specifies they be replaced with a blank (\"). gt_tbl %&gt;% fmt_number(columns = vars(yield), decimals = 1) %&gt;% cols_width(everything() ~ &quot;200px&quot;) %&gt;% cols_align(align = &quot;left&quot;, columns = vars(trt)) %&gt;% cols_align(align = &quot;center&quot;, columns = vars(yield, groups)) %&gt;% cols_label(trt = &quot;Treatment&quot;, yield = html(&quot;Yield (bu acre&lt;sup&gt;-1&lt;/sup&gt;)&quot;), groups = &quot;LSD Groupings&quot;) %&gt;% fmt_missing(columns = TRUE, missing_text = &quot;&quot;) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #jkhmjujtqk .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #jkhmjujtqk .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #jkhmjujtqk .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #jkhmjujtqk .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #jkhmjujtqk .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #jkhmjujtqk .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #jkhmjujtqk .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #jkhmjujtqk .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #jkhmjujtqk .gt_column_spanner_outer:first-child { padding-left: 0; } #jkhmjujtqk .gt_column_spanner_outer:last-child { padding-right: 0; } #jkhmjujtqk .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #jkhmjujtqk .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #jkhmjujtqk .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #jkhmjujtqk .gt_from_md > :first-child { margin-top: 0; } #jkhmjujtqk .gt_from_md > :last-child { margin-bottom: 0; } #jkhmjujtqk .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #jkhmjujtqk .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #jkhmjujtqk .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #jkhmjujtqk .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #jkhmjujtqk .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #jkhmjujtqk .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #jkhmjujtqk .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #jkhmjujtqk .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #jkhmjujtqk .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #jkhmjujtqk .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #jkhmjujtqk .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #jkhmjujtqk .gt_sourcenote { font-size: 90%; padding: 4px; } #jkhmjujtqk .gt_left { text-align: left; } #jkhmjujtqk .gt_center { text-align: center; } #jkhmjujtqk .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #jkhmjujtqk .gt_font_normal { font-weight: normal; } #jkhmjujtqk .gt_font_bold { font-weight: bold; } #jkhmjujtqk .gt_font_italic { font-style: italic; } #jkhmjujtqk .gt_super { font-size: 65%; } #jkhmjujtqk .gt_footnote_marks { font-style: italic; font-size: 65%; } Treatment Yield (bu acre-1) LSD Groupings aa_pre_post 191.3 a aa_pre 181.8 b uan_pre_post 179.4 b uan_pre 175.9 bc aa_post 174.8 bc uan_post 168.0 c control 153.2 d SED 4.4 Number of Reps 4.0 Finally, we would like to make a couple of the lines a little wider: the top and bottom lines of the table, as well as the line under the column labels. We can do this with the tab_options() function, which then allows us to format dozens of table aspects: lines, fonts, background colors, etc. We will use a table of trio.border arguments to widen those three lines. gt_tbl %&gt;% fmt_number(columns = vars(yield), decimals = 1) %&gt;% cols_width(everything() ~ &quot;200px&quot;) %&gt;% cols_align(align = &quot;left&quot;, columns = vars(trt)) %&gt;% cols_align(align = &quot;center&quot;, columns = vars(yield, groups)) %&gt;% cols_label(trt = &quot;Treatment&quot;, yield = html(&quot;Yield (bu acre&lt;sup&gt;-1&lt;/sup&gt;)&quot;), groups = &quot;LSD Groupings&quot;) %&gt;% fmt_missing(columns = TRUE, missing_text = &quot;&quot;) %&gt;% tab_options(table.border.top.width = 4, table.border.bottom.width = 4, column_labels.border.bottom.width = 4) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #rfcujshwhj .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 4px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 4px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #rfcujshwhj .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rfcujshwhj .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #rfcujshwhj .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #rfcujshwhj .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rfcujshwhj .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 4px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rfcujshwhj .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #rfcujshwhj .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #rfcujshwhj .gt_column_spanner_outer:first-child { padding-left: 0; } #rfcujshwhj .gt_column_spanner_outer:last-child { padding-right: 0; } #rfcujshwhj .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 4px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #rfcujshwhj .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #rfcujshwhj .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #rfcujshwhj .gt_from_md > :first-child { margin-top: 0; } #rfcujshwhj .gt_from_md > :last-child { margin-bottom: 0; } #rfcujshwhj .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #rfcujshwhj .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #rfcujshwhj .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rfcujshwhj .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #rfcujshwhj .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rfcujshwhj .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #rfcujshwhj .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #rfcujshwhj .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rfcujshwhj .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rfcujshwhj .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #rfcujshwhj .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rfcujshwhj .gt_sourcenote { font-size: 90%; padding: 4px; } #rfcujshwhj .gt_left { text-align: left; } #rfcujshwhj .gt_center { text-align: center; } #rfcujshwhj .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #rfcujshwhj .gt_font_normal { font-weight: normal; } #rfcujshwhj .gt_font_bold { font-weight: bold; } #rfcujshwhj .gt_font_italic { font-style: italic; } #rfcujshwhj .gt_super { font-size: 65%; } #rfcujshwhj .gt_footnote_marks { font-style: italic; font-size: 65%; } Treatment Yield (bu acre-1) LSD Groupings aa_pre_post 191.3 a aa_pre 181.8 b uan_pre_post 179.4 b uan_pre 175.9 bc aa_post 174.8 bc uan_post 168.0 c control 153.2 d SED 4.4 Number of Reps 4.0 8.10.1.3.6 Saving Your Table Finally, we can assign our table to an object, corn_n_gt, and save is as an .html file to use it in other documents. corn_n_gt = gt_tbl %&gt;% fmt_number(columns = vars(yield), decimals = 1) %&gt;% cols_width(everything() ~ &quot;200px&quot;) %&gt;% cols_align(align = &quot;left&quot;, columns = vars(trt)) %&gt;% cols_align(align = &quot;center&quot;, columns = vars(yield, groups)) %&gt;% cols_label(trt = &quot;Treatment&quot;, yield = html(&quot;Yield (bu acre&lt;sup&gt;-1&lt;/sup&gt;)&quot;), groups = &quot;LSD Groupings&quot;) %&gt;% fmt_missing(columns = TRUE, missing_text = &quot;&quot;) %&gt;% tab_options(table.border.top.width = 4, table.border.bottom.width = 4, column_labels.border.bottom.width = 4) gtsave(corn_n_gt, &quot;corn_n_means_table.html&quot;) We could go on and on, but that is enough for now, and should get you pretty far. More information on the gt() package can be found at: https://gt.rstudio.com/index.html. 8.10.2 Practice: Canola canola = read.csv(&quot;data-unit-8/exercise_data/canola_gd.csv&quot;) head(canola) ## block cultivar yield ## 1 1 Bob 1508.424 ## 2 2 Bob 1962.072 ## 3 3 Bob 1364.861 ## 4 4 Bob 1832.715 ## 5 1 Donna 1212.190 ## 6 2 Donna 1629.680 Since the focus of this exercise is table-building, I will generate the initial lsd means table for you. library(agricolae) canola_model = aov(yield ~ block + cultivar, data = canola) lsd_canola = LSD.test(canola_model, &quot;cultivar&quot;) lsd_canola ## $statistics ## MSerror Df Mean CV t.value LSD ## 109086 17 2296.983 14.37894 2.109816 492.7357 ## ## $parameters ## test p.ajusted name.t ntr alpha ## Fisher-LSD none cultivar 6 0.05 ## ## $means ## yield std r LCL UCL Min Max Q25 ## Bill 2296.268 326.6421 4 1947.851 2644.685 1865.903 2614.081 2142.888 ## Bob 1667.018 277.4666 4 1318.601 2015.435 1364.861 1962.072 1472.533 ## Donna 1833.282 514.0721 4 1484.865 2181.699 1212.190 2358.635 1525.308 ## Jerry 2669.161 176.6376 4 2320.745 3017.578 2545.158 2923.794 2552.218 ## Mickey 2406.509 263.5791 4 2058.092 2754.926 2130.413 2656.553 2207.614 ## Phil 2909.658 275.1842 4 2561.242 3258.075 2565.683 3218.583 2776.782 ## Q50 Q75 ## Bill 2352.545 2505.925 ## Bob 1670.570 1865.055 ## Donna 1881.152 2189.126 ## Jerry 2603.847 2720.790 ## Mickey 2419.535 2618.430 ## Phil 2927.183 3060.060 ## ## $comparison ## NULL ## ## $groups ## yield groups ## Phil 2909.658 a ## Jerry 2669.161 ab ## Mickey 2406.509 b ## Bill 2296.268 bc ## Donna 1833.282 cd ## Bob 1667.018 d ## ## attr(,&quot;class&quot;) ## [1] &quot;group&quot; First, extract the $groups table from the canola_lsd output. canola_means = lsd_canola$groups Next, we need to convert our row names to a column canola_table_w_trt_column = canola_means %&gt;% rownames_to_column(var=&quot;cultivar&quot;) canola_table_w_trt_column ## cultivar yield groups ## 1 Phil 2909.658 a ## 2 Jerry 2669.161 ab ## 3 Mickey 2406.509 b ## 4 Bill 2296.268 bc ## 5 Donna 1833.282 cd ## 6 Bob 1667.018 d Finally, we need to calculate the SED and number of reps and add those to the table r = 4 MSE = lsd_canola$statistics$MSerror SED = sqrt(2*MSE/r) canola_table_w_stats = canola_table_w_trt_column %&gt;% add_row(cultivar=&quot;SED&quot;, yield=SED, groups=&quot;&quot;) %&gt;% add_row(cultivar=&quot;Number of Reps&quot;, yield=r, groups=&quot;&quot;) canola_table_w_stats ## cultivar yield groups ## 1 Phil 2909.6584 a ## 2 Jerry 2669.1615 ab ## 3 Mickey 2406.5090 b ## 4 Bill 2296.2682 bc ## 5 Donna 1833.2820 cd ## 6 Bob 1667.0181 d ## 7 SED 233.5444 ## 8 Number of Reps 4.0000 At this point, the easiest way for you to make your table is to tweak the code below (this is why coding is so powerful). You will need to: change the name of the data.frame from which you are building the table (line 1) change the number of decimals in the fmt_number table to zero (line 2) substitute cultivar for treatment wherever it occurs in the code (lines 3 and 5) change the formal heading for the first column from Treatment to Cultivar canola_means_table = gt(canola_table_w_trt_column) %&gt;% fmt_number(columns = vars(yield), decimals = 1) %&gt;% cols_width(everything() ~ &quot;200px&quot;) %&gt;% cols_align(align = &quot;left&quot;, columns = vars(cultivar)) %&gt;% cols_align(align = &quot;center&quot;, columns = vars(yield, groups)) %&gt;% cols_label(cultivar = &quot;Cultivar&quot;, yield = html(&quot;Yield (bu acre&lt;sup&gt;-1&lt;/sup&gt;)&quot;), groups = &quot;LSD Groupings&quot;) canola_means_table html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #gzrxjcpsfq .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #gzrxjcpsfq .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gzrxjcpsfq .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #gzrxjcpsfq .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #gzrxjcpsfq .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gzrxjcpsfq .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gzrxjcpsfq .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #gzrxjcpsfq .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #gzrxjcpsfq .gt_column_spanner_outer:first-child { padding-left: 0; } #gzrxjcpsfq .gt_column_spanner_outer:last-child { padding-right: 0; } #gzrxjcpsfq .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #gzrxjcpsfq .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #gzrxjcpsfq .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #gzrxjcpsfq .gt_from_md > :first-child { margin-top: 0; } #gzrxjcpsfq .gt_from_md > :last-child { margin-bottom: 0; } #gzrxjcpsfq .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #gzrxjcpsfq .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #gzrxjcpsfq .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gzrxjcpsfq .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #gzrxjcpsfq .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gzrxjcpsfq .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #gzrxjcpsfq .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #gzrxjcpsfq .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gzrxjcpsfq .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gzrxjcpsfq .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #gzrxjcpsfq .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gzrxjcpsfq .gt_sourcenote { font-size: 90%; padding: 4px; } #gzrxjcpsfq .gt_left { text-align: left; } #gzrxjcpsfq .gt_center { text-align: center; } #gzrxjcpsfq .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #gzrxjcpsfq .gt_font_normal { font-weight: normal; } #gzrxjcpsfq .gt_font_bold { font-weight: bold; } #gzrxjcpsfq .gt_font_italic { font-style: italic; } #gzrxjcpsfq .gt_super { font-size: 65%; } #gzrxjcpsfq .gt_footnote_marks { font-style: italic; font-size: 65%; } Cultivar Yield (bu acre-1) LSD Groupings Phil 2,909.7 a Jerry 2,669.2 ab Mickey 2,406.5 b Bill 2,296.3 bc Donna 1,833.3 cd Bob 1,667.0 d 8.10.3 Practice: Broccoli LSD Common bean was grown with organic practices following four previous crops. bean = read.csv(&quot;data-unit-8/exercise_data/common_bean.csv&quot;) head(bean) ## block prev_crop fresh_weight ## 1 1 conv_broccoli 52.15924 ## 2 1 green_manure 49.82191 ## 3 1 organic_broccoli 36.80926 ## 4 1 fallow 54.05342 ## 5 2 conv_broccoli 52.79726 ## 6 2 fallow 47.68364 8.10.3.1 Create the LSD Groups table library(agricolae) # model statement # LSD.test # extract groups table from lsd output # use rownames_to_column to create new column with rownames 8.10.3.2 Prepare the table contents # r = number of reps # extract MSE from lsd output # calculate SED # use add_rows to add SED and r to lsd groups table 8.10.3.3 Create the gt() table # recycle table from above: # change data.frame you feed to gt (line 1) # adjust decimals as appropriate (line 2) # change variable references for treatment and, if appropriate, yield (lines 3-5) # change column labels (lines 5-7) # "]]
